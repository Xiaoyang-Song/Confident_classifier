ic| len(dset): 60000
Namespace(batch_size=64, epochs=100, lr=0.001, no_cuda=False, seed=1, log_interval=100, dataset='MNIST-FashionMNIST', dataroot='./data', imageSize=32, outf='./results/joint_confidence_loss/MFM-0.01/', wd=0.0, droprate=0.1, decreasing_lr='60', num_classes=10, beta=0.01, num_channels=1)
Random Seed:  1
load InD data for Experiment:  MNIST-FashionMNIST
Load model
DenseNet3(
  (conv1): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock(
    (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock(
    (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(342, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (fc): Linear(in_features=342, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
load GAN
Setup optimizer
0.001
  0%|          | 0/100 [00:00<?, ?it/s]/home/xysong/.conda/envs/OoD/lib/python3.9/site-packages/torch/nn/functional.py:2904: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
  1%|          | 1/100 [03:31<5:48:49, 211.41s/it]  2%|▏         | 2/100 [07:02<5:44:57, 211.20s/it]  3%|▎         | 3/100 [10:33<5:41:19, 211.13s/it]  4%|▍         | 4/100 [14:04<5:37:46, 211.11s/it]  5%|▌         | 5/100 [17:35<5:34:14, 211.10s/it]  6%|▌         | 6/100 [21:06<5:30:42, 211.09s/it]  7%|▋         | 7/100 [24:37<5:27:10, 211.08s/it]  8%|▊         | 8/100 [28:08<5:23:39, 211.08s/it]Classification Train Epoch: 1 [0/60000 (0%)]	Loss: 2.269959, KL fake Loss: 0.031852
Classification Train Epoch: 1 [6400/60000 (11%)]	Loss: 0.308710, KL fake Loss: 1.223053
Classification Train Epoch: 1 [12800/60000 (21%)]	Loss: 0.223526, KL fake Loss: 1.978840
Classification Train Epoch: 1 [19200/60000 (32%)]	Loss: 0.133173, KL fake Loss: 2.346838
Classification Train Epoch: 1 [25600/60000 (43%)]	Loss: 0.097887, KL fake Loss: 2.522449
Classification Train Epoch: 1 [32000/60000 (53%)]	Loss: 0.027923, KL fake Loss: 3.112858
Classification Train Epoch: 1 [38400/60000 (64%)]	Loss: 0.117970, KL fake Loss: 3.116418
Classification Train Epoch: 1 [44800/60000 (75%)]	Loss: 0.093923, KL fake Loss: 3.397847
Classification Train Epoch: 1 [51200/60000 (85%)]	Loss: 0.067655, KL fake Loss: 3.479701
Classification Train Epoch: 1 [57600/60000 (96%)]	Loss: 0.120730, KL fake Loss: 3.579533

Test set: Average loss: 0.0390, Accuracy: 9893/10000 (99%)

Classification Train Epoch: 2 [0/60000 (0%)]	Loss: 0.056788, KL fake Loss: 3.325878
Classification Train Epoch: 2 [6400/60000 (11%)]	Loss: 0.074910, KL fake Loss: 3.743275
Classification Train Epoch: 2 [12800/60000 (21%)]	Loss: 0.064704, KL fake Loss: 3.376941
Classification Train Epoch: 2 [19200/60000 (32%)]	Loss: 0.035603, KL fake Loss: 3.566258
Classification Train Epoch: 2 [25600/60000 (43%)]	Loss: 0.113409, KL fake Loss: 3.746550
Classification Train Epoch: 2 [32000/60000 (53%)]	Loss: 0.026611, KL fake Loss: 3.760472
Classification Train Epoch: 2 [38400/60000 (64%)]	Loss: 0.016997, KL fake Loss: 3.172256
Classification Train Epoch: 2 [44800/60000 (75%)]	Loss: 0.010335, KL fake Loss: 3.679672
Classification Train Epoch: 2 [51200/60000 (85%)]	Loss: 0.036402, KL fake Loss: 3.838099
Classification Train Epoch: 2 [57600/60000 (96%)]	Loss: 0.030809, KL fake Loss: 3.860000

Test set: Average loss: 0.0396, Accuracy: 9897/10000 (99%)

Classification Train Epoch: 3 [0/60000 (0%)]	Loss: 0.025575, KL fake Loss: 3.907732
Classification Train Epoch: 3 [6400/60000 (11%)]	Loss: 0.097284, KL fake Loss: 3.810634
Classification Train Epoch: 3 [12800/60000 (21%)]	Loss: 0.015074, KL fake Loss: 3.857654
Classification Train Epoch: 3 [19200/60000 (32%)]	Loss: 0.011594, KL fake Loss: 3.578727
Classification Train Epoch: 3 [25600/60000 (43%)]	Loss: 0.083699, KL fake Loss: 3.712805
Classification Train Epoch: 3 [32000/60000 (53%)]	Loss: 0.088332, KL fake Loss: 4.034027
Classification Train Epoch: 3 [38400/60000 (64%)]	Loss: 0.016326, KL fake Loss: 3.465618
Classification Train Epoch: 3 [44800/60000 (75%)]	Loss: 0.037573, KL fake Loss: 3.687909
Classification Train Epoch: 3 [51200/60000 (85%)]	Loss: 0.009922, KL fake Loss: 3.572133
Classification Train Epoch: 3 [57600/60000 (96%)]	Loss: 0.022664, KL fake Loss: 3.768602

Test set: Average loss: 0.0359, Accuracy: 9911/10000 (99%)

Classification Train Epoch: 4 [0/60000 (0%)]	Loss: 0.012182, KL fake Loss: 3.279569
Classification Train Epoch: 4 [6400/60000 (11%)]	Loss: 0.007825, KL fake Loss: 4.060832
Classification Train Epoch: 4 [12800/60000 (21%)]	Loss: 0.038204, KL fake Loss: 3.449641
Classification Train Epoch: 4 [19200/60000 (32%)]	Loss: 0.071694, KL fake Loss: 3.819237
Classification Train Epoch: 4 [25600/60000 (43%)]	Loss: 0.023860, KL fake Loss: 3.579193
Classification Train Epoch: 4 [32000/60000 (53%)]	Loss: 0.007116, KL fake Loss: 3.746749
Classification Train Epoch: 4 [38400/60000 (64%)]	Loss: 0.084794, KL fake Loss: 3.744719
Classification Train Epoch: 4 [44800/60000 (75%)]	Loss: 0.060865, KL fake Loss: 3.642829
Classification Train Epoch: 4 [51200/60000 (85%)]	Loss: 0.011882, KL fake Loss: 3.380245
Classification Train Epoch: 4 [57600/60000 (96%)]	Loss: 0.019623, KL fake Loss: 3.703482

Test set: Average loss: 0.0341, Accuracy: 9924/10000 (99%)

Classification Train Epoch: 5 [0/60000 (0%)]	Loss: 0.030168, KL fake Loss: 3.756772
Classification Train Epoch: 5 [6400/60000 (11%)]	Loss: 0.032983, KL fake Loss: 3.627080
Classification Train Epoch: 5 [12800/60000 (21%)]	Loss: 0.076897, KL fake Loss: 3.608201
Classification Train Epoch: 5 [19200/60000 (32%)]	Loss: 0.011346, KL fake Loss: 3.822738
Classification Train Epoch: 5 [25600/60000 (43%)]	Loss: 0.018341, KL fake Loss: 3.682291
Classification Train Epoch: 5 [32000/60000 (53%)]	Loss: 0.048327, KL fake Loss: 3.742561
Classification Train Epoch: 5 [38400/60000 (64%)]	Loss: 0.024833, KL fake Loss: 3.225993
Classification Train Epoch: 5 [44800/60000 (75%)]	Loss: 0.018565, KL fake Loss: 3.293618
Classification Train Epoch: 5 [51200/60000 (85%)]	Loss: 0.044625, KL fake Loss: 3.706987
Classification Train Epoch: 5 [57600/60000 (96%)]	Loss: 0.048761, KL fake Loss: 3.157762

Test set: Average loss: 0.0242, Accuracy: 9952/10000 (100%)

Classification Train Epoch: 6 [0/60000 (0%)]	Loss: 0.013062, KL fake Loss: 3.579878
Classification Train Epoch: 6 [6400/60000 (11%)]	Loss: 0.007086, KL fake Loss: 3.111487
Classification Train Epoch: 6 [12800/60000 (21%)]	Loss: 0.019879, KL fake Loss: 3.232361
Classification Train Epoch: 6 [19200/60000 (32%)]	Loss: 0.019426, KL fake Loss: 3.355455
Classification Train Epoch: 6 [25600/60000 (43%)]	Loss: 0.035024, KL fake Loss: 3.311025
Classification Train Epoch: 6 [32000/60000 (53%)]	Loss: 0.015890, KL fake Loss: 4.044902
Classification Train Epoch: 6 [38400/60000 (64%)]	Loss: 0.018781, KL fake Loss: 3.841374
Classification Train Epoch: 6 [44800/60000 (75%)]	Loss: 0.009174, KL fake Loss: 3.559798
Classification Train Epoch: 6 [51200/60000 (85%)]	Loss: 0.005893, KL fake Loss: 3.360096
Classification Train Epoch: 6 [57600/60000 (96%)]	Loss: 0.026611, KL fake Loss: 3.432390

Test set: Average loss: 0.0251, Accuracy: 9952/10000 (100%)

Classification Train Epoch: 7 [0/60000 (0%)]	Loss: 0.008473, KL fake Loss: 3.575622
Classification Train Epoch: 7 [6400/60000 (11%)]	Loss: 0.028978, KL fake Loss: 3.309143
Classification Train Epoch: 7 [12800/60000 (21%)]	Loss: 0.080152, KL fake Loss: 2.917946
Classification Train Epoch: 7 [19200/60000 (32%)]	Loss: 0.039340, KL fake Loss: 3.402041
Classification Train Epoch: 7 [25600/60000 (43%)]	Loss: 0.007303, KL fake Loss: 2.971004
Classification Train Epoch: 7 [32000/60000 (53%)]	Loss: 0.023526, KL fake Loss: 3.156575
Classification Train Epoch: 7 [38400/60000 (64%)]	Loss: 0.049954, KL fake Loss: 3.448945
Classification Train Epoch: 7 [44800/60000 (75%)]	Loss: 0.075133, KL fake Loss: 3.392467
Classification Train Epoch: 7 [51200/60000 (85%)]	Loss: 0.024248, KL fake Loss: 2.660570
Classification Train Epoch: 7 [57600/60000 (96%)]	Loss: 0.008441, KL fake Loss: 2.866899

Test set: Average loss: 0.0404, Accuracy: 9928/10000 (99%)

Classification Train Epoch: 8 [0/60000 (0%)]	Loss: 0.012525, KL fake Loss: 2.994766
Classification Train Epoch: 8 [6400/60000 (11%)]	Loss: 0.016329, KL fake Loss: 3.764586
Classification Train Epoch: 8 [12800/60000 (21%)]	Loss: 0.008610, KL fake Loss: 2.684490
Classification Train Epoch: 8 [19200/60000 (32%)]	Loss: 0.006432, KL fake Loss: 2.862607
Classification Train Epoch: 8 [25600/60000 (43%)]	Loss: 0.027266, KL fake Loss: 3.159001
Classification Train Epoch: 8 [32000/60000 (53%)]	Loss: 0.022029, KL fake Loss: 2.183079
Classification Train Epoch: 8 [38400/60000 (64%)]	Loss: 0.008716, KL fake Loss: 2.486121
Classification Train Epoch: 8 [44800/60000 (75%)]	Loss: 0.007784, KL fake Loss: 2.535396
Classification Train Epoch: 8 [51200/60000 (85%)]	Loss: 0.006829, KL fake Loss: 3.260145
Classification Train Epoch: 8 [57600/60000 (96%)]	Loss: 0.006585, KL fake Loss: 2.574962

Test set: Average loss: 0.0440, Accuracy: 9944/10000 (99%)

Classification Train Epoch: 9 [0/60000 (0%)]	Loss: 0.012967, KL fake Loss: 2.497159
Classification Train Epoch: 9 [6400/60000 (11%)]	Loss: 0.083976, KL fake Loss: 2.587613
Classification Train Epoch: 9 [12800/60000 (21%)]	Loss: 0.034521, KL fake Loss: 2.339070
Classification Train Epoch: 9 [19200/60000 (32%)]	Loss: 0.006573, KL fake Loss: 2.565604
Classification Train Epoch: 9 [25600/60000 (43%)]	Loss: 0.004141, KL fake Loss: 2.277811
Classification Train Epoch: 9 [32000/60000 (53%)]	Loss: 0.030411, KL fake Loss: 2.727727
Classification Train Epoch: 9 [38400/60000 (64%)]	Loss: 0.008282, KL fake Loss: 4.215040
  9%|▉         | 9/100 [31:39<5:20:08, 211.08s/it] 10%|█         | 10/100 [35:11<5:16:37, 211.08s/it] 11%|█         | 11/100 [38:42<5:13:06, 211.08s/it] 12%|█▏        | 12/100 [42:13<5:09:35, 211.08s/it] 13%|█▎        | 13/100 [45:44<5:06:04, 211.08s/it] 14%|█▍        | 14/100 [49:15<5:02:33, 211.08s/it] 15%|█▌        | 15/100 [52:46<4:59:02, 211.09s/it] 16%|█▌        | 16/100 [56:17<4:55:31, 211.10s/it] 17%|█▋        | 17/100 [59:48<4:52:00, 211.09s/it]Classification Train Epoch: 9 [44800/60000 (75%)]	Loss: 0.003597, KL fake Loss: 1.751638
Classification Train Epoch: 9 [51200/60000 (85%)]	Loss: 0.004834, KL fake Loss: 3.626258
Classification Train Epoch: 9 [57600/60000 (96%)]	Loss: 0.064677, KL fake Loss: 1.687630

Test set: Average loss: 0.0721, Accuracy: 9938/10000 (99%)

Classification Train Epoch: 10 [0/60000 (0%)]	Loss: 0.005480, KL fake Loss: 1.988473
Classification Train Epoch: 10 [6400/60000 (11%)]	Loss: 0.033691, KL fake Loss: 1.426715
Classification Train Epoch: 10 [12800/60000 (21%)]	Loss: 0.042763, KL fake Loss: 1.443919
Classification Train Epoch: 10 [19200/60000 (32%)]	Loss: 0.004084, KL fake Loss: 1.626579
Classification Train Epoch: 10 [25600/60000 (43%)]	Loss: 0.015051, KL fake Loss: 2.471408
Classification Train Epoch: 10 [32000/60000 (53%)]	Loss: 0.004935, KL fake Loss: 1.117604
Classification Train Epoch: 10 [38400/60000 (64%)]	Loss: 0.002946, KL fake Loss: 4.421995
Classification Train Epoch: 10 [44800/60000 (75%)]	Loss: 0.007538, KL fake Loss: 2.531660
Classification Train Epoch: 10 [51200/60000 (85%)]	Loss: 0.028714, KL fake Loss: 2.279877
Classification Train Epoch: 10 [57600/60000 (96%)]	Loss: 0.003368, KL fake Loss: 1.670639

Test set: Average loss: 0.1114, Accuracy: 9907/10000 (99%)

Classification Train Epoch: 11 [0/60000 (0%)]	Loss: 0.040945, KL fake Loss: 2.846470
Classification Train Epoch: 11 [6400/60000 (11%)]	Loss: 0.015106, KL fake Loss: 4.044328
Classification Train Epoch: 11 [12800/60000 (21%)]	Loss: 0.005127, KL fake Loss: 5.088189
Classification Train Epoch: 11 [19200/60000 (32%)]	Loss: 0.014820, KL fake Loss: 5.108538
Classification Train Epoch: 11 [25600/60000 (43%)]	Loss: 0.035605, KL fake Loss: 1.936530
Classification Train Epoch: 11 [32000/60000 (53%)]	Loss: 0.005023, KL fake Loss: 1.957543
Classification Train Epoch: 11 [38400/60000 (64%)]	Loss: 0.043104, KL fake Loss: 4.701578
Classification Train Epoch: 11 [44800/60000 (75%)]	Loss: 0.006663, KL fake Loss: 1.176115
Classification Train Epoch: 11 [51200/60000 (85%)]	Loss: 0.061331, KL fake Loss: 4.304730
Classification Train Epoch: 11 [57600/60000 (96%)]	Loss: 0.037172, KL fake Loss: 0.579449

Test set: Average loss: 0.1647, Accuracy: 9903/10000 (99%)

Classification Train Epoch: 12 [0/60000 (0%)]	Loss: 0.023164, KL fake Loss: 1.543012
Classification Train Epoch: 12 [6400/60000 (11%)]	Loss: 0.007508, KL fake Loss: 1.675654
Classification Train Epoch: 12 [12800/60000 (21%)]	Loss: 0.002028, KL fake Loss: 1.012173
Classification Train Epoch: 12 [19200/60000 (32%)]	Loss: 0.050266, KL fake Loss: 2.063193
Classification Train Epoch: 12 [25600/60000 (43%)]	Loss: 0.086094, KL fake Loss: 3.565972
Classification Train Epoch: 12 [32000/60000 (53%)]	Loss: 0.019611, KL fake Loss: 1.141107
Classification Train Epoch: 12 [38400/60000 (64%)]	Loss: 0.007748, KL fake Loss: 1.608105
Classification Train Epoch: 12 [44800/60000 (75%)]	Loss: 0.017609, KL fake Loss: 3.418665
Classification Train Epoch: 12 [51200/60000 (85%)]	Loss: 0.047068, KL fake Loss: 2.405392
Classification Train Epoch: 12 [57600/60000 (96%)]	Loss: 0.033132, KL fake Loss: 0.474576

Test set: Average loss: 0.1280, Accuracy: 9943/10000 (99%)

Classification Train Epoch: 13 [0/60000 (0%)]	Loss: 0.002711, KL fake Loss: 0.700953
Classification Train Epoch: 13 [6400/60000 (11%)]	Loss: 0.015812, KL fake Loss: 0.696205
Classification Train Epoch: 13 [12800/60000 (21%)]	Loss: 0.010918, KL fake Loss: 5.588325
Classification Train Epoch: 13 [19200/60000 (32%)]	Loss: 0.002408, KL fake Loss: 1.571297
Classification Train Epoch: 13 [25600/60000 (43%)]	Loss: 0.016053, KL fake Loss: 1.074771
Classification Train Epoch: 13 [32000/60000 (53%)]	Loss: 0.030374, KL fake Loss: 0.744110
Classification Train Epoch: 13 [38400/60000 (64%)]	Loss: 0.013893, KL fake Loss: 1.826972
Classification Train Epoch: 13 [44800/60000 (75%)]	Loss: 0.004424, KL fake Loss: 0.584054
Classification Train Epoch: 13 [51200/60000 (85%)]	Loss: 0.041810, KL fake Loss: 2.717171
Classification Train Epoch: 13 [57600/60000 (96%)]	Loss: 0.031129, KL fake Loss: 2.250046

Test set: Average loss: 0.0591, Accuracy: 9933/10000 (99%)

Classification Train Epoch: 14 [0/60000 (0%)]	Loss: 0.014131, KL fake Loss: 1.278288
Classification Train Epoch: 14 [6400/60000 (11%)]	Loss: 0.002890, KL fake Loss: 0.845740
Classification Train Epoch: 14 [12800/60000 (21%)]	Loss: 0.019580, KL fake Loss: 1.320802
Classification Train Epoch: 14 [19200/60000 (32%)]	Loss: 0.002783, KL fake Loss: 3.384376
Classification Train Epoch: 14 [25600/60000 (43%)]	Loss: 0.014580, KL fake Loss: 2.383736
Classification Train Epoch: 14 [32000/60000 (53%)]	Loss: 0.216097, KL fake Loss: 0.780157
Classification Train Epoch: 14 [38400/60000 (64%)]	Loss: 0.147590, KL fake Loss: 0.964266
Classification Train Epoch: 14 [44800/60000 (75%)]	Loss: 0.002279, KL fake Loss: 2.877476
Classification Train Epoch: 14 [51200/60000 (85%)]	Loss: 0.022659, KL fake Loss: 1.258904
Classification Train Epoch: 14 [57600/60000 (96%)]	Loss: 0.010532, KL fake Loss: 2.166215

Test set: Average loss: 0.0916, Accuracy: 9945/10000 (99%)

Classification Train Epoch: 15 [0/60000 (0%)]	Loss: 0.002943, KL fake Loss: 2.515821
Classification Train Epoch: 15 [6400/60000 (11%)]	Loss: 0.001803, KL fake Loss: 1.178017
Classification Train Epoch: 15 [12800/60000 (21%)]	Loss: 0.016324, KL fake Loss: 0.877903
Classification Train Epoch: 15 [19200/60000 (32%)]	Loss: 0.014518, KL fake Loss: 0.496170
Classification Train Epoch: 15 [25600/60000 (43%)]	Loss: 0.002427, KL fake Loss: 3.206373
Classification Train Epoch: 15 [32000/60000 (53%)]	Loss: 0.009292, KL fake Loss: 0.991510
Classification Train Epoch: 15 [38400/60000 (64%)]	Loss: 0.076813, KL fake Loss: 3.233715
Classification Train Epoch: 15 [44800/60000 (75%)]	Loss: 0.002623, KL fake Loss: 1.988317
Classification Train Epoch: 15 [51200/60000 (85%)]	Loss: 0.009729, KL fake Loss: 1.176283
Classification Train Epoch: 15 [57600/60000 (96%)]	Loss: 0.023770, KL fake Loss: 0.943850

Test set: Average loss: 0.0582, Accuracy: 9940/10000 (99%)

Classification Train Epoch: 16 [0/60000 (0%)]	Loss: 0.024730, KL fake Loss: 1.558627
Classification Train Epoch: 16 [6400/60000 (11%)]	Loss: 0.001427, KL fake Loss: 2.473025
Classification Train Epoch: 16 [12800/60000 (21%)]	Loss: 0.000958, KL fake Loss: 1.273630
Classification Train Epoch: 16 [19200/60000 (32%)]	Loss: 0.002855, KL fake Loss: 0.249527
Classification Train Epoch: 16 [25600/60000 (43%)]	Loss: 0.116469, KL fake Loss: 0.439514
Classification Train Epoch: 16 [32000/60000 (53%)]	Loss: 0.004083, KL fake Loss: 0.959769
Classification Train Epoch: 16 [38400/60000 (64%)]	Loss: 0.001350, KL fake Loss: 5.642541
Classification Train Epoch: 16 [44800/60000 (75%)]	Loss: 0.003981, KL fake Loss: 0.459647
Classification Train Epoch: 16 [51200/60000 (85%)]	Loss: 0.004944, KL fake Loss: 0.463339
Classification Train Epoch: 16 [57600/60000 (96%)]	Loss: 0.023008, KL fake Loss: 3.206589

Test set: Average loss: 0.1781, Accuracy: 9957/10000 (100%)

Classification Train Epoch: 17 [0/60000 (0%)]	Loss: 0.003610, KL fake Loss: 1.241953
Classification Train Epoch: 17 [6400/60000 (11%)]	Loss: 0.035937, KL fake Loss: 2.731812
Classification Train Epoch: 17 [12800/60000 (21%)]	Loss: 0.002652, KL fake Loss: 0.718175
Classification Train Epoch: 17 [19200/60000 (32%)]	Loss: 0.002884, KL fake Loss: 1.282631
Classification Train Epoch: 17 [25600/60000 (43%)]	Loss: 0.000915, KL fake Loss: 0.702377
Classification Train Epoch: 17 [32000/60000 (53%)]	Loss: 0.007931, KL fake Loss: 1.577174
Classification Train Epoch: 17 [38400/60000 (64%)]	Loss: 0.038499, KL fake Loss: 0.590137
Classification Train Epoch: 17 [44800/60000 (75%)]	Loss: 0.165524, KL fake Loss: 0.643997
Classification Train Epoch: 17 [51200/60000 (85%)]	Loss: 0.003217, KL fake Loss: 0.526379
Classification Train Epoch: 17 [57600/60000 (96%)]	Loss: 0.006247, KL fake Loss: 1.327811

Test set: Average loss: 0.2990, Accuracy: 9915/10000 (99%)

Classification Train Epoch: 18 [0/60000 (0%)]	Loss: 0.012648, KL fake Loss: 0.535090
Classification Train Epoch: 18 [6400/60000 (11%)]	Loss: 0.406234, KL fake Loss: 0.300698
 18%|█▊        | 18/100 [1:03:19<4:48:29, 211.09s/it] 19%|█▉        | 19/100 [1:06:50<4:44:57, 211.08s/it] 20%|██        | 20/100 [1:10:22<4:41:29, 211.12s/it] 21%|██        | 21/100 [1:13:53<4:37:57, 211.11s/it] 22%|██▏       | 22/100 [1:17:24<4:34:26, 211.11s/it] 23%|██▎       | 23/100 [1:20:55<4:30:55, 211.11s/it] 24%|██▍       | 24/100 [1:24:26<4:27:24, 211.12s/it] 25%|██▌       | 25/100 [1:27:57<4:23:53, 211.11s/it]Classification Train Epoch: 18 [12800/60000 (21%)]	Loss: 0.008419, KL fake Loss: 2.261138
Classification Train Epoch: 18 [19200/60000 (32%)]	Loss: 0.008775, KL fake Loss: 0.292759
Classification Train Epoch: 18 [25600/60000 (43%)]	Loss: 0.042594, KL fake Loss: 1.153180
Classification Train Epoch: 18 [32000/60000 (53%)]	Loss: 0.001202, KL fake Loss: 1.119975
Classification Train Epoch: 18 [38400/60000 (64%)]	Loss: 0.001235, KL fake Loss: 1.365851
Classification Train Epoch: 18 [44800/60000 (75%)]	Loss: 0.009673, KL fake Loss: 0.899266
Classification Train Epoch: 18 [51200/60000 (85%)]	Loss: 0.080348, KL fake Loss: 1.395701
Classification Train Epoch: 18 [57600/60000 (96%)]	Loss: 0.002066, KL fake Loss: 0.877150

Test set: Average loss: 0.3613, Accuracy: 9940/10000 (99%)

Classification Train Epoch: 19 [0/60000 (0%)]	Loss: 0.001797, KL fake Loss: 0.554142
Classification Train Epoch: 19 [6400/60000 (11%)]	Loss: 0.032009, KL fake Loss: 1.920812
Classification Train Epoch: 19 [12800/60000 (21%)]	Loss: 0.001827, KL fake Loss: 0.317263
Classification Train Epoch: 19 [19200/60000 (32%)]	Loss: 0.002235, KL fake Loss: 0.842792
Classification Train Epoch: 19 [25600/60000 (43%)]	Loss: 0.003728, KL fake Loss: 0.664485
Classification Train Epoch: 19 [32000/60000 (53%)]	Loss: 0.002062, KL fake Loss: 4.141241
Classification Train Epoch: 19 [38400/60000 (64%)]	Loss: 0.017400, KL fake Loss: 0.747357
Classification Train Epoch: 19 [44800/60000 (75%)]	Loss: 0.006013, KL fake Loss: 0.696932
Classification Train Epoch: 19 [51200/60000 (85%)]	Loss: 0.010817, KL fake Loss: 0.298935
Classification Train Epoch: 19 [57600/60000 (96%)]	Loss: 0.001129, KL fake Loss: 5.331461

Test set: Average loss: 0.5579, Accuracy: 9894/10000 (99%)

Classification Train Epoch: 20 [0/60000 (0%)]	Loss: 0.027912, KL fake Loss: 0.909237
Classification Train Epoch: 20 [6400/60000 (11%)]	Loss: 0.001705, KL fake Loss: 0.535749
Classification Train Epoch: 20 [12800/60000 (21%)]	Loss: 0.002030, KL fake Loss: 3.908477
Classification Train Epoch: 20 [19200/60000 (32%)]	Loss: 0.001729, KL fake Loss: 0.490082
Classification Train Epoch: 20 [25600/60000 (43%)]	Loss: 0.011489, KL fake Loss: 2.271402
Classification Train Epoch: 20 [32000/60000 (53%)]	Loss: 0.001363, KL fake Loss: 0.247657
Classification Train Epoch: 20 [38400/60000 (64%)]	Loss: 0.010947, KL fake Loss: 0.460046
Classification Train Epoch: 20 [44800/60000 (75%)]	Loss: 0.031082, KL fake Loss: 0.448347
Classification Train Epoch: 20 [51200/60000 (85%)]	Loss: 0.031595, KL fake Loss: 1.835526
Classification Train Epoch: 20 [57600/60000 (96%)]	Loss: 0.004447, KL fake Loss: 0.994172

Test set: Average loss: 0.3645, Accuracy: 9938/10000 (99%)

Classification Train Epoch: 21 [0/60000 (0%)]	Loss: 0.001858, KL fake Loss: 0.628608
Classification Train Epoch: 21 [6400/60000 (11%)]	Loss: 0.003076, KL fake Loss: 0.533679
Classification Train Epoch: 21 [12800/60000 (21%)]	Loss: 0.000480, KL fake Loss: 0.368824
Classification Train Epoch: 21 [19200/60000 (32%)]	Loss: 0.008421, KL fake Loss: 1.477827
Classification Train Epoch: 21 [25600/60000 (43%)]	Loss: 0.005822, KL fake Loss: 0.159782
Classification Train Epoch: 21 [32000/60000 (53%)]	Loss: 0.066429, KL fake Loss: 0.566132
Classification Train Epoch: 21 [38400/60000 (64%)]	Loss: 0.001107, KL fake Loss: 0.253159
Classification Train Epoch: 21 [44800/60000 (75%)]	Loss: 0.004746, KL fake Loss: 0.442982
Classification Train Epoch: 21 [51200/60000 (85%)]	Loss: 0.008691, KL fake Loss: 0.391405
Classification Train Epoch: 21 [57600/60000 (96%)]	Loss: 0.031207, KL fake Loss: 0.199839

Test set: Average loss: 0.5198, Accuracy: 9928/10000 (99%)

Classification Train Epoch: 22 [0/60000 (0%)]	Loss: 0.012685, KL fake Loss: 0.172792
Classification Train Epoch: 22 [6400/60000 (11%)]	Loss: 0.022456, KL fake Loss: 0.349510
Classification Train Epoch: 22 [12800/60000 (21%)]	Loss: 0.037621, KL fake Loss: 4.474318
Classification Train Epoch: 22 [19200/60000 (32%)]	Loss: 0.000975, KL fake Loss: 1.095655
Classification Train Epoch: 22 [25600/60000 (43%)]	Loss: 0.018611, KL fake Loss: 0.205642
Classification Train Epoch: 22 [32000/60000 (53%)]	Loss: 0.014621, KL fake Loss: 0.156219
Classification Train Epoch: 22 [38400/60000 (64%)]	Loss: 0.002143, KL fake Loss: 0.372405
Classification Train Epoch: 22 [44800/60000 (75%)]	Loss: 0.001326, KL fake Loss: 1.421945
Classification Train Epoch: 22 [51200/60000 (85%)]	Loss: 0.001166, KL fake Loss: 0.948042
Classification Train Epoch: 22 [57600/60000 (96%)]	Loss: 0.014226, KL fake Loss: 0.233754

Test set: Average loss: 0.0632, Accuracy: 9934/10000 (99%)

Classification Train Epoch: 23 [0/60000 (0%)]	Loss: 0.001865, KL fake Loss: 0.500530
Classification Train Epoch: 23 [6400/60000 (11%)]	Loss: 0.001590, KL fake Loss: 0.956236
Classification Train Epoch: 23 [12800/60000 (21%)]	Loss: 0.002950, KL fake Loss: 2.205258
Classification Train Epoch: 23 [19200/60000 (32%)]	Loss: 0.038108, KL fake Loss: 0.582877
Classification Train Epoch: 23 [25600/60000 (43%)]	Loss: 0.014864, KL fake Loss: 4.859203
Classification Train Epoch: 23 [32000/60000 (53%)]	Loss: 0.029946, KL fake Loss: 0.171149
Classification Train Epoch: 23 [38400/60000 (64%)]	Loss: 0.101850, KL fake Loss: 0.492978
Classification Train Epoch: 23 [44800/60000 (75%)]	Loss: 0.023393, KL fake Loss: 1.042990
Classification Train Epoch: 23 [51200/60000 (85%)]	Loss: 0.000540, KL fake Loss: 0.773352
Classification Train Epoch: 23 [57600/60000 (96%)]	Loss: 0.001108, KL fake Loss: 0.188304

Test set: Average loss: 0.3872, Accuracy: 9948/10000 (99%)

Classification Train Epoch: 24 [0/60000 (0%)]	Loss: 0.002107, KL fake Loss: 1.680105
Classification Train Epoch: 24 [6400/60000 (11%)]	Loss: 0.001468, KL fake Loss: 0.874911
Classification Train Epoch: 24 [12800/60000 (21%)]	Loss: 0.002445, KL fake Loss: 2.922392
Classification Train Epoch: 24 [19200/60000 (32%)]	Loss: 0.033151, KL fake Loss: 2.388950
Classification Train Epoch: 24 [25600/60000 (43%)]	Loss: 0.023107, KL fake Loss: 0.329432
Classification Train Epoch: 24 [32000/60000 (53%)]	Loss: 0.010481, KL fake Loss: 0.894045
Classification Train Epoch: 24 [38400/60000 (64%)]	Loss: 0.001684, KL fake Loss: 0.450969
Classification Train Epoch: 24 [44800/60000 (75%)]	Loss: 0.004618, KL fake Loss: 0.145800
Classification Train Epoch: 24 [51200/60000 (85%)]	Loss: 0.000566, KL fake Loss: 0.133638
Classification Train Epoch: 24 [57600/60000 (96%)]	Loss: 0.000735, KL fake Loss: 0.121606

Test set: Average loss: 0.6005, Accuracy: 9949/10000 (99%)

Classification Train Epoch: 25 [0/60000 (0%)]	Loss: 0.010231, KL fake Loss: 1.540522
Classification Train Epoch: 25 [6400/60000 (11%)]	Loss: 0.000682, KL fake Loss: 3.310886
Classification Train Epoch: 25 [12800/60000 (21%)]	Loss: 0.013013, KL fake Loss: 2.481682
Classification Train Epoch: 25 [19200/60000 (32%)]	Loss: 0.002347, KL fake Loss: 0.176463
Classification Train Epoch: 25 [25600/60000 (43%)]	Loss: 0.000493, KL fake Loss: 0.968085
Classification Train Epoch: 25 [32000/60000 (53%)]	Loss: 0.001051, KL fake Loss: 0.285070
Classification Train Epoch: 25 [38400/60000 (64%)]	Loss: 0.006585, KL fake Loss: 0.879727
Classification Train Epoch: 25 [44800/60000 (75%)]	Loss: 0.001666, KL fake Loss: 0.185168
Classification Train Epoch: 25 [51200/60000 (85%)]	Loss: 0.154360, KL fake Loss: 0.465479
Classification Train Epoch: 25 [57600/60000 (96%)]	Loss: 0.038624, KL fake Loss: 0.238743

Test set: Average loss: 0.5832, Accuracy: 9931/10000 (99%)

Classification Train Epoch: 26 [0/60000 (0%)]	Loss: 0.041175, KL fake Loss: 0.244342
Classification Train Epoch: 26 [6400/60000 (11%)]	Loss: 0.030000, KL fake Loss: 0.640814
Classification Train Epoch: 26 [12800/60000 (21%)]	Loss: 0.001694, KL fake Loss: 0.343266
Classification Train Epoch: 26 [19200/60000 (32%)]	Loss: 0.004818, KL fake Loss: 0.560519
Classification Train Epoch: 26 [25600/60000 (43%)]	Loss: 0.000569, KL fake Loss: 0.102372
Classification Train Epoch: 26 [32000/60000 (53%)]	Loss: 0.002190, KL fake Loss: 0.324652
Classification Train Epoch: 26 [38400/60000 (64%)]	Loss: 0.095504, KL fake Loss: 0.594668
Classification Train Epoch: 26 [44800/60000 (75%)]	Loss: 0.008124, KL fake Loss: 0.250071
 26%|██▌       | 26/100 [1:31:28<4:20:22, 211.11s/it] 27%|██▋       | 27/100 [1:34:59<4:16:50, 211.11s/it] 28%|██▊       | 28/100 [1:38:30<4:13:19, 211.11s/it] 29%|██▉       | 29/100 [1:42:01<4:09:48, 211.10s/it] 30%|███       | 30/100 [1:45:33<4:06:17, 211.10s/it] 31%|███       | 31/100 [1:49:04<4:02:46, 211.10s/it] 32%|███▏      | 32/100 [1:52:35<3:59:15, 211.11s/it] 33%|███▎      | 33/100 [1:56:06<3:55:44, 211.11s/it] 34%|███▍      | 34/100 [1:59:37<3:52:12, 211.10s/it]Classification Train Epoch: 26 [51200/60000 (85%)]	Loss: 0.002937, KL fake Loss: 0.638926
Classification Train Epoch: 26 [57600/60000 (96%)]	Loss: 0.013582, KL fake Loss: 0.460970

Test set: Average loss: 0.9106, Accuracy: 9916/10000 (99%)

Classification Train Epoch: 27 [0/60000 (0%)]	Loss: 0.017691, KL fake Loss: 0.807453
Classification Train Epoch: 27 [6400/60000 (11%)]	Loss: 0.001230, KL fake Loss: 0.251823
Classification Train Epoch: 27 [12800/60000 (21%)]	Loss: 0.187086, KL fake Loss: 1.080501
Classification Train Epoch: 27 [19200/60000 (32%)]	Loss: 0.000653, KL fake Loss: 4.683989
Classification Train Epoch: 27 [25600/60000 (43%)]	Loss: 0.000988, KL fake Loss: 0.163865
Classification Train Epoch: 27 [32000/60000 (53%)]	Loss: 0.007342, KL fake Loss: 1.019120
Classification Train Epoch: 27 [38400/60000 (64%)]	Loss: 0.004333, KL fake Loss: 0.603740
Classification Train Epoch: 27 [44800/60000 (75%)]	Loss: 0.000850, KL fake Loss: 0.228532
Classification Train Epoch: 27 [51200/60000 (85%)]	Loss: 0.004126, KL fake Loss: 0.150473
Classification Train Epoch: 27 [57600/60000 (96%)]	Loss: 0.000455, KL fake Loss: 0.158736

Test set: Average loss: 1.0542, Accuracy: 9720/10000 (97%)

Classification Train Epoch: 28 [0/60000 (0%)]	Loss: 0.001954, KL fake Loss: 0.164512
Classification Train Epoch: 28 [6400/60000 (11%)]	Loss: 0.001122, KL fake Loss: 0.242141
Classification Train Epoch: 28 [12800/60000 (21%)]	Loss: 0.029159, KL fake Loss: 0.233123
Classification Train Epoch: 28 [19200/60000 (32%)]	Loss: 0.007787, KL fake Loss: 0.088220
Classification Train Epoch: 28 [25600/60000 (43%)]	Loss: 0.069836, KL fake Loss: 0.188051
Classification Train Epoch: 28 [32000/60000 (53%)]	Loss: 0.020365, KL fake Loss: 5.969171
Classification Train Epoch: 28 [38400/60000 (64%)]	Loss: 0.000461, KL fake Loss: 1.804807
Classification Train Epoch: 28 [44800/60000 (75%)]	Loss: 0.077641, KL fake Loss: 0.775721
Classification Train Epoch: 28 [51200/60000 (85%)]	Loss: 0.013938, KL fake Loss: 0.221001
Classification Train Epoch: 28 [57600/60000 (96%)]	Loss: 0.013871, KL fake Loss: 1.923689

Test set: Average loss: 0.7613, Accuracy: 9940/10000 (99%)

Classification Train Epoch: 29 [0/60000 (0%)]	Loss: 0.000773, KL fake Loss: 0.238592
Classification Train Epoch: 29 [6400/60000 (11%)]	Loss: 0.001075, KL fake Loss: 0.544975
Classification Train Epoch: 29 [12800/60000 (21%)]	Loss: 0.013498, KL fake Loss: 0.377331
Classification Train Epoch: 29 [19200/60000 (32%)]	Loss: 0.004922, KL fake Loss: 0.291155
Classification Train Epoch: 29 [25600/60000 (43%)]	Loss: 0.001229, KL fake Loss: 0.106851
Classification Train Epoch: 29 [32000/60000 (53%)]	Loss: 0.001420, KL fake Loss: 0.067972
Classification Train Epoch: 29 [38400/60000 (64%)]	Loss: 0.000326, KL fake Loss: 0.141015
Classification Train Epoch: 29 [44800/60000 (75%)]	Loss: 0.004219, KL fake Loss: 0.089120
Classification Train Epoch: 29 [51200/60000 (85%)]	Loss: 0.007140, KL fake Loss: 1.797807
Classification Train Epoch: 29 [57600/60000 (96%)]	Loss: 0.000700, KL fake Loss: 0.106048

Test set: Average loss: 0.2389, Accuracy: 9836/10000 (98%)

Classification Train Epoch: 30 [0/60000 (0%)]	Loss: 0.002618, KL fake Loss: 0.367528
Classification Train Epoch: 30 [6400/60000 (11%)]	Loss: 0.019391, KL fake Loss: 2.584059
Classification Train Epoch: 30 [12800/60000 (21%)]	Loss: 0.004783, KL fake Loss: 4.572069
Classification Train Epoch: 30 [19200/60000 (32%)]	Loss: 0.001162, KL fake Loss: 0.280674
Classification Train Epoch: 30 [25600/60000 (43%)]	Loss: 0.001596, KL fake Loss: 3.378507
Classification Train Epoch: 30 [32000/60000 (53%)]	Loss: 0.004353, KL fake Loss: 0.297212
Classification Train Epoch: 30 [38400/60000 (64%)]	Loss: 0.000887, KL fake Loss: 0.140365
Classification Train Epoch: 30 [44800/60000 (75%)]	Loss: 0.004407, KL fake Loss: 0.663165
Classification Train Epoch: 30 [51200/60000 (85%)]	Loss: 0.055335, KL fake Loss: 3.192380
Classification Train Epoch: 30 [57600/60000 (96%)]	Loss: 0.002661, KL fake Loss: 0.256695

Test set: Average loss: 0.4231, Accuracy: 9953/10000 (100%)

Classification Train Epoch: 31 [0/60000 (0%)]	Loss: 0.006616, KL fake Loss: 0.198913
Classification Train Epoch: 31 [6400/60000 (11%)]	Loss: 0.006721, KL fake Loss: 0.494443
Classification Train Epoch: 31 [12800/60000 (21%)]	Loss: 0.015322, KL fake Loss: 0.240522
Classification Train Epoch: 31 [19200/60000 (32%)]	Loss: 0.000831, KL fake Loss: 0.083936
Classification Train Epoch: 31 [25600/60000 (43%)]	Loss: 0.002681, KL fake Loss: 1.483582
Classification Train Epoch: 31 [32000/60000 (53%)]	Loss: 0.002548, KL fake Loss: 5.455667
Classification Train Epoch: 31 [38400/60000 (64%)]	Loss: 0.016229, KL fake Loss: 0.675674
Classification Train Epoch: 31 [44800/60000 (75%)]	Loss: 0.011599, KL fake Loss: 0.210501
Classification Train Epoch: 31 [51200/60000 (85%)]	Loss: 0.053170, KL fake Loss: 6.917431
Classification Train Epoch: 31 [57600/60000 (96%)]	Loss: 0.001742, KL fake Loss: 0.264918

Test set: Average loss: 0.3846, Accuracy: 9933/10000 (99%)

Classification Train Epoch: 32 [0/60000 (0%)]	Loss: 0.000608, KL fake Loss: 0.146168
Classification Train Epoch: 32 [6400/60000 (11%)]	Loss: 0.001454, KL fake Loss: 0.185502
Classification Train Epoch: 32 [12800/60000 (21%)]	Loss: 0.001324, KL fake Loss: 1.019884
Classification Train Epoch: 32 [19200/60000 (32%)]	Loss: 0.000719, KL fake Loss: 0.209319
Classification Train Epoch: 32 [25600/60000 (43%)]	Loss: 0.005774, KL fake Loss: 0.225969
Classification Train Epoch: 32 [32000/60000 (53%)]	Loss: 0.005014, KL fake Loss: 2.568811
Classification Train Epoch: 32 [38400/60000 (64%)]	Loss: 0.032794, KL fake Loss: 2.137292
Classification Train Epoch: 32 [44800/60000 (75%)]	Loss: 0.002433, KL fake Loss: 1.418643
Classification Train Epoch: 32 [51200/60000 (85%)]	Loss: 0.001750, KL fake Loss: 4.212316
Classification Train Epoch: 32 [57600/60000 (96%)]	Loss: 0.011989, KL fake Loss: 0.130855

Test set: Average loss: 0.3765, Accuracy: 9952/10000 (100%)

Classification Train Epoch: 33 [0/60000 (0%)]	Loss: 0.000847, KL fake Loss: 0.296288
Classification Train Epoch: 33 [6400/60000 (11%)]	Loss: 0.012560, KL fake Loss: 2.331400
Classification Train Epoch: 33 [12800/60000 (21%)]	Loss: 0.000834, KL fake Loss: 0.328858
Classification Train Epoch: 33 [19200/60000 (32%)]	Loss: 0.108366, KL fake Loss: 0.152060
Classification Train Epoch: 33 [25600/60000 (43%)]	Loss: 0.000415, KL fake Loss: 0.099647
Classification Train Epoch: 33 [32000/60000 (53%)]	Loss: 0.000971, KL fake Loss: 5.627315
Classification Train Epoch: 33 [38400/60000 (64%)]	Loss: 0.024257, KL fake Loss: 0.155904
Classification Train Epoch: 33 [44800/60000 (75%)]	Loss: 0.010072, KL fake Loss: 3.674525
Classification Train Epoch: 33 [51200/60000 (85%)]	Loss: 0.002493, KL fake Loss: 4.568754
Classification Train Epoch: 33 [57600/60000 (96%)]	Loss: 0.000577, KL fake Loss: 0.117016

Test set: Average loss: 0.7536, Accuracy: 9288/10000 (93%)

Classification Train Epoch: 34 [0/60000 (0%)]	Loss: 0.017229, KL fake Loss: 0.097262
Classification Train Epoch: 34 [6400/60000 (11%)]	Loss: 0.000592, KL fake Loss: 0.407881
Classification Train Epoch: 34 [12800/60000 (21%)]	Loss: 0.011713, KL fake Loss: 0.197055
Classification Train Epoch: 34 [19200/60000 (32%)]	Loss: 0.001807, KL fake Loss: 0.115879
Classification Train Epoch: 34 [25600/60000 (43%)]	Loss: 0.015863, KL fake Loss: 0.116741
Classification Train Epoch: 34 [32000/60000 (53%)]	Loss: 0.001747, KL fake Loss: 0.307757
Classification Train Epoch: 34 [38400/60000 (64%)]	Loss: 0.003777, KL fake Loss: 0.119814
Classification Train Epoch: 34 [44800/60000 (75%)]	Loss: 0.000297, KL fake Loss: 0.267702
Classification Train Epoch: 34 [51200/60000 (85%)]	Loss: 0.027604, KL fake Loss: 1.325536
Classification Train Epoch: 34 [57600/60000 (96%)]	Loss: 0.000387, KL fake Loss: 0.079412

Test set: Average loss: 0.8333, Accuracy: 9957/10000 (100%)

Classification Train Epoch: 35 [0/60000 (0%)]	Loss: 0.000531, KL fake Loss: 0.104071
Classification Train Epoch: 35 [6400/60000 (11%)]	Loss: 0.014258, KL fake Loss: 0.167079
Classification Train Epoch: 35 [12800/60000 (21%)]	Loss: 0.009781, KL fake Loss: 0.107526
 35%|███▌      | 35/100 [2:03:08<3:48:41, 211.11s/it] 36%|███▌      | 36/100 [2:06:39<3:45:10, 211.11s/it] 37%|███▋      | 37/100 [2:10:10<3:41:39, 211.10s/it] 38%|███▊      | 38/100 [2:13:41<3:38:08, 211.10s/it] 39%|███▉      | 39/100 [2:17:13<3:34:37, 211.10s/it] 40%|████      | 40/100 [2:20:44<3:31:08, 211.14s/it] 41%|████      | 41/100 [2:24:15<3:27:36, 211.13s/it] 42%|████▏     | 42/100 [2:27:46<3:24:04, 211.12s/it]Classification Train Epoch: 35 [19200/60000 (32%)]	Loss: 0.000476, KL fake Loss: 0.714047
Classification Train Epoch: 35 [25600/60000 (43%)]	Loss: 0.002381, KL fake Loss: 0.256685
Classification Train Epoch: 35 [32000/60000 (53%)]	Loss: 0.004311, KL fake Loss: 3.638009
Classification Train Epoch: 35 [38400/60000 (64%)]	Loss: 0.010428, KL fake Loss: 0.362864
Classification Train Epoch: 35 [44800/60000 (75%)]	Loss: 0.000400, KL fake Loss: 0.065474
Classification Train Epoch: 35 [51200/60000 (85%)]	Loss: 0.000336, KL fake Loss: 0.066221
Classification Train Epoch: 35 [57600/60000 (96%)]	Loss: 0.007977, KL fake Loss: 0.083962

Test set: Average loss: 1.3759, Accuracy: 8701/10000 (87%)

Classification Train Epoch: 36 [0/60000 (0%)]	Loss: 0.003334, KL fake Loss: 0.084622
Classification Train Epoch: 36 [6400/60000 (11%)]	Loss: 0.001134, KL fake Loss: 0.091634
Classification Train Epoch: 36 [12800/60000 (21%)]	Loss: 0.000483, KL fake Loss: 0.095170
Classification Train Epoch: 36 [19200/60000 (32%)]	Loss: 0.016817, KL fake Loss: 0.072799
Classification Train Epoch: 36 [25600/60000 (43%)]	Loss: 0.000663, KL fake Loss: 0.268178
Classification Train Epoch: 36 [32000/60000 (53%)]	Loss: 0.014366, KL fake Loss: 0.095964
Classification Train Epoch: 36 [38400/60000 (64%)]	Loss: 0.004168, KL fake Loss: 0.778274
Classification Train Epoch: 36 [44800/60000 (75%)]	Loss: 0.000535, KL fake Loss: 0.135864
Classification Train Epoch: 36 [51200/60000 (85%)]	Loss: 0.027634, KL fake Loss: 0.249160
Classification Train Epoch: 36 [57600/60000 (96%)]	Loss: 0.000274, KL fake Loss: 1.758891

Test set: Average loss: 1.3747, Accuracy: 9225/10000 (92%)

Classification Train Epoch: 37 [0/60000 (0%)]	Loss: 0.008165, KL fake Loss: 0.058936
Classification Train Epoch: 37 [6400/60000 (11%)]	Loss: 0.003823, KL fake Loss: 1.762741
Classification Train Epoch: 37 [12800/60000 (21%)]	Loss: 0.008749, KL fake Loss: 0.109713
Classification Train Epoch: 37 [19200/60000 (32%)]	Loss: 0.000492, KL fake Loss: 0.112197
Classification Train Epoch: 37 [25600/60000 (43%)]	Loss: 0.000589, KL fake Loss: 0.229367
Classification Train Epoch: 37 [32000/60000 (53%)]	Loss: 0.000448, KL fake Loss: 0.447467
Classification Train Epoch: 37 [38400/60000 (64%)]	Loss: 0.000500, KL fake Loss: 1.566075
Classification Train Epoch: 37 [44800/60000 (75%)]	Loss: 0.018560, KL fake Loss: 0.147862
Classification Train Epoch: 37 [51200/60000 (85%)]	Loss: 0.000582, KL fake Loss: 1.192035
Classification Train Epoch: 37 [57600/60000 (96%)]	Loss: 0.002520, KL fake Loss: 1.149810

Test set: Average loss: 0.5894, Accuracy: 9858/10000 (99%)

Classification Train Epoch: 38 [0/60000 (0%)]	Loss: 0.001037, KL fake Loss: 0.465063
Classification Train Epoch: 38 [6400/60000 (11%)]	Loss: 0.000802, KL fake Loss: 0.430024
Classification Train Epoch: 38 [12800/60000 (21%)]	Loss: 0.001903, KL fake Loss: 0.058179
Classification Train Epoch: 38 [19200/60000 (32%)]	Loss: 0.001372, KL fake Loss: 1.545529
Classification Train Epoch: 38 [25600/60000 (43%)]	Loss: 0.001191, KL fake Loss: 1.023809
Classification Train Epoch: 38 [32000/60000 (53%)]	Loss: 0.002688, KL fake Loss: 1.304352
Classification Train Epoch: 38 [38400/60000 (64%)]	Loss: 0.001823, KL fake Loss: 0.163636
Classification Train Epoch: 38 [44800/60000 (75%)]	Loss: 0.000649, KL fake Loss: 0.073341
Classification Train Epoch: 38 [51200/60000 (85%)]	Loss: 0.000469, KL fake Loss: 0.068427
Classification Train Epoch: 38 [57600/60000 (96%)]	Loss: 0.015922, KL fake Loss: 0.281122

Test set: Average loss: 1.0209, Accuracy: 8714/10000 (87%)

Classification Train Epoch: 39 [0/60000 (0%)]	Loss: 0.000741, KL fake Loss: 0.114902
Classification Train Epoch: 39 [6400/60000 (11%)]	Loss: 0.000956, KL fake Loss: 0.081062
Classification Train Epoch: 39 [12800/60000 (21%)]	Loss: 0.000225, KL fake Loss: 0.143111
Classification Train Epoch: 39 [19200/60000 (32%)]	Loss: 0.000448, KL fake Loss: 0.545590
Classification Train Epoch: 39 [25600/60000 (43%)]	Loss: 0.030847, KL fake Loss: 0.092286
Classification Train Epoch: 39 [32000/60000 (53%)]	Loss: 0.000329, KL fake Loss: 0.059927
Classification Train Epoch: 39 [38400/60000 (64%)]	Loss: 0.044942, KL fake Loss: 0.060308
Classification Train Epoch: 39 [44800/60000 (75%)]	Loss: 0.000698, KL fake Loss: 3.248132
Classification Train Epoch: 39 [51200/60000 (85%)]	Loss: 0.000757, KL fake Loss: 2.892455
Classification Train Epoch: 39 [57600/60000 (96%)]	Loss: 0.000576, KL fake Loss: 0.313951

Test set: Average loss: 0.8192, Accuracy: 8098/10000 (81%)

Classification Train Epoch: 40 [0/60000 (0%)]	Loss: 0.001317, KL fake Loss: 0.233151
Classification Train Epoch: 40 [6400/60000 (11%)]	Loss: 0.067264, KL fake Loss: 7.499788
Classification Train Epoch: 40 [12800/60000 (21%)]	Loss: 0.001036, KL fake Loss: 0.268913
Classification Train Epoch: 40 [19200/60000 (32%)]	Loss: 0.106463, KL fake Loss: 0.267930
Classification Train Epoch: 40 [25600/60000 (43%)]	Loss: 0.000790, KL fake Loss: 0.055092
Classification Train Epoch: 40 [32000/60000 (53%)]	Loss: 0.006917, KL fake Loss: 0.634601
Classification Train Epoch: 40 [38400/60000 (64%)]	Loss: 0.012808, KL fake Loss: 0.111901
Classification Train Epoch: 40 [44800/60000 (75%)]	Loss: 0.014643, KL fake Loss: 0.077378
Classification Train Epoch: 40 [51200/60000 (85%)]	Loss: 0.000344, KL fake Loss: 0.043341
Classification Train Epoch: 40 [57600/60000 (96%)]	Loss: 0.000134, KL fake Loss: 0.030439

Test set: Average loss: 1.3816, Accuracy: 7652/10000 (77%)

Classification Train Epoch: 41 [0/60000 (0%)]	Loss: 0.001611, KL fake Loss: 0.049914
Classification Train Epoch: 41 [6400/60000 (11%)]	Loss: 0.000352, KL fake Loss: 0.137958
Classification Train Epoch: 41 [12800/60000 (21%)]	Loss: 0.000594, KL fake Loss: 0.132089
Classification Train Epoch: 41 [19200/60000 (32%)]	Loss: 0.063879, KL fake Loss: 0.229087
Classification Train Epoch: 41 [25600/60000 (43%)]	Loss: 0.003449, KL fake Loss: 7.100060
Classification Train Epoch: 41 [32000/60000 (53%)]	Loss: 0.025847, KL fake Loss: 3.207075
Classification Train Epoch: 41 [38400/60000 (64%)]	Loss: 0.002640, KL fake Loss: 0.499606
Classification Train Epoch: 41 [44800/60000 (75%)]	Loss: 0.012170, KL fake Loss: 0.078036
Classification Train Epoch: 41 [51200/60000 (85%)]	Loss: 0.000396, KL fake Loss: 0.091996
Classification Train Epoch: 41 [57600/60000 (96%)]	Loss: 0.000815, KL fake Loss: 0.118029

Test set: Average loss: 0.7108, Accuracy: 8767/10000 (88%)

Classification Train Epoch: 42 [0/60000 (0%)]	Loss: 0.000802, KL fake Loss: 0.088354
Classification Train Epoch: 42 [6400/60000 (11%)]	Loss: 0.000498, KL fake Loss: 0.072791
Classification Train Epoch: 42 [12800/60000 (21%)]	Loss: 0.004286, KL fake Loss: 0.108033
Classification Train Epoch: 42 [19200/60000 (32%)]	Loss: 0.000317, KL fake Loss: 0.062972
Classification Train Epoch: 42 [25600/60000 (43%)]	Loss: 0.000374, KL fake Loss: 0.817581
Classification Train Epoch: 42 [32000/60000 (53%)]	Loss: 0.010209, KL fake Loss: 0.166075
Classification Train Epoch: 42 [38400/60000 (64%)]	Loss: 0.001524, KL fake Loss: 0.085028
Classification Train Epoch: 42 [44800/60000 (75%)]	Loss: 0.000403, KL fake Loss: 0.059483
Classification Train Epoch: 42 [51200/60000 (85%)]	Loss: 0.002974, KL fake Loss: 0.153011
Classification Train Epoch: 42 [57600/60000 (96%)]	Loss: 0.000642, KL fake Loss: 0.066987

Test set: Average loss: 1.2470, Accuracy: 6805/10000 (68%)

Classification Train Epoch: 43 [0/60000 (0%)]	Loss: 0.000261, KL fake Loss: 0.566659
Classification Train Epoch: 43 [6400/60000 (11%)]	Loss: 0.001807, KL fake Loss: 0.071657
Classification Train Epoch: 43 [12800/60000 (21%)]	Loss: 0.000872, KL fake Loss: 0.255383
Classification Train Epoch: 43 [19200/60000 (32%)]	Loss: 0.000510, KL fake Loss: 0.036429
Classification Train Epoch: 43 [25600/60000 (43%)]	Loss: 0.074756, KL fake Loss: 1.321463
Classification Train Epoch: 43 [32000/60000 (53%)]	Loss: 0.000494, KL fake Loss: 0.465103
Classification Train Epoch: 43 [38400/60000 (64%)]	Loss: 0.004157, KL fake Loss: 0.227027
Classification Train Epoch: 43 [44800/60000 (75%)]	Loss: 0.000805, KL fake Loss: 2.006678
Classification Train Epoch: 43 [51200/60000 (85%)]	Loss: 0.007755, KL fake Loss: 0.340219
 43%|████▎     | 43/100 [2:31:17<3:20:33, 211.11s/it] 44%|████▍     | 44/100 [2:34:48<3:17:02, 211.11s/it] 45%|████▌     | 45/100 [2:38:19<3:13:30, 211.11s/it] 46%|████▌     | 46/100 [2:41:50<3:09:59, 211.10s/it] 47%|████▋     | 47/100 [2:45:21<3:06:28, 211.10s/it] 48%|████▊     | 48/100 [2:48:53<3:02:57, 211.10s/it] 49%|████▉     | 49/100 [2:52:24<2:59:25, 211.09s/it] 50%|█████     | 50/100 [2:55:55<2:55:54, 211.09s/it] 51%|█████     | 51/100 [2:59:26<2:52:23, 211.09s/it]Classification Train Epoch: 43 [57600/60000 (96%)]	Loss: 0.000720, KL fake Loss: 0.062198

Test set: Average loss: 1.3850, Accuracy: 8011/10000 (80%)

Classification Train Epoch: 44 [0/60000 (0%)]	Loss: 0.000479, KL fake Loss: 0.308881
Classification Train Epoch: 44 [6400/60000 (11%)]	Loss: 0.000269, KL fake Loss: 0.061907
Classification Train Epoch: 44 [12800/60000 (21%)]	Loss: 0.001300, KL fake Loss: 0.107778
Classification Train Epoch: 44 [19200/60000 (32%)]	Loss: 0.032978, KL fake Loss: 0.523412
Classification Train Epoch: 44 [25600/60000 (43%)]	Loss: 0.000444, KL fake Loss: 0.052584
Classification Train Epoch: 44 [32000/60000 (53%)]	Loss: 0.000192, KL fake Loss: 0.051243
Classification Train Epoch: 44 [38400/60000 (64%)]	Loss: 0.000285, KL fake Loss: 0.069414
Classification Train Epoch: 44 [44800/60000 (75%)]	Loss: 0.002518, KL fake Loss: 0.045784
Classification Train Epoch: 44 [51200/60000 (85%)]	Loss: 0.000645, KL fake Loss: 0.039453
Classification Train Epoch: 44 [57600/60000 (96%)]	Loss: 0.000248, KL fake Loss: 0.053349

Test set: Average loss: 1.0340, Accuracy: 7914/10000 (79%)

Classification Train Epoch: 45 [0/60000 (0%)]	Loss: 0.000767, KL fake Loss: 0.042661
Classification Train Epoch: 45 [6400/60000 (11%)]	Loss: 0.000349, KL fake Loss: 0.058752
Classification Train Epoch: 45 [12800/60000 (21%)]	Loss: 0.001364, KL fake Loss: 0.038681
Classification Train Epoch: 45 [19200/60000 (32%)]	Loss: 0.000240, KL fake Loss: 0.047474
Classification Train Epoch: 45 [25600/60000 (43%)]	Loss: 0.008131, KL fake Loss: 0.135590
Classification Train Epoch: 45 [32000/60000 (53%)]	Loss: 0.001170, KL fake Loss: 0.088617
Classification Train Epoch: 45 [38400/60000 (64%)]	Loss: 0.000164, KL fake Loss: 0.058611
Classification Train Epoch: 45 [44800/60000 (75%)]	Loss: 0.000799, KL fake Loss: 0.046756
Classification Train Epoch: 45 [51200/60000 (85%)]	Loss: 0.000315, KL fake Loss: 0.053787
Classification Train Epoch: 45 [57600/60000 (96%)]	Loss: 0.000557, KL fake Loss: 0.029223

Test set: Average loss: 1.0292, Accuracy: 8980/10000 (90%)

Classification Train Epoch: 46 [0/60000 (0%)]	Loss: 0.000273, KL fake Loss: 0.047096
Classification Train Epoch: 46 [6400/60000 (11%)]	Loss: 0.000225, KL fake Loss: 0.044022
Classification Train Epoch: 46 [12800/60000 (21%)]	Loss: 0.004045, KL fake Loss: 0.065226
Classification Train Epoch: 46 [19200/60000 (32%)]	Loss: 0.010559, KL fake Loss: 1.459424
Classification Train Epoch: 46 [25600/60000 (43%)]	Loss: 0.000622, KL fake Loss: 0.285122
Classification Train Epoch: 46 [32000/60000 (53%)]	Loss: 0.000536, KL fake Loss: 0.087290
Classification Train Epoch: 46 [38400/60000 (64%)]	Loss: 0.001041, KL fake Loss: 0.220338
Classification Train Epoch: 46 [44800/60000 (75%)]	Loss: 0.000229, KL fake Loss: 0.189624
Classification Train Epoch: 46 [51200/60000 (85%)]	Loss: 0.000656, KL fake Loss: 5.330632
Classification Train Epoch: 46 [57600/60000 (96%)]	Loss: 0.017334, KL fake Loss: 0.093342

Test set: Average loss: 1.5920, Accuracy: 8327/10000 (83%)

Classification Train Epoch: 47 [0/60000 (0%)]	Loss: 0.000430, KL fake Loss: 0.063852
Classification Train Epoch: 47 [6400/60000 (11%)]	Loss: 0.000645, KL fake Loss: 0.046239
Classification Train Epoch: 47 [12800/60000 (21%)]	Loss: 0.001131, KL fake Loss: 0.121487
Classification Train Epoch: 47 [19200/60000 (32%)]	Loss: 0.031143, KL fake Loss: 0.641468
Classification Train Epoch: 47 [25600/60000 (43%)]	Loss: 0.000560, KL fake Loss: 0.115960
Classification Train Epoch: 47 [32000/60000 (53%)]	Loss: 0.001242, KL fake Loss: 0.183130
Classification Train Epoch: 47 [38400/60000 (64%)]	Loss: 0.001267, KL fake Loss: 0.048847
Classification Train Epoch: 47 [44800/60000 (75%)]	Loss: 0.000556, KL fake Loss: 0.102116
Classification Train Epoch: 47 [51200/60000 (85%)]	Loss: 0.001273, KL fake Loss: 0.055089
Classification Train Epoch: 47 [57600/60000 (96%)]	Loss: 0.000723, KL fake Loss: 0.844385

Test set: Average loss: 1.0776, Accuracy: 9392/10000 (94%)

Classification Train Epoch: 48 [0/60000 (0%)]	Loss: 0.000501, KL fake Loss: 0.044794
Classification Train Epoch: 48 [6400/60000 (11%)]	Loss: 0.000422, KL fake Loss: 0.047434
Classification Train Epoch: 48 [12800/60000 (21%)]	Loss: 0.000118, KL fake Loss: 0.044210
Classification Train Epoch: 48 [19200/60000 (32%)]	Loss: 0.000475, KL fake Loss: 0.041974
Classification Train Epoch: 48 [25600/60000 (43%)]	Loss: 0.000098, KL fake Loss: 0.034613
Classification Train Epoch: 48 [32000/60000 (53%)]	Loss: 0.000533, KL fake Loss: 0.034206
Classification Train Epoch: 48 [38400/60000 (64%)]	Loss: 0.000734, KL fake Loss: 0.037828
Classification Train Epoch: 48 [44800/60000 (75%)]	Loss: 0.000711, KL fake Loss: 0.101542
Classification Train Epoch: 48 [51200/60000 (85%)]	Loss: 0.000641, KL fake Loss: 4.252737
Classification Train Epoch: 48 [57600/60000 (96%)]	Loss: 0.000464, KL fake Loss: 0.081238

Test set: Average loss: 0.9949, Accuracy: 6867/10000 (69%)

Classification Train Epoch: 49 [0/60000 (0%)]	Loss: 0.001264, KL fake Loss: 0.047028
Classification Train Epoch: 49 [6400/60000 (11%)]	Loss: 0.014275, KL fake Loss: 0.132212
Classification Train Epoch: 49 [12800/60000 (21%)]	Loss: 0.000789, KL fake Loss: 0.238845
Classification Train Epoch: 49 [19200/60000 (32%)]	Loss: 0.000377, KL fake Loss: 0.056984
Classification Train Epoch: 49 [25600/60000 (43%)]	Loss: 0.130951, KL fake Loss: 0.202580
Classification Train Epoch: 49 [32000/60000 (53%)]	Loss: 0.000078, KL fake Loss: 0.556703
Classification Train Epoch: 49 [38400/60000 (64%)]	Loss: 0.000226, KL fake Loss: 0.049285
Classification Train Epoch: 49 [44800/60000 (75%)]	Loss: 0.000377, KL fake Loss: 0.255141
Classification Train Epoch: 49 [51200/60000 (85%)]	Loss: 0.042467, KL fake Loss: 0.081599
Classification Train Epoch: 49 [57600/60000 (96%)]	Loss: 0.000664, KL fake Loss: 0.077156

Test set: Average loss: 1.0607, Accuracy: 6614/10000 (66%)

Classification Train Epoch: 50 [0/60000 (0%)]	Loss: 0.000194, KL fake Loss: 0.058666
Classification Train Epoch: 50 [6400/60000 (11%)]	Loss: 0.001958, KL fake Loss: 0.042038
Classification Train Epoch: 50 [12800/60000 (21%)]	Loss: 0.000350, KL fake Loss: 0.070197
Classification Train Epoch: 50 [19200/60000 (32%)]	Loss: 0.000453, KL fake Loss: 0.042739
Classification Train Epoch: 50 [25600/60000 (43%)]	Loss: 0.000467, KL fake Loss: 0.051765
Classification Train Epoch: 50 [32000/60000 (53%)]	Loss: 0.000220, KL fake Loss: 0.165264
Classification Train Epoch: 50 [38400/60000 (64%)]	Loss: 0.000481, KL fake Loss: 0.055159
Classification Train Epoch: 50 [44800/60000 (75%)]	Loss: 0.000847, KL fake Loss: 0.053236
Classification Train Epoch: 50 [51200/60000 (85%)]	Loss: 0.001351, KL fake Loss: 0.049627
Classification Train Epoch: 50 [57600/60000 (96%)]	Loss: 0.000215, KL fake Loss: 0.047120

Test set: Average loss: 0.5045, Accuracy: 9852/10000 (99%)

Classification Train Epoch: 51 [0/60000 (0%)]	Loss: 0.045672, KL fake Loss: 0.227583
Classification Train Epoch: 51 [6400/60000 (11%)]	Loss: 0.001097, KL fake Loss: 0.058853
Classification Train Epoch: 51 [12800/60000 (21%)]	Loss: 0.001768, KL fake Loss: 0.369946
Classification Train Epoch: 51 [19200/60000 (32%)]	Loss: 0.001032, KL fake Loss: 0.049732
Classification Train Epoch: 51 [25600/60000 (43%)]	Loss: 0.000348, KL fake Loss: 0.078896
Classification Train Epoch: 51 [32000/60000 (53%)]	Loss: 0.023589, KL fake Loss: 0.038657
Classification Train Epoch: 51 [38400/60000 (64%)]	Loss: 0.000753, KL fake Loss: 0.049647
Classification Train Epoch: 51 [44800/60000 (75%)]	Loss: 0.002144, KL fake Loss: 0.041827
Classification Train Epoch: 51 [51200/60000 (85%)]	Loss: 0.041456, KL fake Loss: 0.046811
Classification Train Epoch: 51 [57600/60000 (96%)]	Loss: 0.000970, KL fake Loss: 1.276275

Test set: Average loss: 0.9125, Accuracy: 7722/10000 (77%)

Classification Train Epoch: 52 [0/60000 (0%)]	Loss: 0.000202, KL fake Loss: 0.044312
Classification Train Epoch: 52 [6400/60000 (11%)]	Loss: 0.000293, KL fake Loss: 0.065167
Classification Train Epoch: 52 [12800/60000 (21%)]	Loss: 0.000582, KL fake Loss: 0.140047
Classification Train Epoch: 52 [19200/60000 (32%)]	Loss: 0.000552, KL fake Loss: 1.419792
 52%|█████▏    | 52/100 [3:02:57<2:48:52, 211.09s/it] 53%|█████▎    | 53/100 [3:06:28<2:45:21, 211.09s/it] 54%|█████▍    | 54/100 [3:09:59<2:41:50, 211.09s/it] 55%|█████▌    | 55/100 [3:13:30<2:38:19, 211.09s/it] 56%|█████▌    | 56/100 [3:17:01<2:34:47, 211.09s/it] 57%|█████▋    | 57/100 [3:20:32<2:31:16, 211.09s/it] 58%|█████▊    | 58/100 [3:24:03<2:27:45, 211.09s/it] 59%|█████▉    | 59/100 [3:27:35<2:24:14, 211.09s/it]Classification Train Epoch: 52 [25600/60000 (43%)]	Loss: 0.003497, KL fake Loss: 0.053656
Classification Train Epoch: 52 [32000/60000 (53%)]	Loss: 0.025911, KL fake Loss: 0.027844
Classification Train Epoch: 52 [38400/60000 (64%)]	Loss: 0.002268, KL fake Loss: 0.074008
Classification Train Epoch: 52 [44800/60000 (75%)]	Loss: 0.005871, KL fake Loss: 8.040369
Classification Train Epoch: 52 [51200/60000 (85%)]	Loss: 0.003856, KL fake Loss: 0.096275
Classification Train Epoch: 52 [57600/60000 (96%)]	Loss: 0.000328, KL fake Loss: 4.752312

Test set: Average loss: 1.2606, Accuracy: 7310/10000 (73%)

Classification Train Epoch: 53 [0/60000 (0%)]	Loss: 0.000449, KL fake Loss: 0.029733
Classification Train Epoch: 53 [6400/60000 (11%)]	Loss: 0.000816, KL fake Loss: 0.113656
Classification Train Epoch: 53 [12800/60000 (21%)]	Loss: 0.000149, KL fake Loss: 0.027903
Classification Train Epoch: 53 [19200/60000 (32%)]	Loss: 0.058311, KL fake Loss: 0.383731
Classification Train Epoch: 53 [25600/60000 (43%)]	Loss: 0.001540, KL fake Loss: 0.358490
Classification Train Epoch: 53 [32000/60000 (53%)]	Loss: 0.000361, KL fake Loss: 0.180033
Classification Train Epoch: 53 [38400/60000 (64%)]	Loss: 0.001445, KL fake Loss: 0.050933
Classification Train Epoch: 53 [44800/60000 (75%)]	Loss: 0.014844, KL fake Loss: 0.154671
Classification Train Epoch: 53 [51200/60000 (85%)]	Loss: 0.000093, KL fake Loss: 0.033088
Classification Train Epoch: 53 [57600/60000 (96%)]	Loss: 0.000606, KL fake Loss: 0.053003

Test set: Average loss: 1.4226, Accuracy: 4831/10000 (48%)

Classification Train Epoch: 54 [0/60000 (0%)]	Loss: 0.019464, KL fake Loss: 0.033050
Classification Train Epoch: 54 [6400/60000 (11%)]	Loss: 0.001280, KL fake Loss: 8.541927
Classification Train Epoch: 54 [12800/60000 (21%)]	Loss: 0.036218, KL fake Loss: 0.076981
Classification Train Epoch: 54 [19200/60000 (32%)]	Loss: 0.019239, KL fake Loss: 0.052339
Classification Train Epoch: 54 [25600/60000 (43%)]	Loss: 0.000254, KL fake Loss: 0.215577
Classification Train Epoch: 54 [32000/60000 (53%)]	Loss: 0.000325, KL fake Loss: 0.040089
Classification Train Epoch: 54 [38400/60000 (64%)]	Loss: 0.016280, KL fake Loss: 0.337425
Classification Train Epoch: 54 [44800/60000 (75%)]	Loss: 0.000426, KL fake Loss: 0.033979
Classification Train Epoch: 54 [51200/60000 (85%)]	Loss: 0.000412, KL fake Loss: 1.676920
Classification Train Epoch: 54 [57600/60000 (96%)]	Loss: 0.000201, KL fake Loss: 0.048106

Test set: Average loss: 2.5230, Accuracy: 3076/10000 (31%)

Classification Train Epoch: 55 [0/60000 (0%)]	Loss: 0.000083, KL fake Loss: 0.054275
Classification Train Epoch: 55 [6400/60000 (11%)]	Loss: 0.000215, KL fake Loss: 0.055813
Classification Train Epoch: 55 [12800/60000 (21%)]	Loss: 0.000184, KL fake Loss: 0.059989
Classification Train Epoch: 55 [19200/60000 (32%)]	Loss: 0.000208, KL fake Loss: 0.033817
Classification Train Epoch: 55 [25600/60000 (43%)]	Loss: 0.000366, KL fake Loss: 0.090700
Classification Train Epoch: 55 [32000/60000 (53%)]	Loss: 0.041904, KL fake Loss: 0.491777
Classification Train Epoch: 55 [38400/60000 (64%)]	Loss: 0.002109, KL fake Loss: 0.098857
Classification Train Epoch: 55 [44800/60000 (75%)]	Loss: 0.000254, KL fake Loss: 0.032202
Classification Train Epoch: 55 [51200/60000 (85%)]	Loss: 0.003473, KL fake Loss: 0.047956
Classification Train Epoch: 55 [57600/60000 (96%)]	Loss: 0.000319, KL fake Loss: 0.024117

Test set: Average loss: 1.3609, Accuracy: 5561/10000 (56%)

Classification Train Epoch: 56 [0/60000 (0%)]	Loss: 0.000209, KL fake Loss: 0.040284
Classification Train Epoch: 56 [6400/60000 (11%)]	Loss: 0.000127, KL fake Loss: 0.030615
Classification Train Epoch: 56 [12800/60000 (21%)]	Loss: 0.000107, KL fake Loss: 0.023942
Classification Train Epoch: 56 [19200/60000 (32%)]	Loss: 0.000172, KL fake Loss: 0.084522
Classification Train Epoch: 56 [25600/60000 (43%)]	Loss: 0.000405, KL fake Loss: 1.252759
Classification Train Epoch: 56 [32000/60000 (53%)]	Loss: 0.000478, KL fake Loss: 7.364011
Classification Train Epoch: 56 [38400/60000 (64%)]	Loss: 0.008462, KL fake Loss: 0.074472
Classification Train Epoch: 56 [44800/60000 (75%)]	Loss: 0.044469, KL fake Loss: 0.046258
Classification Train Epoch: 56 [51200/60000 (85%)]	Loss: 0.001000, KL fake Loss: 0.756962
Classification Train Epoch: 56 [57600/60000 (96%)]	Loss: 0.000450, KL fake Loss: 0.917928

Test set: Average loss: 1.5340, Accuracy: 5499/10000 (55%)

Classification Train Epoch: 57 [0/60000 (0%)]	Loss: 0.007737, KL fake Loss: 0.042474
Classification Train Epoch: 57 [6400/60000 (11%)]	Loss: 0.001058, KL fake Loss: 0.033088
Classification Train Epoch: 57 [12800/60000 (21%)]	Loss: 0.013262, KL fake Loss: 0.027335
Classification Train Epoch: 57 [19200/60000 (32%)]	Loss: 0.000308, KL fake Loss: 0.055462
Classification Train Epoch: 57 [25600/60000 (43%)]	Loss: 0.000386, KL fake Loss: 0.051784
Classification Train Epoch: 57 [32000/60000 (53%)]	Loss: 0.001494, KL fake Loss: 0.142453
Classification Train Epoch: 57 [38400/60000 (64%)]	Loss: 0.000233, KL fake Loss: 0.033808
Classification Train Epoch: 57 [44800/60000 (75%)]	Loss: 0.000457, KL fake Loss: 0.077697
Classification Train Epoch: 57 [51200/60000 (85%)]	Loss: 0.002329, KL fake Loss: 0.503724
Classification Train Epoch: 57 [57600/60000 (96%)]	Loss: 0.032328, KL fake Loss: 7.368861

Test set: Average loss: 1.9020, Accuracy: 3370/10000 (34%)

Classification Train Epoch: 58 [0/60000 (0%)]	Loss: 0.001842, KL fake Loss: 0.171473
Classification Train Epoch: 58 [6400/60000 (11%)]	Loss: 0.000191, KL fake Loss: 0.741623
Classification Train Epoch: 58 [12800/60000 (21%)]	Loss: 0.014127, KL fake Loss: 7.387819
Classification Train Epoch: 58 [19200/60000 (32%)]	Loss: 0.002543, KL fake Loss: 0.717367
Classification Train Epoch: 58 [25600/60000 (43%)]	Loss: 0.000403, KL fake Loss: 0.077300
Classification Train Epoch: 58 [32000/60000 (53%)]	Loss: 0.002199, KL fake Loss: 0.032217
Classification Train Epoch: 58 [38400/60000 (64%)]	Loss: 0.000433, KL fake Loss: 0.064055
Classification Train Epoch: 58 [44800/60000 (75%)]	Loss: 0.000401, KL fake Loss: 0.050305
Classification Train Epoch: 58 [51200/60000 (85%)]	Loss: 0.000848, KL fake Loss: 0.033152
Classification Train Epoch: 58 [57600/60000 (96%)]	Loss: 0.035987, KL fake Loss: 0.335047

Test set: Average loss: 1.1256, Accuracy: 7680/10000 (77%)

Classification Train Epoch: 59 [0/60000 (0%)]	Loss: 0.007006, KL fake Loss: 7.524596
Classification Train Epoch: 59 [6400/60000 (11%)]	Loss: 0.000404, KL fake Loss: 0.746188
Classification Train Epoch: 59 [12800/60000 (21%)]	Loss: 0.001048, KL fake Loss: 0.100144
Classification Train Epoch: 59 [19200/60000 (32%)]	Loss: 0.000349, KL fake Loss: 0.039878
Classification Train Epoch: 59 [25600/60000 (43%)]	Loss: 0.000464, KL fake Loss: 0.025629
Classification Train Epoch: 59 [32000/60000 (53%)]	Loss: 0.000253, KL fake Loss: 0.059505
Classification Train Epoch: 59 [38400/60000 (64%)]	Loss: 0.000498, KL fake Loss: 0.028482
Classification Train Epoch: 59 [44800/60000 (75%)]	Loss: 0.000288, KL fake Loss: 0.875933
Classification Train Epoch: 59 [51200/60000 (85%)]	Loss: 0.000687, KL fake Loss: 0.026910
Classification Train Epoch: 59 [57600/60000 (96%)]	Loss: 0.001514, KL fake Loss: 0.031286

Test set: Average loss: 2.2685, Accuracy: 2789/10000 (28%)

Classification Train Epoch: 60 [0/60000 (0%)]	Loss: 0.052932, KL fake Loss: 0.028746
Classification Train Epoch: 60 [6400/60000 (11%)]	Loss: 0.000100, KL fake Loss: 0.577585
Classification Train Epoch: 60 [12800/60000 (21%)]	Loss: 0.004975, KL fake Loss: 0.054425
Classification Train Epoch: 60 [19200/60000 (32%)]	Loss: 0.000335, KL fake Loss: 0.039096
Classification Train Epoch: 60 [25600/60000 (43%)]	Loss: 0.010969, KL fake Loss: 0.312657
Classification Train Epoch: 60 [32000/60000 (53%)]	Loss: 0.000308, KL fake Loss: 0.032695
Classification Train Epoch: 60 [38400/60000 (64%)]	Loss: 0.038810, KL fake Loss: 0.119624
Classification Train Epoch: 60 [44800/60000 (75%)]	Loss: 0.000141, KL fake Loss: 0.034301
Classification Train Epoch: 60 [51200/60000 (85%)]	Loss: 0.000128, KL fake Loss: 0.026449
Classification Train Epoch: 60 [57600/60000 (96%)]	Loss: 0.000265, KL fake Loss: 0.045941
 60%|██████    | 60/100 [3:31:06<2:20:44, 211.12s/it] 61%|██████    | 61/100 [3:34:37<2:17:13, 211.12s/it] 62%|██████▏   | 62/100 [3:38:08<2:13:42, 211.11s/it] 63%|██████▎   | 63/100 [3:41:39<2:10:10, 211.11s/it] 64%|██████▍   | 64/100 [3:45:10<2:06:39, 211.11s/it] 65%|██████▌   | 65/100 [3:48:41<2:03:08, 211.10s/it] 66%|██████▌   | 66/100 [3:52:12<1:59:37, 211.10s/it] 67%|██████▋   | 67/100 [3:55:43<1:56:06, 211.10s/it] 68%|██████▊   | 68/100 [3:59:15<1:52:35, 211.10s/it]
Test set: Average loss: 3.1033, Accuracy: 1195/10000 (12%)

Classification Train Epoch: 61 [0/60000 (0%)]	Loss: 0.000129, KL fake Loss: 0.024114
Classification Train Epoch: 61 [6400/60000 (11%)]	Loss: 0.000298, KL fake Loss: 0.027692
Classification Train Epoch: 61 [12800/60000 (21%)]	Loss: 0.000290, KL fake Loss: 0.027931
Classification Train Epoch: 61 [19200/60000 (32%)]	Loss: 0.000208, KL fake Loss: 0.024345
Classification Train Epoch: 61 [25600/60000 (43%)]	Loss: 0.000154, KL fake Loss: 0.022953
Classification Train Epoch: 61 [32000/60000 (53%)]	Loss: 0.000157, KL fake Loss: 0.019365
Classification Train Epoch: 61 [38400/60000 (64%)]	Loss: 0.002052, KL fake Loss: 0.024532
Classification Train Epoch: 61 [44800/60000 (75%)]	Loss: 0.000290, KL fake Loss: 0.021809
Classification Train Epoch: 61 [51200/60000 (85%)]	Loss: 0.000317, KL fake Loss: 0.050426
Classification Train Epoch: 61 [57600/60000 (96%)]	Loss: 0.000543, KL fake Loss: 0.023008

Test set: Average loss: 2.6333, Accuracy: 571/10000 (6%)

Classification Train Epoch: 62 [0/60000 (0%)]	Loss: 0.000508, KL fake Loss: 0.030053
Classification Train Epoch: 62 [6400/60000 (11%)]	Loss: 0.000123, KL fake Loss: 0.020874
Classification Train Epoch: 62 [12800/60000 (21%)]	Loss: 0.000423, KL fake Loss: 0.019470
Classification Train Epoch: 62 [19200/60000 (32%)]	Loss: 0.000263, KL fake Loss: 0.025628
Classification Train Epoch: 62 [25600/60000 (43%)]	Loss: 0.000145, KL fake Loss: 0.022906
Classification Train Epoch: 62 [32000/60000 (53%)]	Loss: 0.000353, KL fake Loss: 0.035159
Classification Train Epoch: 62 [38400/60000 (64%)]	Loss: 0.000088, KL fake Loss: 0.024506
Classification Train Epoch: 62 [44800/60000 (75%)]	Loss: 0.000184, KL fake Loss: 0.022173
Classification Train Epoch: 62 [51200/60000 (85%)]	Loss: 0.000125, KL fake Loss: 0.020809
Classification Train Epoch: 62 [57600/60000 (96%)]	Loss: 0.000239, KL fake Loss: 0.018519

Test set: Average loss: 2.7059, Accuracy: 536/10000 (5%)

Classification Train Epoch: 63 [0/60000 (0%)]	Loss: 0.000151, KL fake Loss: 0.052051
Classification Train Epoch: 63 [6400/60000 (11%)]	Loss: 0.000204, KL fake Loss: 0.021254
Classification Train Epoch: 63 [12800/60000 (21%)]	Loss: 0.000064, KL fake Loss: 0.023941
Classification Train Epoch: 63 [19200/60000 (32%)]	Loss: 0.000053, KL fake Loss: 0.068788
Classification Train Epoch: 63 [25600/60000 (43%)]	Loss: 0.000087, KL fake Loss: 0.019285
Classification Train Epoch: 63 [32000/60000 (53%)]	Loss: 0.000156, KL fake Loss: 0.021570
Classification Train Epoch: 63 [38400/60000 (64%)]	Loss: 0.000112, KL fake Loss: 0.016228
Classification Train Epoch: 63 [44800/60000 (75%)]	Loss: 0.000065, KL fake Loss: 0.017201
Classification Train Epoch: 63 [51200/60000 (85%)]	Loss: 0.000374, KL fake Loss: 0.016407
Classification Train Epoch: 63 [57600/60000 (96%)]	Loss: 0.000094, KL fake Loss: 0.017001

Test set: Average loss: 2.6589, Accuracy: 526/10000 (5%)

Classification Train Epoch: 64 [0/60000 (0%)]	Loss: 0.000242, KL fake Loss: 0.021365
Classification Train Epoch: 64 [6400/60000 (11%)]	Loss: 0.000043, KL fake Loss: 0.019559
Classification Train Epoch: 64 [12800/60000 (21%)]	Loss: 0.000077, KL fake Loss: 0.018648
Classification Train Epoch: 64 [19200/60000 (32%)]	Loss: 0.000734, KL fake Loss: 0.170000
Classification Train Epoch: 64 [25600/60000 (43%)]	Loss: 0.000060, KL fake Loss: 0.195816
Classification Train Epoch: 64 [32000/60000 (53%)]	Loss: 0.000232, KL fake Loss: 0.054487
Classification Train Epoch: 64 [38400/60000 (64%)]	Loss: 0.000565, KL fake Loss: 0.022682
Classification Train Epoch: 64 [44800/60000 (75%)]	Loss: 0.000055, KL fake Loss: 0.015125
Classification Train Epoch: 64 [51200/60000 (85%)]	Loss: 0.000128, KL fake Loss: 0.031868
Classification Train Epoch: 64 [57600/60000 (96%)]	Loss: 0.001521, KL fake Loss: 0.015639

Test set: Average loss: 2.7767, Accuracy: 660/10000 (7%)

Classification Train Epoch: 65 [0/60000 (0%)]	Loss: 0.000420, KL fake Loss: 0.019390
Classification Train Epoch: 65 [6400/60000 (11%)]	Loss: 0.000107, KL fake Loss: 0.019074
Classification Train Epoch: 65 [12800/60000 (21%)]	Loss: 0.000238, KL fake Loss: 0.028151
Classification Train Epoch: 65 [19200/60000 (32%)]	Loss: 0.000160, KL fake Loss: 0.060754
Classification Train Epoch: 65 [25600/60000 (43%)]	Loss: 0.000081, KL fake Loss: 0.017438
Classification Train Epoch: 65 [32000/60000 (53%)]	Loss: 0.001270, KL fake Loss: 0.022954
Classification Train Epoch: 65 [38400/60000 (64%)]	Loss: 0.000161, KL fake Loss: 0.017076
Classification Train Epoch: 65 [44800/60000 (75%)]	Loss: 0.000152, KL fake Loss: 0.020365
Classification Train Epoch: 65 [51200/60000 (85%)]	Loss: 0.000151, KL fake Loss: 0.017803
Classification Train Epoch: 65 [57600/60000 (96%)]	Loss: 0.000074, KL fake Loss: 0.077589

Test set: Average loss: 3.2418, Accuracy: 244/10000 (2%)

Classification Train Epoch: 66 [0/60000 (0%)]	Loss: 0.000763, KL fake Loss: 0.021021
Classification Train Epoch: 66 [6400/60000 (11%)]	Loss: 0.000080, KL fake Loss: 0.018732
Classification Train Epoch: 66 [12800/60000 (21%)]	Loss: 0.000240, KL fake Loss: 0.019369
Classification Train Epoch: 66 [19200/60000 (32%)]	Loss: 0.000217, KL fake Loss: 0.021924
Classification Train Epoch: 66 [25600/60000 (43%)]	Loss: 0.000066, KL fake Loss: 0.018229
Classification Train Epoch: 66 [32000/60000 (53%)]	Loss: 0.000264, KL fake Loss: 0.021367
Classification Train Epoch: 66 [38400/60000 (64%)]	Loss: 0.000089, KL fake Loss: 0.019647
Classification Train Epoch: 66 [44800/60000 (75%)]	Loss: 0.000112, KL fake Loss: 0.017750
Classification Train Epoch: 66 [51200/60000 (85%)]	Loss: 0.000049, KL fake Loss: 0.016108
Classification Train Epoch: 66 [57600/60000 (96%)]	Loss: 0.000043, KL fake Loss: 0.018391

Test set: Average loss: 3.2809, Accuracy: 730/10000 (7%)

Classification Train Epoch: 67 [0/60000 (0%)]	Loss: 0.000055, KL fake Loss: 0.022254
Classification Train Epoch: 67 [6400/60000 (11%)]	Loss: 0.000053, KL fake Loss: 0.016616
Classification Train Epoch: 67 [12800/60000 (21%)]	Loss: 0.000226, KL fake Loss: 0.031503
Classification Train Epoch: 67 [19200/60000 (32%)]	Loss: 0.000087, KL fake Loss: 0.022265
Classification Train Epoch: 67 [25600/60000 (43%)]	Loss: 0.000078, KL fake Loss: 0.015587
Classification Train Epoch: 67 [32000/60000 (53%)]	Loss: 0.000142, KL fake Loss: 0.017044
Classification Train Epoch: 67 [38400/60000 (64%)]	Loss: 0.000051, KL fake Loss: 0.016880
Classification Train Epoch: 67 [44800/60000 (75%)]	Loss: 0.000037, KL fake Loss: 0.014100
Classification Train Epoch: 67 [51200/60000 (85%)]	Loss: 0.000358, KL fake Loss: 0.022055
Classification Train Epoch: 67 [57600/60000 (96%)]	Loss: 0.000147, KL fake Loss: 0.012337

Test set: Average loss: 3.2661, Accuracy: 742/10000 (7%)

Classification Train Epoch: 68 [0/60000 (0%)]	Loss: 0.000069, KL fake Loss: 0.015325
Classification Train Epoch: 68 [6400/60000 (11%)]	Loss: 0.000055, KL fake Loss: 0.013025
Classification Train Epoch: 68 [12800/60000 (21%)]	Loss: 0.000068, KL fake Loss: 0.035176
Classification Train Epoch: 68 [19200/60000 (32%)]	Loss: 0.000465, KL fake Loss: 0.016646
Classification Train Epoch: 68 [25600/60000 (43%)]	Loss: 0.000836, KL fake Loss: 0.014912
Classification Train Epoch: 68 [32000/60000 (53%)]	Loss: 0.000030, KL fake Loss: 0.044033
Classification Train Epoch: 68 [38400/60000 (64%)]	Loss: 0.000069, KL fake Loss: 0.012210
Classification Train Epoch: 68 [44800/60000 (75%)]	Loss: 0.000472, KL fake Loss: 0.014173
Classification Train Epoch: 68 [51200/60000 (85%)]	Loss: 0.000046, KL fake Loss: 0.012799
Classification Train Epoch: 68 [57600/60000 (96%)]	Loss: 0.000098, KL fake Loss: 0.011289

Test set: Average loss: 3.5555, Accuracy: 130/10000 (1%)

Classification Train Epoch: 69 [0/60000 (0%)]	Loss: 0.000044, KL fake Loss: 0.022002
Classification Train Epoch: 69 [6400/60000 (11%)]	Loss: 0.000426, KL fake Loss: 0.010280
Classification Train Epoch: 69 [12800/60000 (21%)]	Loss: 0.000132, KL fake Loss: 0.054348
Classification Train Epoch: 69 [19200/60000 (32%)]	Loss: 0.000069, KL fake Loss: 0.044898
Classification Train Epoch: 69 [25600/60000 (43%)]	Loss: 0.000045, KL fake Loss: 0.016050
 69%|██████▉   | 69/100 [4:02:46<1:49:04, 211.10s/it] 70%|███████   | 70/100 [4:06:17<1:45:33, 211.10s/it] 71%|███████   | 71/100 [4:09:48<1:42:01, 211.10s/it] 72%|███████▏  | 72/100 [4:13:19<1:38:30, 211.11s/it] 73%|███████▎  | 73/100 [4:16:50<1:34:59, 211.10s/it] 74%|███████▍  | 74/100 [4:20:21<1:31:28, 211.10s/it] 75%|███████▌  | 75/100 [4:23:52<1:27:57, 211.10s/it] 76%|███████▌  | 76/100 [4:27:23<1:24:26, 211.11s/it] 77%|███████▋  | 77/100 [4:30:54<1:20:55, 211.11s/it]Classification Train Epoch: 69 [32000/60000 (53%)]	Loss: 0.000488, KL fake Loss: 0.023631
Classification Train Epoch: 69 [38400/60000 (64%)]	Loss: 0.000054, KL fake Loss: 0.013205
Classification Train Epoch: 69 [44800/60000 (75%)]	Loss: 0.000044, KL fake Loss: 0.012618
Classification Train Epoch: 69 [51200/60000 (85%)]	Loss: 0.000043, KL fake Loss: 0.012266
Classification Train Epoch: 69 [57600/60000 (96%)]	Loss: 0.000032, KL fake Loss: 0.015230

Test set: Average loss: 3.7873, Accuracy: 936/10000 (9%)

Classification Train Epoch: 70 [0/60000 (0%)]	Loss: 0.000091, KL fake Loss: 0.020427
Classification Train Epoch: 70 [6400/60000 (11%)]	Loss: 0.000021, KL fake Loss: 0.011440
Classification Train Epoch: 70 [12800/60000 (21%)]	Loss: 0.000300, KL fake Loss: 0.030366
Classification Train Epoch: 70 [19200/60000 (32%)]	Loss: 0.000080, KL fake Loss: 0.014107
Classification Train Epoch: 70 [25600/60000 (43%)]	Loss: 0.000073, KL fake Loss: 0.011047
Classification Train Epoch: 70 [32000/60000 (53%)]	Loss: 0.000028, KL fake Loss: 0.013038
Classification Train Epoch: 70 [38400/60000 (64%)]	Loss: 0.000057, KL fake Loss: 0.013853
Classification Train Epoch: 70 [44800/60000 (75%)]	Loss: 0.000132, KL fake Loss: 0.011496
Classification Train Epoch: 70 [51200/60000 (85%)]	Loss: 0.000118, KL fake Loss: 0.404225
Classification Train Epoch: 70 [57600/60000 (96%)]	Loss: 0.000068, KL fake Loss: 0.015867

Test set: Average loss: 3.6144, Accuracy: 871/10000 (9%)

Classification Train Epoch: 71 [0/60000 (0%)]	Loss: 0.000152, KL fake Loss: 0.019342
Classification Train Epoch: 71 [6400/60000 (11%)]	Loss: 0.000107, KL fake Loss: 0.015436
Classification Train Epoch: 71 [12800/60000 (21%)]	Loss: 0.000091, KL fake Loss: 0.021670
Classification Train Epoch: 71 [19200/60000 (32%)]	Loss: 0.000301, KL fake Loss: 0.015262
Classification Train Epoch: 71 [25600/60000 (43%)]	Loss: 0.000052, KL fake Loss: 0.018605
Classification Train Epoch: 71 [32000/60000 (53%)]	Loss: 0.000029, KL fake Loss: 0.012579
Classification Train Epoch: 71 [38400/60000 (64%)]	Loss: 0.000047, KL fake Loss: 0.009899
Classification Train Epoch: 71 [44800/60000 (75%)]	Loss: 0.000043, KL fake Loss: 0.009092
Classification Train Epoch: 71 [51200/60000 (85%)]	Loss: 0.000638, KL fake Loss: 0.009200
Classification Train Epoch: 71 [57600/60000 (96%)]	Loss: 0.000053, KL fake Loss: 0.066007

Test set: Average loss: 3.5429, Accuracy: 948/10000 (9%)

Classification Train Epoch: 72 [0/60000 (0%)]	Loss: 0.000049, KL fake Loss: 0.010343
Classification Train Epoch: 72 [6400/60000 (11%)]	Loss: 0.000164, KL fake Loss: 0.025038
Classification Train Epoch: 72 [12800/60000 (21%)]	Loss: 0.000070, KL fake Loss: 0.012999
Classification Train Epoch: 72 [19200/60000 (32%)]	Loss: 0.000085, KL fake Loss: 0.013116
Classification Train Epoch: 72 [25600/60000 (43%)]	Loss: 0.000035, KL fake Loss: 0.012790
Classification Train Epoch: 72 [32000/60000 (53%)]	Loss: 0.000019, KL fake Loss: 0.010985
Classification Train Epoch: 72 [38400/60000 (64%)]	Loss: 0.000126, KL fake Loss: 0.009430
Classification Train Epoch: 72 [44800/60000 (75%)]	Loss: 0.000080, KL fake Loss: 0.011420
Classification Train Epoch: 72 [51200/60000 (85%)]	Loss: 0.000160, KL fake Loss: 0.024215
Classification Train Epoch: 72 [57600/60000 (96%)]	Loss: 0.000079, KL fake Loss: 0.012745

Test set: Average loss: 3.7717, Accuracy: 917/10000 (9%)

Classification Train Epoch: 73 [0/60000 (0%)]	Loss: 0.000131, KL fake Loss: 0.014132
Classification Train Epoch: 73 [6400/60000 (11%)]	Loss: 0.000045, KL fake Loss: 0.040104
Classification Train Epoch: 73 [12800/60000 (21%)]	Loss: 0.000057, KL fake Loss: 0.013769
Classification Train Epoch: 73 [19200/60000 (32%)]	Loss: 0.000056, KL fake Loss: 0.014219
Classification Train Epoch: 73 [25600/60000 (43%)]	Loss: 0.000032, KL fake Loss: 0.012684
Classification Train Epoch: 73 [32000/60000 (53%)]	Loss: 0.000183, KL fake Loss: 0.009405
Classification Train Epoch: 73 [38400/60000 (64%)]	Loss: 0.000070, KL fake Loss: 0.010871
Classification Train Epoch: 73 [44800/60000 (75%)]	Loss: 0.000205, KL fake Loss: 0.008593
Classification Train Epoch: 73 [51200/60000 (85%)]	Loss: 0.000038, KL fake Loss: 0.012107
Classification Train Epoch: 73 [57600/60000 (96%)]	Loss: 0.000029, KL fake Loss: 0.014309

Test set: Average loss: 3.5066, Accuracy: 921/10000 (9%)

Classification Train Epoch: 74 [0/60000 (0%)]	Loss: 0.000012, KL fake Loss: 0.010547
Classification Train Epoch: 74 [6400/60000 (11%)]	Loss: 0.000066, KL fake Loss: 0.017936
Classification Train Epoch: 74 [12800/60000 (21%)]	Loss: 0.000098, KL fake Loss: 0.015419
Classification Train Epoch: 74 [19200/60000 (32%)]	Loss: 0.000080, KL fake Loss: 0.007760
Classification Train Epoch: 74 [25600/60000 (43%)]	Loss: 0.000045, KL fake Loss: 0.010282
Classification Train Epoch: 74 [32000/60000 (53%)]	Loss: 0.000219, KL fake Loss: 0.012842
Classification Train Epoch: 74 [38400/60000 (64%)]	Loss: 0.000031, KL fake Loss: 0.010312
Classification Train Epoch: 74 [44800/60000 (75%)]	Loss: 0.000021, KL fake Loss: 0.007607
Classification Train Epoch: 74 [51200/60000 (85%)]	Loss: 0.000505, KL fake Loss: 0.008744
Classification Train Epoch: 74 [57600/60000 (96%)]	Loss: 0.000143, KL fake Loss: 0.013193

Test set: Average loss: 3.7759, Accuracy: 952/10000 (10%)

Classification Train Epoch: 75 [0/60000 (0%)]	Loss: 0.000066, KL fake Loss: 0.009496
Classification Train Epoch: 75 [6400/60000 (11%)]	Loss: 0.000022, KL fake Loss: 0.020340
Classification Train Epoch: 75 [12800/60000 (21%)]	Loss: 0.000662, KL fake Loss: 0.010884
Classification Train Epoch: 75 [19200/60000 (32%)]	Loss: 0.000066, KL fake Loss: 0.012775
Classification Train Epoch: 75 [25600/60000 (43%)]	Loss: 0.000047, KL fake Loss: 0.007892
Classification Train Epoch: 75 [32000/60000 (53%)]	Loss: 0.000021, KL fake Loss: 0.009343
Classification Train Epoch: 75 [38400/60000 (64%)]	Loss: 0.007672, KL fake Loss: 0.008377
Classification Train Epoch: 75 [44800/60000 (75%)]	Loss: 0.000310, KL fake Loss: 0.034734
Classification Train Epoch: 75 [51200/60000 (85%)]	Loss: 0.000032, KL fake Loss: 0.010730
Classification Train Epoch: 75 [57600/60000 (96%)]	Loss: 0.000051, KL fake Loss: 0.008688

Test set: Average loss: 3.5847, Accuracy: 889/10000 (9%)

Classification Train Epoch: 76 [0/60000 (0%)]	Loss: 0.000057, KL fake Loss: 0.010280
Classification Train Epoch: 76 [6400/60000 (11%)]	Loss: 0.000027, KL fake Loss: 0.008094
Classification Train Epoch: 76 [12800/60000 (21%)]	Loss: 0.000029, KL fake Loss: 0.008224
Classification Train Epoch: 76 [19200/60000 (32%)]	Loss: 0.000061, KL fake Loss: 0.011575
Classification Train Epoch: 76 [25600/60000 (43%)]	Loss: 0.000068, KL fake Loss: 0.014392
Classification Train Epoch: 76 [32000/60000 (53%)]	Loss: 0.000017, KL fake Loss: 0.010539
Classification Train Epoch: 76 [38400/60000 (64%)]	Loss: 0.000041, KL fake Loss: 0.010881
Classification Train Epoch: 76 [44800/60000 (75%)]	Loss: 0.000037, KL fake Loss: 0.009096
Classification Train Epoch: 76 [51200/60000 (85%)]	Loss: 0.001822, KL fake Loss: 0.012464
Classification Train Epoch: 76 [57600/60000 (96%)]	Loss: 0.000023, KL fake Loss: 0.011331

Test set: Average loss: 3.2693, Accuracy: 874/10000 (9%)

Classification Train Epoch: 77 [0/60000 (0%)]	Loss: 0.000519, KL fake Loss: 0.009072
Classification Train Epoch: 77 [6400/60000 (11%)]	Loss: 0.000118, KL fake Loss: 0.009797
Classification Train Epoch: 77 [12800/60000 (21%)]	Loss: 0.000072, KL fake Loss: 0.019111
Classification Train Epoch: 77 [19200/60000 (32%)]	Loss: 0.000043, KL fake Loss: 0.019336
Classification Train Epoch: 77 [25600/60000 (43%)]	Loss: 0.000018, KL fake Loss: 0.009699
Classification Train Epoch: 77 [32000/60000 (53%)]	Loss: 0.000043, KL fake Loss: 0.008982
Classification Train Epoch: 77 [38400/60000 (64%)]	Loss: 0.001748, KL fake Loss: 0.065769
Classification Train Epoch: 77 [44800/60000 (75%)]	Loss: 0.000144, KL fake Loss: 0.015813
Classification Train Epoch: 77 [51200/60000 (85%)]	Loss: 0.000142, KL fake Loss: 0.006151
Classification Train Epoch: 77 [57600/60000 (96%)]	Loss: 0.000055, KL fake Loss: 0.015956

Test set: Average loss: 3.4494, Accuracy: 781/10000 (8%)

 78%|███████▊  | 78/100 [4:34:26<1:17:24, 211.11s/it] 79%|███████▉  | 79/100 [4:37:57<1:13:53, 211.10s/it] 80%|████████  | 80/100 [4:41:28<1:10:22, 211.13s/it] 81%|████████  | 81/100 [4:44:59<1:06:51, 211.12s/it] 82%|████████▏ | 82/100 [4:48:30<1:03:20, 211.11s/it] 83%|████████▎ | 83/100 [4:52:01<59:48, 211.11s/it]   84%|████████▍ | 84/100 [4:55:32<56:17, 211.11s/it] 85%|████████▌ | 85/100 [4:59:03<52:46, 211.10s/it]Classification Train Epoch: 78 [0/60000 (0%)]	Loss: 0.000026, KL fake Loss: 0.009894
Classification Train Epoch: 78 [6400/60000 (11%)]	Loss: 0.000020, KL fake Loss: 0.008389
Classification Train Epoch: 78 [12800/60000 (21%)]	Loss: 0.000033, KL fake Loss: 0.006531
Classification Train Epoch: 78 [19200/60000 (32%)]	Loss: 0.000146, KL fake Loss: 0.007124
Classification Train Epoch: 78 [25600/60000 (43%)]	Loss: 0.000031, KL fake Loss: 0.009220
Classification Train Epoch: 78 [32000/60000 (53%)]	Loss: 0.000034, KL fake Loss: 0.006552
Classification Train Epoch: 78 [38400/60000 (64%)]	Loss: 0.000024, KL fake Loss: 0.018556
Classification Train Epoch: 78 [44800/60000 (75%)]	Loss: 0.000034, KL fake Loss: 0.011400
Classification Train Epoch: 78 [51200/60000 (85%)]	Loss: 0.000015, KL fake Loss: 0.005931
Classification Train Epoch: 78 [57600/60000 (96%)]	Loss: 0.000316, KL fake Loss: 0.009340

Test set: Average loss: 3.1716, Accuracy: 700/10000 (7%)

Classification Train Epoch: 79 [0/60000 (0%)]	Loss: 0.000020, KL fake Loss: 0.009370
Classification Train Epoch: 79 [6400/60000 (11%)]	Loss: 0.000011, KL fake Loss: 0.009962
Classification Train Epoch: 79 [12800/60000 (21%)]	Loss: 0.000015, KL fake Loss: 0.008666
Classification Train Epoch: 79 [19200/60000 (32%)]	Loss: 0.000030, KL fake Loss: 0.009204
Classification Train Epoch: 79 [25600/60000 (43%)]	Loss: 0.000365, KL fake Loss: 0.034707
Classification Train Epoch: 79 [32000/60000 (53%)]	Loss: 0.000093, KL fake Loss: 0.007646
Classification Train Epoch: 79 [38400/60000 (64%)]	Loss: 0.000072, KL fake Loss: 0.013360
Classification Train Epoch: 79 [44800/60000 (75%)]	Loss: 0.000040, KL fake Loss: 0.009128
Classification Train Epoch: 79 [51200/60000 (85%)]	Loss: 0.000100, KL fake Loss: 0.009031
Classification Train Epoch: 79 [57600/60000 (96%)]	Loss: 0.000049, KL fake Loss: 0.009086

Test set: Average loss: 3.2949, Accuracy: 687/10000 (7%)

Classification Train Epoch: 80 [0/60000 (0%)]	Loss: 0.000021, KL fake Loss: 0.009005
Classification Train Epoch: 80 [6400/60000 (11%)]	Loss: 0.000067, KL fake Loss: 0.005750
Classification Train Epoch: 80 [12800/60000 (21%)]	Loss: 0.000114, KL fake Loss: 0.011518
Classification Train Epoch: 80 [19200/60000 (32%)]	Loss: 0.000031, KL fake Loss: 0.009672
Classification Train Epoch: 80 [25600/60000 (43%)]	Loss: 0.000028, KL fake Loss: 0.007514
Classification Train Epoch: 80 [32000/60000 (53%)]	Loss: 0.000026, KL fake Loss: 0.006681
Classification Train Epoch: 80 [38400/60000 (64%)]	Loss: 0.000064, KL fake Loss: 0.013978
Classification Train Epoch: 80 [44800/60000 (75%)]	Loss: 0.000051, KL fake Loss: 0.008002
Classification Train Epoch: 80 [51200/60000 (85%)]	Loss: 0.000017, KL fake Loss: 0.009058
Classification Train Epoch: 80 [57600/60000 (96%)]	Loss: 0.000259, KL fake Loss: 0.008450

Test set: Average loss: 2.7356, Accuracy: 850/10000 (8%)

Classification Train Epoch: 81 [0/60000 (0%)]	Loss: 0.000022, KL fake Loss: 0.009770
Classification Train Epoch: 81 [6400/60000 (11%)]	Loss: 0.000042, KL fake Loss: 0.009716
Classification Train Epoch: 81 [12800/60000 (21%)]	Loss: 0.000018, KL fake Loss: 0.008440
Classification Train Epoch: 81 [19200/60000 (32%)]	Loss: 0.000020, KL fake Loss: 0.006466
Classification Train Epoch: 81 [25600/60000 (43%)]	Loss: 0.000021, KL fake Loss: 0.008672
Classification Train Epoch: 81 [32000/60000 (53%)]	Loss: 0.000017, KL fake Loss: 0.031287
Classification Train Epoch: 81 [38400/60000 (64%)]	Loss: 0.000062, KL fake Loss: 0.008038
Classification Train Epoch: 81 [44800/60000 (75%)]	Loss: 0.000042, KL fake Loss: 0.014004
Classification Train Epoch: 81 [51200/60000 (85%)]	Loss: 0.000029, KL fake Loss: 0.010513
Classification Train Epoch: 81 [57600/60000 (96%)]	Loss: 0.000067, KL fake Loss: 0.008505

Test set: Average loss: 2.8114, Accuracy: 951/10000 (10%)

Classification Train Epoch: 82 [0/60000 (0%)]	Loss: 0.000018, KL fake Loss: 0.010177
Classification Train Epoch: 82 [6400/60000 (11%)]	Loss: 0.000094, KL fake Loss: 0.007787
Classification Train Epoch: 82 [12800/60000 (21%)]	Loss: 0.000074, KL fake Loss: 0.008471
Classification Train Epoch: 82 [19200/60000 (32%)]	Loss: 0.000052, KL fake Loss: 0.029080
Classification Train Epoch: 82 [25600/60000 (43%)]	Loss: 0.000051, KL fake Loss: 0.009617
Classification Train Epoch: 82 [32000/60000 (53%)]	Loss: 0.000035, KL fake Loss: 0.020143
Classification Train Epoch: 82 [38400/60000 (64%)]	Loss: 0.000025, KL fake Loss: 0.009157
Classification Train Epoch: 82 [44800/60000 (75%)]	Loss: 0.000021, KL fake Loss: 0.007347
Classification Train Epoch: 82 [51200/60000 (85%)]	Loss: 0.000017, KL fake Loss: 0.008087
Classification Train Epoch: 82 [57600/60000 (96%)]	Loss: 0.000035, KL fake Loss: 0.009819

Test set: Average loss: 3.0928, Accuracy: 679/10000 (7%)

Classification Train Epoch: 83 [0/60000 (0%)]	Loss: 0.000025, KL fake Loss: 0.007293
Classification Train Epoch: 83 [6400/60000 (11%)]	Loss: 0.000020, KL fake Loss: 0.008360
Classification Train Epoch: 83 [12800/60000 (21%)]	Loss: 0.000023, KL fake Loss: 0.015583
Classification Train Epoch: 83 [19200/60000 (32%)]	Loss: 0.000085, KL fake Loss: 0.008533
Classification Train Epoch: 83 [25600/60000 (43%)]	Loss: 0.000019, KL fake Loss: 0.007412
Classification Train Epoch: 83 [32000/60000 (53%)]	Loss: 0.000058, KL fake Loss: 0.010034
Classification Train Epoch: 83 [38400/60000 (64%)]	Loss: 0.000019, KL fake Loss: 0.010040
Classification Train Epoch: 83 [44800/60000 (75%)]	Loss: 0.000022, KL fake Loss: 0.012763
Classification Train Epoch: 83 [51200/60000 (85%)]	Loss: 0.000016, KL fake Loss: 0.006812
Classification Train Epoch: 83 [57600/60000 (96%)]	Loss: 0.000032, KL fake Loss: 0.008494

Test set: Average loss: 3.1257, Accuracy: 716/10000 (7%)

Classification Train Epoch: 84 [0/60000 (0%)]	Loss: 0.000022, KL fake Loss: 0.008698
Classification Train Epoch: 84 [6400/60000 (11%)]	Loss: 0.000018, KL fake Loss: 0.010746
Classification Train Epoch: 84 [12800/60000 (21%)]	Loss: 0.000067, KL fake Loss: 0.007006
Classification Train Epoch: 84 [19200/60000 (32%)]	Loss: 0.000013, KL fake Loss: 0.012719
Classification Train Epoch: 84 [25600/60000 (43%)]	Loss: 0.000029, KL fake Loss: 0.055666
Classification Train Epoch: 84 [32000/60000 (53%)]	Loss: 0.000033, KL fake Loss: 0.011219
Classification Train Epoch: 84 [38400/60000 (64%)]	Loss: 0.000087, KL fake Loss: 0.007500
Classification Train Epoch: 84 [44800/60000 (75%)]	Loss: 0.000039, KL fake Loss: 0.008486
Classification Train Epoch: 84 [51200/60000 (85%)]	Loss: 0.000044, KL fake Loss: 0.011951
Classification Train Epoch: 84 [57600/60000 (96%)]	Loss: 0.000033, KL fake Loss: 0.008085

Test set: Average loss: 3.5238, Accuracy: 812/10000 (8%)

Classification Train Epoch: 85 [0/60000 (0%)]	Loss: 0.000030, KL fake Loss: 0.005266
Classification Train Epoch: 85 [6400/60000 (11%)]	Loss: 0.000034, KL fake Loss: 0.005290
Classification Train Epoch: 85 [12800/60000 (21%)]	Loss: 0.000038, KL fake Loss: 0.007276
Classification Train Epoch: 85 [19200/60000 (32%)]	Loss: 0.000055, KL fake Loss: 0.088981
Classification Train Epoch: 85 [25600/60000 (43%)]	Loss: 0.000070, KL fake Loss: 0.004918
Classification Train Epoch: 85 [32000/60000 (53%)]	Loss: 0.000066, KL fake Loss: 0.005741
Classification Train Epoch: 85 [38400/60000 (64%)]	Loss: 0.000248, KL fake Loss: 0.007770
Classification Train Epoch: 85 [44800/60000 (75%)]	Loss: 0.000016, KL fake Loss: 0.010705
Classification Train Epoch: 85 [51200/60000 (85%)]	Loss: 0.000041, KL fake Loss: 0.006967
Classification Train Epoch: 85 [57600/60000 (96%)]	Loss: 0.000056, KL fake Loss: 0.007722

Test set: Average loss: 4.0090, Accuracy: 906/10000 (9%)

Classification Train Epoch: 86 [0/60000 (0%)]	Loss: 0.000022, KL fake Loss: 0.006791
Classification Train Epoch: 86 [6400/60000 (11%)]	Loss: 0.000012, KL fake Loss: 0.007557
Classification Train Epoch: 86 [12800/60000 (21%)]	Loss: 0.000020, KL fake Loss: 0.006533
Classification Train Epoch: 86 [19200/60000 (32%)]	Loss: 0.000023, KL fake Loss: 0.008263
Classification Train Epoch: 86 [25600/60000 (43%)]	Loss: 0.000746, KL fake Loss: 0.009402
Classification Train Epoch: 86 [32000/60000 (53%)]	Loss: 0.000039, KL fake Loss: 0.005416
 86%|████████▌ | 86/100 [5:02:34<49:15, 211.10s/it] 87%|████████▋ | 87/100 [5:06:06<45:44, 211.09s/it] 88%|████████▊ | 88/100 [5:09:37<42:13, 211.09s/it] 89%|████████▉ | 89/100 [5:13:08<38:41, 211.09s/it] 90%|█████████ | 90/100 [5:16:39<35:10, 211.09s/it] 91%|█████████ | 91/100 [5:20:10<31:39, 211.09s/it] 92%|█████████▏| 92/100 [5:23:41<28:08, 211.09s/it] 93%|█████████▎| 93/100 [5:27:12<24:37, 211.09s/it] 94%|█████████▍| 94/100 [5:30:43<21:06, 211.09s/it]Classification Train Epoch: 86 [38400/60000 (64%)]	Loss: 0.000019, KL fake Loss: 0.005648
Classification Train Epoch: 86 [44800/60000 (75%)]	Loss: 0.000042, KL fake Loss: 0.004435
Classification Train Epoch: 86 [51200/60000 (85%)]	Loss: 0.000770, KL fake Loss: 0.025058
Classification Train Epoch: 86 [57600/60000 (96%)]	Loss: 0.000013, KL fake Loss: 0.011412

Test set: Average loss: 3.4567, Accuracy: 942/10000 (9%)

Classification Train Epoch: 87 [0/60000 (0%)]	Loss: 0.000011, KL fake Loss: 0.014475
Classification Train Epoch: 87 [6400/60000 (11%)]	Loss: 0.000018, KL fake Loss: 0.005858
Classification Train Epoch: 87 [12800/60000 (21%)]	Loss: 0.000016, KL fake Loss: 0.006968
Classification Train Epoch: 87 [19200/60000 (32%)]	Loss: 0.000109, KL fake Loss: 0.008402
Classification Train Epoch: 87 [25600/60000 (43%)]	Loss: 0.000020, KL fake Loss: 0.007218
Classification Train Epoch: 87 [32000/60000 (53%)]	Loss: 0.000066, KL fake Loss: 0.057618
Classification Train Epoch: 87 [38400/60000 (64%)]	Loss: 0.000014, KL fake Loss: 0.008235
Classification Train Epoch: 87 [44800/60000 (75%)]	Loss: 0.000042, KL fake Loss: 0.005454
Classification Train Epoch: 87 [51200/60000 (85%)]	Loss: 0.000037, KL fake Loss: 0.007076
Classification Train Epoch: 87 [57600/60000 (96%)]	Loss: 0.000037, KL fake Loss: 0.006566

Test set: Average loss: 3.6541, Accuracy: 848/10000 (8%)

Classification Train Epoch: 88 [0/60000 (0%)]	Loss: 0.000018, KL fake Loss: 0.007422
Classification Train Epoch: 88 [6400/60000 (11%)]	Loss: 0.000043, KL fake Loss: 0.029565
Classification Train Epoch: 88 [12800/60000 (21%)]	Loss: 0.000060, KL fake Loss: 0.012052
Classification Train Epoch: 88 [19200/60000 (32%)]	Loss: 0.000042, KL fake Loss: 0.007427
Classification Train Epoch: 88 [25600/60000 (43%)]	Loss: 0.000767, KL fake Loss: 0.006897
Classification Train Epoch: 88 [32000/60000 (53%)]	Loss: 0.000063, KL fake Loss: 0.006659
Classification Train Epoch: 88 [38400/60000 (64%)]	Loss: 0.000014, KL fake Loss: 0.008135
Classification Train Epoch: 88 [44800/60000 (75%)]	Loss: 0.000083, KL fake Loss: 0.004511
Classification Train Epoch: 88 [51200/60000 (85%)]	Loss: 0.000042, KL fake Loss: 0.006298
Classification Train Epoch: 88 [57600/60000 (96%)]	Loss: 0.000022, KL fake Loss: 0.005603

Test set: Average loss: 3.8045, Accuracy: 866/10000 (9%)

Classification Train Epoch: 89 [0/60000 (0%)]	Loss: 0.000014, KL fake Loss: 0.006250
Classification Train Epoch: 89 [6400/60000 (11%)]	Loss: 0.000008, KL fake Loss: 0.007157
Classification Train Epoch: 89 [12800/60000 (21%)]	Loss: 0.000069, KL fake Loss: 0.006194
Classification Train Epoch: 89 [19200/60000 (32%)]	Loss: 0.000021, KL fake Loss: 0.007480
Classification Train Epoch: 89 [25600/60000 (43%)]	Loss: 0.000017, KL fake Loss: 0.007299
Classification Train Epoch: 89 [32000/60000 (53%)]	Loss: 0.000016, KL fake Loss: 0.007700
Classification Train Epoch: 89 [38400/60000 (64%)]	Loss: 0.000024, KL fake Loss: 0.005902
Classification Train Epoch: 89 [44800/60000 (75%)]	Loss: 0.000161, KL fake Loss: 0.007773
Classification Train Epoch: 89 [51200/60000 (85%)]	Loss: 0.000023, KL fake Loss: 0.144838
Classification Train Epoch: 89 [57600/60000 (96%)]	Loss: 0.000048, KL fake Loss: 0.007322

Test set: Average loss: 4.0545, Accuracy: 937/10000 (9%)

Classification Train Epoch: 90 [0/60000 (0%)]	Loss: 0.000019, KL fake Loss: 0.006572
Classification Train Epoch: 90 [6400/60000 (11%)]	Loss: 0.000081, KL fake Loss: 0.005363
Classification Train Epoch: 90 [12800/60000 (21%)]	Loss: 0.000061, KL fake Loss: 0.005679
Classification Train Epoch: 90 [19200/60000 (32%)]	Loss: 0.000031, KL fake Loss: 0.005825
Classification Train Epoch: 90 [25600/60000 (43%)]	Loss: 0.000039, KL fake Loss: 0.005658
Classification Train Epoch: 90 [32000/60000 (53%)]	Loss: 0.000025, KL fake Loss: 0.006361
Classification Train Epoch: 90 [38400/60000 (64%)]	Loss: 0.000068, KL fake Loss: 0.006819
Classification Train Epoch: 90 [44800/60000 (75%)]	Loss: 0.000017, KL fake Loss: 0.006346
Classification Train Epoch: 90 [51200/60000 (85%)]	Loss: 0.000417, KL fake Loss: 0.007185
Classification Train Epoch: 90 [57600/60000 (96%)]	Loss: 0.000013, KL fake Loss: 0.014188

Test set: Average loss: 3.5783, Accuracy: 937/10000 (9%)

Classification Train Epoch: 91 [0/60000 (0%)]	Loss: 0.000031, KL fake Loss: 0.005705
Classification Train Epoch: 91 [6400/60000 (11%)]	Loss: 0.000012, KL fake Loss: 0.087339
Classification Train Epoch: 91 [12800/60000 (21%)]	Loss: 0.000106, KL fake Loss: 0.006269
Classification Train Epoch: 91 [19200/60000 (32%)]	Loss: 0.000245, KL fake Loss: 0.006464
Classification Train Epoch: 91 [25600/60000 (43%)]	Loss: 0.000053, KL fake Loss: 0.005919
Classification Train Epoch: 91 [32000/60000 (53%)]	Loss: 0.000039, KL fake Loss: 0.005218
Classification Train Epoch: 91 [38400/60000 (64%)]	Loss: 0.000059, KL fake Loss: 0.004689
Classification Train Epoch: 91 [44800/60000 (75%)]	Loss: 0.000032, KL fake Loss: 0.004712
Classification Train Epoch: 91 [51200/60000 (85%)]	Loss: 0.000023, KL fake Loss: 0.008355
Classification Train Epoch: 91 [57600/60000 (96%)]	Loss: 0.000214, KL fake Loss: 0.004721

Test set: Average loss: 3.4732, Accuracy: 894/10000 (9%)

Classification Train Epoch: 92 [0/60000 (0%)]	Loss: 0.000023, KL fake Loss: 0.006859
Classification Train Epoch: 92 [6400/60000 (11%)]	Loss: 0.000085, KL fake Loss: 0.009303
Classification Train Epoch: 92 [12800/60000 (21%)]	Loss: 0.000118, KL fake Loss: 0.005764
Classification Train Epoch: 92 [19200/60000 (32%)]	Loss: 0.001034, KL fake Loss: 0.004324
Classification Train Epoch: 92 [25600/60000 (43%)]	Loss: 0.000018, KL fake Loss: 0.013614
Classification Train Epoch: 92 [32000/60000 (53%)]	Loss: 0.000010, KL fake Loss: 0.006993
Classification Train Epoch: 92 [38400/60000 (64%)]	Loss: 0.000023, KL fake Loss: 0.008749
Classification Train Epoch: 92 [44800/60000 (75%)]	Loss: 0.000010, KL fake Loss: 0.011869
Classification Train Epoch: 92 [51200/60000 (85%)]	Loss: 0.000012, KL fake Loss: 0.007658
Classification Train Epoch: 92 [57600/60000 (96%)]	Loss: 0.000025, KL fake Loss: 0.006603

Test set: Average loss: 3.8203, Accuracy: 944/10000 (9%)

Classification Train Epoch: 93 [0/60000 (0%)]	Loss: 0.000059, KL fake Loss: 0.007715
Classification Train Epoch: 93 [6400/60000 (11%)]	Loss: 0.000029, KL fake Loss: 0.009056
Classification Train Epoch: 93 [12800/60000 (21%)]	Loss: 0.000029, KL fake Loss: 0.007169
Classification Train Epoch: 93 [19200/60000 (32%)]	Loss: 0.000027, KL fake Loss: 0.007003
Classification Train Epoch: 93 [25600/60000 (43%)]	Loss: 0.000014, KL fake Loss: 0.393666
Classification Train Epoch: 93 [32000/60000 (53%)]	Loss: 0.000020, KL fake Loss: 0.243198
Classification Train Epoch: 93 [38400/60000 (64%)]	Loss: 0.000107, KL fake Loss: 0.005404
Classification Train Epoch: 93 [44800/60000 (75%)]	Loss: 0.000014, KL fake Loss: 0.006108
Classification Train Epoch: 93 [51200/60000 (85%)]	Loss: 0.000024, KL fake Loss: 0.010104
Classification Train Epoch: 93 [57600/60000 (96%)]	Loss: 0.000033, KL fake Loss: 0.007383

Test set: Average loss: 3.6087, Accuracy: 945/10000 (9%)

Classification Train Epoch: 94 [0/60000 (0%)]	Loss: 0.000019, KL fake Loss: 0.026811
Classification Train Epoch: 94 [6400/60000 (11%)]	Loss: 0.000123, KL fake Loss: 0.009673
Classification Train Epoch: 94 [12800/60000 (21%)]	Loss: 0.000046, KL fake Loss: 0.006788
Classification Train Epoch: 94 [19200/60000 (32%)]	Loss: 0.000292, KL fake Loss: 0.008007
Classification Train Epoch: 94 [25600/60000 (43%)]	Loss: 0.000026, KL fake Loss: 0.006746
Classification Train Epoch: 94 [32000/60000 (53%)]	Loss: 0.000036, KL fake Loss: 0.006121
Classification Train Epoch: 94 [38400/60000 (64%)]	Loss: 0.000214, KL fake Loss: 0.006192
Classification Train Epoch: 94 [44800/60000 (75%)]	Loss: 0.000012, KL fake Loss: 0.007527
Classification Train Epoch: 94 [51200/60000 (85%)]	Loss: 0.000048, KL fake Loss: 0.005162
Classification Train Epoch: 94 [57600/60000 (96%)]	Loss: 0.000010, KL fake Loss: 0.048980

Test set: Average loss: 3.9831, Accuracy: 965/10000 (10%)

Classification Train Epoch: 95 [0/60000 (0%)]	Loss: 0.000027, KL fake Loss: 0.013375
 95%|█████████▌| 95/100 [5:34:14<17:35, 211.09s/it] 96%|█████████▌| 96/100 [5:37:45<14:04, 211.09s/it] 97%|█████████▋| 97/100 [5:41:16<10:33, 211.10s/it] 98%|█████████▊| 98/100 [5:44:48<07:02, 211.09s/it] 99%|█████████▉| 99/100 [5:48:19<03:31, 211.10s/it]100%|██████████| 100/100 [5:51:50<00:00, 211.14s/it]100%|██████████| 100/100 [5:51:50<00:00, 211.10s/it]
Classification Train Epoch: 95 [6400/60000 (11%)]	Loss: 0.000012, KL fake Loss: 0.007569
Classification Train Epoch: 95 [12800/60000 (21%)]	Loss: 0.000018, KL fake Loss: 0.005604
Classification Train Epoch: 95 [19200/60000 (32%)]	Loss: 0.000009, KL fake Loss: 0.006694
Classification Train Epoch: 95 [25600/60000 (43%)]	Loss: 0.000016, KL fake Loss: 0.033492
Classification Train Epoch: 95 [32000/60000 (53%)]	Loss: 0.000027, KL fake Loss: 0.005428
Classification Train Epoch: 95 [38400/60000 (64%)]	Loss: 0.000015, KL fake Loss: 0.005831
Classification Train Epoch: 95 [44800/60000 (75%)]	Loss: 0.000011, KL fake Loss: 0.004636
Classification Train Epoch: 95 [51200/60000 (85%)]	Loss: 0.000011, KL fake Loss: 0.007109
Classification Train Epoch: 95 [57600/60000 (96%)]	Loss: 0.000032, KL fake Loss: 0.007050

Test set: Average loss: 3.9358, Accuracy: 938/10000 (9%)

Classification Train Epoch: 96 [0/60000 (0%)]	Loss: 0.000034, KL fake Loss: 0.007690
Classification Train Epoch: 96 [6400/60000 (11%)]	Loss: 0.000035, KL fake Loss: 0.005051
Classification Train Epoch: 96 [12800/60000 (21%)]	Loss: 0.000067, KL fake Loss: 0.006774
Classification Train Epoch: 96 [19200/60000 (32%)]	Loss: 0.000065, KL fake Loss: 0.006913
Classification Train Epoch: 96 [25600/60000 (43%)]	Loss: 0.000015, KL fake Loss: 0.007859
Classification Train Epoch: 96 [32000/60000 (53%)]	Loss: 0.000072, KL fake Loss: 0.007153
Classification Train Epoch: 96 [38400/60000 (64%)]	Loss: 0.000022, KL fake Loss: 0.005162
Classification Train Epoch: 96 [44800/60000 (75%)]	Loss: 0.000017, KL fake Loss: 0.004511
Classification Train Epoch: 96 [51200/60000 (85%)]	Loss: 0.000021, KL fake Loss: 0.006879
Classification Train Epoch: 96 [57600/60000 (96%)]	Loss: 0.000028, KL fake Loss: 0.005227

Test set: Average loss: 3.5711, Accuracy: 965/10000 (10%)

Classification Train Epoch: 97 [0/60000 (0%)]	Loss: 0.000062, KL fake Loss: 0.007791
Classification Train Epoch: 97 [6400/60000 (11%)]	Loss: 0.008058, KL fake Loss: 0.011278
Classification Train Epoch: 97 [12800/60000 (21%)]	Loss: 0.000026, KL fake Loss: 0.007381
Classification Train Epoch: 97 [19200/60000 (32%)]	Loss: 0.000026, KL fake Loss: 0.004850
Classification Train Epoch: 97 [25600/60000 (43%)]	Loss: 0.000019, KL fake Loss: 0.006666
Classification Train Epoch: 97 [32000/60000 (53%)]	Loss: 0.000023, KL fake Loss: 0.006133
Classification Train Epoch: 97 [38400/60000 (64%)]	Loss: 0.000020, KL fake Loss: 0.143561
Classification Train Epoch: 97 [44800/60000 (75%)]	Loss: 0.000020, KL fake Loss: 0.007973
Classification Train Epoch: 97 [51200/60000 (85%)]	Loss: 0.000035, KL fake Loss: 0.007933
Classification Train Epoch: 97 [57600/60000 (96%)]	Loss: 0.000017, KL fake Loss: 0.012341

Test set: Average loss: 3.8394, Accuracy: 973/10000 (10%)

Classification Train Epoch: 98 [0/60000 (0%)]	Loss: 0.000023, KL fake Loss: 0.006972
Classification Train Epoch: 98 [6400/60000 (11%)]	Loss: 0.000016, KL fake Loss: 0.004181
Classification Train Epoch: 98 [12800/60000 (21%)]	Loss: 0.000036, KL fake Loss: 0.012931
Classification Train Epoch: 98 [19200/60000 (32%)]	Loss: 0.000019, KL fake Loss: 0.006305
Classification Train Epoch: 98 [25600/60000 (43%)]	Loss: 0.000007, KL fake Loss: 0.004422
Classification Train Epoch: 98 [32000/60000 (53%)]	Loss: 0.000122, KL fake Loss: 0.006269
Classification Train Epoch: 98 [38400/60000 (64%)]	Loss: 0.000010, KL fake Loss: 0.007053
Classification Train Epoch: 98 [44800/60000 (75%)]	Loss: 0.000040, KL fake Loss: 0.006601
Classification Train Epoch: 98 [51200/60000 (85%)]	Loss: 0.000045, KL fake Loss: 0.005495
Classification Train Epoch: 98 [57600/60000 (96%)]	Loss: 0.000033, KL fake Loss: 0.030243

Test set: Average loss: 3.9248, Accuracy: 973/10000 (10%)

Classification Train Epoch: 99 [0/60000 (0%)]	Loss: 0.000020, KL fake Loss: 0.008581
Classification Train Epoch: 99 [6400/60000 (11%)]	Loss: 0.000020, KL fake Loss: 0.005008
Classification Train Epoch: 99 [12800/60000 (21%)]	Loss: 0.003632, KL fake Loss: 0.012672
Classification Train Epoch: 99 [19200/60000 (32%)]	Loss: 0.000021, KL fake Loss: 0.004186
Classification Train Epoch: 99 [25600/60000 (43%)]	Loss: 0.000039, KL fake Loss: 0.005745
Classification Train Epoch: 99 [32000/60000 (53%)]	Loss: 0.000022, KL fake Loss: 0.005913
Classification Train Epoch: 99 [38400/60000 (64%)]	Loss: 0.000029, KL fake Loss: 0.005828
Classification Train Epoch: 99 [44800/60000 (75%)]	Loss: 0.000027, KL fake Loss: 0.009193
Classification Train Epoch: 99 [51200/60000 (85%)]	Loss: 0.000018, KL fake Loss: 0.004836
Classification Train Epoch: 99 [57600/60000 (96%)]	Loss: 0.000203, KL fake Loss: 0.009556

Test set: Average loss: 3.8002, Accuracy: 950/10000 (10%)

Classification Train Epoch: 100 [0/60000 (0%)]	Loss: 0.000020, KL fake Loss: 0.004644
Classification Train Epoch: 100 [6400/60000 (11%)]	Loss: 0.000017, KL fake Loss: 0.007144
Classification Train Epoch: 100 [12800/60000 (21%)]	Loss: 0.000249, KL fake Loss: 0.004122
Classification Train Epoch: 100 [19200/60000 (32%)]	Loss: 0.000050, KL fake Loss: 0.003580
Classification Train Epoch: 100 [25600/60000 (43%)]	Loss: 0.000282, KL fake Loss: 0.006079
Classification Train Epoch: 100 [32000/60000 (53%)]	Loss: 0.000009, KL fake Loss: 0.021849
Classification Train Epoch: 100 [38400/60000 (64%)]	Loss: 0.000023, KL fake Loss: 0.003820
Classification Train Epoch: 100 [44800/60000 (75%)]	Loss: 0.000031, KL fake Loss: 0.005517
Classification Train Epoch: 100 [51200/60000 (85%)]	Loss: 0.000015, KL fake Loss: 0.007940
Classification Train Epoch: 100 [57600/60000 (96%)]	Loss: 0.000165, KL fake Loss: 0.005419

Test set: Average loss: 3.9104, Accuracy: 970/10000 (10%)

Namespace(batch_size=128, no_cuda=False, seed=1, dataset='MNIST-FashionMNIST', dataroot='./data', imageSize=32, outf='./results/joint_confidence_loss/MFM-0.01/', out_dataset='MNIST-FashionMNIST', num_classes=10, num_channels=1, pre_trained_net='results/joint_confidence_loss/MFM-0.01/model_epoch_100.pth')
Random Seed:  1
Load model
DenseNet3(
  (conv1): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock(
    (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock(
    (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(342, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (fc): Linear(in_features=342, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)ic| len(dset): 60000
ic| len(dset): 60000

load target data:  MNIST-FashionMNIST
load non target data:  MNIST-FashionMNIST
generate log from in-distribution data

 Final Accuracy: 970/10000 (9.70%)

generate log  from out-of-distribution data
calculate metrics
  Performance of Baseline detector
TNR at TPR 95%:            36.826%
TNR at TPR 99%:            19.777%
AUROC:                     71.337%
Detection acc:             70.065%
AUPR In:                   60.297%
AUPR Out:                  77.223%

ic| len(dset): 60000
ic| len(dset): 10000
Namespace(batch_size=64, epochs=100, lr=0.001, no_cuda=False, seed=1, log_interval=100, dataset='FashionMNIST', dataroot='./data', imageSize=32, outf='./results/joint_confidence_loss/FM-0.1/', wd=0.0, droprate=0.1, decreasing_lr='60', num_classes=8, beta=0.1, num_channels=1)
Random Seed:  1
load InD data for Experiment:  FashionMNIST
Load model
DenseNet3(
  (conv1): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock(
    (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock(
    (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(342, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (fc): Linear(in_features=342, out_features=8, bias=True)
  (softmax): Softmax(dim=1)
)
load GAN
Setup optimizer
0.001
  0%|          | 0/100 [00:00<?, ?it/s]/home/xysong/.conda/envs/OoD/lib/python3.9/site-packages/torch/nn/functional.py:2904: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
  1%|          | 1/100 [02:43<4:30:16, 163.81s/it]  2%|▏         | 2/100 [05:27<4:27:23, 163.71s/it]  3%|▎         | 3/100 [08:11<4:24:38, 163.70s/it]  4%|▍         | 4/100 [10:54<4:21:52, 163.67s/it]  5%|▌         | 5/100 [13:38<4:19:03, 163.61s/it]  6%|▌         | 6/100 [16:21<4:16:15, 163.57s/it]  7%|▋         | 7/100 [19:05<4:13:30, 163.55s/it]  8%|▊         | 8/100 [21:48<4:10:44, 163.53s/it]  9%|▉         | 9/100 [24:32<4:08:00, 163.52s/it] 10%|█         | 10/100 [27:15<4:05:22, 163.58s/it]Classification Train Epoch: 1 [0/48000 (0%)]	Loss: 2.131171, KL fake Loss: 0.036327
Classification Train Epoch: 1 [6400/48000 (13%)]	Loss: 0.586763, KL fake Loss: 0.384033
Classification Train Epoch: 1 [12800/48000 (27%)]	Loss: 0.417625, KL fake Loss: 0.347239
Classification Train Epoch: 1 [19200/48000 (40%)]	Loss: 0.444614, KL fake Loss: 0.215441
Classification Train Epoch: 1 [25600/48000 (53%)]	Loss: 0.286483, KL fake Loss: 0.251062
Classification Train Epoch: 1 [32000/48000 (67%)]	Loss: 0.272003, KL fake Loss: 2.475772
Classification Train Epoch: 1 [38400/48000 (80%)]	Loss: 0.345809, KL fake Loss: 0.180611
Classification Train Epoch: 1 [44800/48000 (93%)]	Loss: 0.329780, KL fake Loss: 0.116560

Test set: Average loss: 0.6872, Accuracy: 6770/8000 (85%)

Classification Train Epoch: 2 [0/48000 (0%)]	Loss: 0.452006, KL fake Loss: 0.086575
Classification Train Epoch: 2 [6400/48000 (13%)]	Loss: 0.421957, KL fake Loss: 1.147435
Classification Train Epoch: 2 [12800/48000 (27%)]	Loss: 0.278456, KL fake Loss: 0.221515
Classification Train Epoch: 2 [19200/48000 (40%)]	Loss: 0.407052, KL fake Loss: 0.154803
Classification Train Epoch: 2 [25600/48000 (53%)]	Loss: 0.181861, KL fake Loss: 0.093477
Classification Train Epoch: 2 [32000/48000 (67%)]	Loss: 0.173070, KL fake Loss: 0.137718
Classification Train Epoch: 2 [38400/48000 (80%)]	Loss: 0.110259, KL fake Loss: 0.170843
Classification Train Epoch: 2 [44800/48000 (93%)]	Loss: 0.316449, KL fake Loss: 0.142092

Test set: Average loss: 0.4158, Accuracy: 6932/8000 (87%)

Classification Train Epoch: 3 [0/48000 (0%)]	Loss: 0.294495, KL fake Loss: 0.158883
Classification Train Epoch: 3 [6400/48000 (13%)]	Loss: 0.305303, KL fake Loss: 0.067258
Classification Train Epoch: 3 [12800/48000 (27%)]	Loss: 0.199781, KL fake Loss: 0.054769
Classification Train Epoch: 3 [19200/48000 (40%)]	Loss: 0.345754, KL fake Loss: 0.051258
Classification Train Epoch: 3 [25600/48000 (53%)]	Loss: 0.304825, KL fake Loss: 0.049364
Classification Train Epoch: 3 [32000/48000 (67%)]	Loss: 0.229433, KL fake Loss: 0.052592
Classification Train Epoch: 3 [38400/48000 (80%)]	Loss: 0.440029, KL fake Loss: 0.072387
Classification Train Epoch: 3 [44800/48000 (93%)]	Loss: 0.149859, KL fake Loss: 0.043440

Test set: Average loss: 0.7077, Accuracy: 7221/8000 (90%)

Classification Train Epoch: 4 [0/48000 (0%)]	Loss: 0.291016, KL fake Loss: 0.032975
Classification Train Epoch: 4 [6400/48000 (13%)]	Loss: 0.266414, KL fake Loss: 0.047058
Classification Train Epoch: 4 [12800/48000 (27%)]	Loss: 0.179138, KL fake Loss: 0.037305
Classification Train Epoch: 4 [19200/48000 (40%)]	Loss: 0.141130, KL fake Loss: 0.045423
Classification Train Epoch: 4 [25600/48000 (53%)]	Loss: 0.348765, KL fake Loss: 0.060961
Classification Train Epoch: 4 [32000/48000 (67%)]	Loss: 0.341835, KL fake Loss: 0.047078
Classification Train Epoch: 4 [38400/48000 (80%)]	Loss: 0.218452, KL fake Loss: 1.923177
Classification Train Epoch: 4 [44800/48000 (93%)]	Loss: 0.260555, KL fake Loss: 0.283944

Test set: Average loss: 0.7116, Accuracy: 6930/8000 (87%)

Classification Train Epoch: 5 [0/48000 (0%)]	Loss: 0.160841, KL fake Loss: 0.074318
Classification Train Epoch: 5 [6400/48000 (13%)]	Loss: 0.162195, KL fake Loss: 0.040061
Classification Train Epoch: 5 [12800/48000 (27%)]	Loss: 0.248705, KL fake Loss: 0.211461
Classification Train Epoch: 5 [19200/48000 (40%)]	Loss: 0.239809, KL fake Loss: 0.041734
Classification Train Epoch: 5 [25600/48000 (53%)]	Loss: 0.352204, KL fake Loss: 2.412695
Classification Train Epoch: 5 [32000/48000 (67%)]	Loss: 0.268900, KL fake Loss: 0.840479
Classification Train Epoch: 5 [38400/48000 (80%)]	Loss: 0.180555, KL fake Loss: 0.066159
Classification Train Epoch: 5 [44800/48000 (93%)]	Loss: 0.231769, KL fake Loss: 0.030933

Test set: Average loss: 0.9031, Accuracy: 6778/8000 (85%)

Classification Train Epoch: 6 [0/48000 (0%)]	Loss: 0.190698, KL fake Loss: 0.028429
Classification Train Epoch: 6 [6400/48000 (13%)]	Loss: 0.199375, KL fake Loss: 0.036986
Classification Train Epoch: 6 [12800/48000 (27%)]	Loss: 0.222451, KL fake Loss: 0.034890
Classification Train Epoch: 6 [19200/48000 (40%)]	Loss: 0.087496, KL fake Loss: 0.030576
Classification Train Epoch: 6 [25600/48000 (53%)]	Loss: 0.084777, KL fake Loss: 0.034243
Classification Train Epoch: 6 [32000/48000 (67%)]	Loss: 0.300129, KL fake Loss: 0.044311
Classification Train Epoch: 6 [38400/48000 (80%)]	Loss: 0.288267, KL fake Loss: 0.027492
Classification Train Epoch: 6 [44800/48000 (93%)]	Loss: 0.120167, KL fake Loss: 0.027908

Test set: Average loss: 0.7115, Accuracy: 6988/8000 (87%)

Classification Train Epoch: 7 [0/48000 (0%)]	Loss: 0.270278, KL fake Loss: 0.027878
Classification Train Epoch: 7 [6400/48000 (13%)]	Loss: 0.129317, KL fake Loss: 0.020358
Classification Train Epoch: 7 [12800/48000 (27%)]	Loss: 0.254244, KL fake Loss: 0.028234
Classification Train Epoch: 7 [19200/48000 (40%)]	Loss: 0.236091, KL fake Loss: 0.024659
Classification Train Epoch: 7 [25600/48000 (53%)]	Loss: 0.117875, KL fake Loss: 0.022008
Classification Train Epoch: 7 [32000/48000 (67%)]	Loss: 0.076833, KL fake Loss: 0.022732
Classification Train Epoch: 7 [38400/48000 (80%)]	Loss: 0.279287, KL fake Loss: 0.033433
Classification Train Epoch: 7 [44800/48000 (93%)]	Loss: 0.129335, KL fake Loss: 0.025115

Test set: Average loss: 0.9445, Accuracy: 6958/8000 (87%)

Classification Train Epoch: 8 [0/48000 (0%)]	Loss: 0.184355, KL fake Loss: 0.019725
Classification Train Epoch: 8 [6400/48000 (13%)]	Loss: 0.200506, KL fake Loss: 0.018494
Classification Train Epoch: 8 [12800/48000 (27%)]	Loss: 0.191116, KL fake Loss: 0.020238
Classification Train Epoch: 8 [19200/48000 (40%)]	Loss: 0.182473, KL fake Loss: 0.059842
Classification Train Epoch: 8 [25600/48000 (53%)]	Loss: 0.158275, KL fake Loss: 0.021490
Classification Train Epoch: 8 [32000/48000 (67%)]	Loss: 0.185768, KL fake Loss: 0.029799
Classification Train Epoch: 8 [38400/48000 (80%)]	Loss: 0.156476, KL fake Loss: 0.054326
Classification Train Epoch: 8 [44800/48000 (93%)]	Loss: 0.098257, KL fake Loss: 0.021388

Test set: Average loss: 1.0321, Accuracy: 6845/8000 (86%)

Classification Train Epoch: 9 [0/48000 (0%)]	Loss: 0.076476, KL fake Loss: 0.055694
Classification Train Epoch: 9 [6400/48000 (13%)]	Loss: 0.364238, KL fake Loss: 0.016768
Classification Train Epoch: 9 [12800/48000 (27%)]	Loss: 0.172570, KL fake Loss: 0.022800
Classification Train Epoch: 9 [19200/48000 (40%)]	Loss: 0.099562, KL fake Loss: 0.028072
Classification Train Epoch: 9 [25600/48000 (53%)]	Loss: 0.185684, KL fake Loss: 0.158417
Classification Train Epoch: 9 [32000/48000 (67%)]	Loss: 0.158328, KL fake Loss: 0.022599
Classification Train Epoch: 9 [38400/48000 (80%)]	Loss: 0.151862, KL fake Loss: 0.021627
Classification Train Epoch: 9 [44800/48000 (93%)]	Loss: 0.121952, KL fake Loss: 0.028305

Test set: Average loss: 1.2093, Accuracy: 6662/8000 (83%)

Classification Train Epoch: 10 [0/48000 (0%)]	Loss: 0.286409, KL fake Loss: 0.013920
Classification Train Epoch: 10 [6400/48000 (13%)]	Loss: 0.048023, KL fake Loss: 0.011781
Classification Train Epoch: 10 [12800/48000 (27%)]	Loss: 0.127815, KL fake Loss: 0.022627
Classification Train Epoch: 10 [19200/48000 (40%)]	Loss: 0.113229, KL fake Loss: 0.024209
Classification Train Epoch: 10 [25600/48000 (53%)]	Loss: 0.262630, KL fake Loss: 0.155877
Classification Train Epoch: 10 [32000/48000 (67%)]	Loss: 0.210188, KL fake Loss: 0.041305
Classification Train Epoch: 10 [38400/48000 (80%)]	Loss: 0.124232, KL fake Loss: 0.092937
Classification Train Epoch: 10 [44800/48000 (93%)]	Loss: 0.207720, KL fake Loss: 0.024718

Test set: Average loss: 0.8384, Accuracy: 7013/8000 (88%)

Classification Train Epoch: 11 [0/48000 (0%)]	Loss: 0.268253, KL fake Loss: 0.023377
Classification Train Epoch: 11 [6400/48000 (13%)]	Loss: 0.155584, KL fake Loss: 0.035317
Classification Train Epoch: 11 [12800/48000 (27%)]	Loss: 0.181085, KL fake Loss: 0.021195
Classification Train Epoch: 11 [19200/48000 (40%)]	Loss: 0.097098, KL fake Loss: 0.014697
Classification Train Epoch: 11 [25600/48000 (53%)]	Loss: 0.044298, KL fake Loss: 0.025230
 11%|█         | 11/100 [29:59<4:02:42, 163.62s/it] 12%|█▏        | 12/100 [32:43<3:59:56, 163.60s/it] 13%|█▎        | 13/100 [35:26<3:57:10, 163.57s/it] 14%|█▍        | 14/100 [38:10<3:54:25, 163.55s/it] 15%|█▌        | 15/100 [40:53<3:51:40, 163.53s/it] 16%|█▌        | 16/100 [43:37<3:48:56, 163.53s/it] 17%|█▋        | 17/100 [46:20<3:46:13, 163.54s/it] 18%|█▊        | 18/100 [49:04<3:43:34, 163.60s/it] 19%|█▉        | 19/100 [51:48<3:40:54, 163.64s/it] 20%|██        | 20/100 [54:31<3:38:09, 163.62s/it] 21%|██        | 21/100 [57:15<3:35:23, 163.59s/it]Classification Train Epoch: 11 [32000/48000 (67%)]	Loss: 0.094821, KL fake Loss: 0.011218
Classification Train Epoch: 11 [38400/48000 (80%)]	Loss: 0.251195, KL fake Loss: 0.017970
Classification Train Epoch: 11 [44800/48000 (93%)]	Loss: 0.099577, KL fake Loss: 0.031535

Test set: Average loss: 1.2930, Accuracy: 6108/8000 (76%)

Classification Train Epoch: 12 [0/48000 (0%)]	Loss: 0.051998, KL fake Loss: 0.031420
Classification Train Epoch: 12 [6400/48000 (13%)]	Loss: 0.029075, KL fake Loss: 0.014145
Classification Train Epoch: 12 [12800/48000 (27%)]	Loss: 0.125316, KL fake Loss: 0.012465
Classification Train Epoch: 12 [19200/48000 (40%)]	Loss: 0.082641, KL fake Loss: 0.024691
Classification Train Epoch: 12 [25600/48000 (53%)]	Loss: 0.241854, KL fake Loss: 0.022942
Classification Train Epoch: 12 [32000/48000 (67%)]	Loss: 0.337084, KL fake Loss: 0.013969
Classification Train Epoch: 12 [38400/48000 (80%)]	Loss: 0.103250, KL fake Loss: 0.018210
Classification Train Epoch: 12 [44800/48000 (93%)]	Loss: 0.180372, KL fake Loss: 0.032367

Test set: Average loss: 1.2497, Accuracy: 6835/8000 (85%)

Classification Train Epoch: 13 [0/48000 (0%)]	Loss: 0.118757, KL fake Loss: 0.021787
Classification Train Epoch: 13 [6400/48000 (13%)]	Loss: 0.049056, KL fake Loss: 0.021399
Classification Train Epoch: 13 [12800/48000 (27%)]	Loss: 0.041510, KL fake Loss: 0.011227
Classification Train Epoch: 13 [19200/48000 (40%)]	Loss: 0.064734, KL fake Loss: 0.030429
Classification Train Epoch: 13 [25600/48000 (53%)]	Loss: 0.042899, KL fake Loss: 0.023273
Classification Train Epoch: 13 [32000/48000 (67%)]	Loss: 0.148688, KL fake Loss: 0.018545
Classification Train Epoch: 13 [38400/48000 (80%)]	Loss: 0.078693, KL fake Loss: 0.015772
Classification Train Epoch: 13 [44800/48000 (93%)]	Loss: 0.220325, KL fake Loss: 0.015152

Test set: Average loss: 1.0874, Accuracy: 6660/8000 (83%)

Classification Train Epoch: 14 [0/48000 (0%)]	Loss: 0.059509, KL fake Loss: 0.012973
Classification Train Epoch: 14 [6400/48000 (13%)]	Loss: 0.018503, KL fake Loss: 0.018529
Classification Train Epoch: 14 [12800/48000 (27%)]	Loss: 0.051994, KL fake Loss: 0.013607
Classification Train Epoch: 14 [19200/48000 (40%)]	Loss: 0.077409, KL fake Loss: 0.021652
Classification Train Epoch: 14 [25600/48000 (53%)]	Loss: 0.039243, KL fake Loss: 0.014480
Classification Train Epoch: 14 [32000/48000 (67%)]	Loss: 0.049005, KL fake Loss: 0.025173
Classification Train Epoch: 14 [38400/48000 (80%)]	Loss: 0.041547, KL fake Loss: 0.012531
Classification Train Epoch: 14 [44800/48000 (93%)]	Loss: 0.065238, KL fake Loss: 0.012166

Test set: Average loss: 1.1847, Accuracy: 6735/8000 (84%)

Classification Train Epoch: 15 [0/48000 (0%)]	Loss: 0.069922, KL fake Loss: 0.013516
Classification Train Epoch: 15 [6400/48000 (13%)]	Loss: 0.097162, KL fake Loss: 0.008924
Classification Train Epoch: 15 [12800/48000 (27%)]	Loss: 0.094507, KL fake Loss: 0.017214
Classification Train Epoch: 15 [19200/48000 (40%)]	Loss: 0.089522, KL fake Loss: 0.018483
Classification Train Epoch: 15 [25600/48000 (53%)]	Loss: 0.162326, KL fake Loss: 0.050937
Classification Train Epoch: 15 [32000/48000 (67%)]	Loss: 0.130969, KL fake Loss: 0.091186
Classification Train Epoch: 15 [38400/48000 (80%)]	Loss: 0.080180, KL fake Loss: 0.015626
Classification Train Epoch: 15 [44800/48000 (93%)]	Loss: 0.076421, KL fake Loss: 0.019735

Test set: Average loss: 1.4421, Accuracy: 6545/8000 (82%)

Classification Train Epoch: 16 [0/48000 (0%)]	Loss: 0.038005, KL fake Loss: 0.018750
Classification Train Epoch: 16 [6400/48000 (13%)]	Loss: 0.112811, KL fake Loss: 0.015607
Classification Train Epoch: 16 [12800/48000 (27%)]	Loss: 0.062032, KL fake Loss: 5.793524
Classification Train Epoch: 16 [19200/48000 (40%)]	Loss: 0.049247, KL fake Loss: 0.028599
Classification Train Epoch: 16 [25600/48000 (53%)]	Loss: 0.077659, KL fake Loss: 0.097396
Classification Train Epoch: 16 [32000/48000 (67%)]	Loss: 0.102932, KL fake Loss: 0.022596
Classification Train Epoch: 16 [38400/48000 (80%)]	Loss: 0.067873, KL fake Loss: 0.016192
Classification Train Epoch: 16 [44800/48000 (93%)]	Loss: 0.167923, KL fake Loss: 0.024909

Test set: Average loss: 0.9776, Accuracy: 6948/8000 (87%)

Classification Train Epoch: 17 [0/48000 (0%)]	Loss: 0.082712, KL fake Loss: 0.026860
Classification Train Epoch: 17 [6400/48000 (13%)]	Loss: 0.061298, KL fake Loss: 0.016574
Classification Train Epoch: 17 [12800/48000 (27%)]	Loss: 0.188085, KL fake Loss: 0.021991
Classification Train Epoch: 17 [19200/48000 (40%)]	Loss: 0.038024, KL fake Loss: 0.014753
Classification Train Epoch: 17 [25600/48000 (53%)]	Loss: 0.133362, KL fake Loss: 0.938178
Classification Train Epoch: 17 [32000/48000 (67%)]	Loss: 0.105737, KL fake Loss: 0.020019
Classification Train Epoch: 17 [38400/48000 (80%)]	Loss: 0.128183, KL fake Loss: 0.018802
Classification Train Epoch: 17 [44800/48000 (93%)]	Loss: 0.029782, KL fake Loss: 0.011006

Test set: Average loss: 1.3213, Accuracy: 6420/8000 (80%)

Classification Train Epoch: 18 [0/48000 (0%)]	Loss: 0.008867, KL fake Loss: 0.022962
Classification Train Epoch: 18 [6400/48000 (13%)]	Loss: 0.033465, KL fake Loss: 0.008243
Classification Train Epoch: 18 [12800/48000 (27%)]	Loss: 0.061185, KL fake Loss: 0.010891
Classification Train Epoch: 18 [19200/48000 (40%)]	Loss: 0.079097, KL fake Loss: 0.013957
Classification Train Epoch: 18 [25600/48000 (53%)]	Loss: 0.030739, KL fake Loss: 0.010148
Classification Train Epoch: 18 [32000/48000 (67%)]	Loss: 0.098565, KL fake Loss: 0.011127
Classification Train Epoch: 18 [38400/48000 (80%)]	Loss: 0.030925, KL fake Loss: 0.007352
Classification Train Epoch: 18 [44800/48000 (93%)]	Loss: 0.185075, KL fake Loss: 0.011303

Test set: Average loss: 1.2376, Accuracy: 6771/8000 (85%)

Classification Train Epoch: 19 [0/48000 (0%)]	Loss: 0.050236, KL fake Loss: 0.013069
Classification Train Epoch: 19 [6400/48000 (13%)]	Loss: 0.061129, KL fake Loss: 0.385739
Classification Train Epoch: 19 [12800/48000 (27%)]	Loss: 0.013243, KL fake Loss: 0.014832
Classification Train Epoch: 19 [19200/48000 (40%)]	Loss: 0.112531, KL fake Loss: 0.009034
Classification Train Epoch: 19 [25600/48000 (53%)]	Loss: 0.016658, KL fake Loss: 0.008837
Classification Train Epoch: 19 [32000/48000 (67%)]	Loss: 0.082094, KL fake Loss: 0.013463
Classification Train Epoch: 19 [38400/48000 (80%)]	Loss: 0.023330, KL fake Loss: 0.011993
Classification Train Epoch: 19 [44800/48000 (93%)]	Loss: 0.043051, KL fake Loss: 0.017730

Test set: Average loss: 1.4538, Accuracy: 6438/8000 (80%)

Classification Train Epoch: 20 [0/48000 (0%)]	Loss: 0.021791, KL fake Loss: 0.009103
Classification Train Epoch: 20 [6400/48000 (13%)]	Loss: 0.002993, KL fake Loss: 0.007635
Classification Train Epoch: 20 [12800/48000 (27%)]	Loss: 0.039920, KL fake Loss: 0.005919
Classification Train Epoch: 20 [19200/48000 (40%)]	Loss: 0.007938, KL fake Loss: 0.006633
Classification Train Epoch: 20 [25600/48000 (53%)]	Loss: 0.068247, KL fake Loss: 0.007893
Classification Train Epoch: 20 [32000/48000 (67%)]	Loss: 0.037938, KL fake Loss: 0.015185
Classification Train Epoch: 20 [38400/48000 (80%)]	Loss: 0.020701, KL fake Loss: 0.012869
Classification Train Epoch: 20 [44800/48000 (93%)]	Loss: 0.124988, KL fake Loss: 0.016017

Test set: Average loss: 1.3482, Accuracy: 6631/8000 (83%)

Classification Train Epoch: 21 [0/48000 (0%)]	Loss: 0.007952, KL fake Loss: 0.008253
Classification Train Epoch: 21 [6400/48000 (13%)]	Loss: 0.107203, KL fake Loss: 0.007194
Classification Train Epoch: 21 [12800/48000 (27%)]	Loss: 0.018370, KL fake Loss: 0.008599
Classification Train Epoch: 21 [19200/48000 (40%)]	Loss: 0.223817, KL fake Loss: 0.013675
Classification Train Epoch: 21 [25600/48000 (53%)]	Loss: 0.109977, KL fake Loss: 0.014073
Classification Train Epoch: 21 [32000/48000 (67%)]	Loss: 0.128540, KL fake Loss: 0.015397
Classification Train Epoch: 21 [38400/48000 (80%)]	Loss: 0.049247, KL fake Loss: 0.006776
Classification Train Epoch: 21 [44800/48000 (93%)]	Loss: 0.075903, KL fake Loss: 0.006504

Test set: Average loss: 1.3939, Accuracy: 6413/8000 (80%)

Classification Train Epoch: 22 [0/48000 (0%)]	Loss: 0.005235, KL fake Loss: 0.010986
 22%|██▏       | 22/100 [59:58<3:32:38, 163.57s/it] 23%|██▎       | 23/100 [1:02:42<3:29:52, 163.54s/it] 24%|██▍       | 24/100 [1:05:25<3:27:08, 163.53s/it] 25%|██▌       | 25/100 [1:08:09<3:24:24, 163.53s/it] 26%|██▌       | 26/100 [1:10:52<3:21:41, 163.53s/it] 27%|██▋       | 27/100 [1:13:36<3:18:57, 163.53s/it] 28%|██▊       | 28/100 [1:16:20<3:16:15, 163.55s/it] 29%|██▉       | 29/100 [1:19:03<3:13:30, 163.53s/it] 30%|███       | 30/100 [1:21:47<3:10:46, 163.53s/it] 31%|███       | 31/100 [1:24:30<3:08:02, 163.52s/it]Classification Train Epoch: 22 [6400/48000 (13%)]	Loss: 0.152851, KL fake Loss: 0.008659
Classification Train Epoch: 22 [12800/48000 (27%)]	Loss: 0.071897, KL fake Loss: 0.009489
Classification Train Epoch: 22 [19200/48000 (40%)]	Loss: 0.053052, KL fake Loss: 0.011212
Classification Train Epoch: 22 [25600/48000 (53%)]	Loss: 0.034009, KL fake Loss: 0.020391
Classification Train Epoch: 22 [32000/48000 (67%)]	Loss: 0.053796, KL fake Loss: 0.021490
Classification Train Epoch: 22 [38400/48000 (80%)]	Loss: 0.010923, KL fake Loss: 0.008985
Classification Train Epoch: 22 [44800/48000 (93%)]	Loss: 0.169248, KL fake Loss: 0.094581

Test set: Average loss: 1.1144, Accuracy: 7017/8000 (88%)

Classification Train Epoch: 23 [0/48000 (0%)]	Loss: 0.080798, KL fake Loss: 0.029809
Classification Train Epoch: 23 [6400/48000 (13%)]	Loss: 0.086322, KL fake Loss: 6.014643
Classification Train Epoch: 23 [12800/48000 (27%)]	Loss: 0.070123, KL fake Loss: 0.023219
Classification Train Epoch: 23 [19200/48000 (40%)]	Loss: 0.043721, KL fake Loss: 0.018593
Classification Train Epoch: 23 [25600/48000 (53%)]	Loss: 0.020310, KL fake Loss: 0.014199
Classification Train Epoch: 23 [32000/48000 (67%)]	Loss: 0.086648, KL fake Loss: 0.014586
Classification Train Epoch: 23 [38400/48000 (80%)]	Loss: 0.035756, KL fake Loss: 0.009434
Classification Train Epoch: 23 [44800/48000 (93%)]	Loss: 0.022668, KL fake Loss: 0.016861

Test set: Average loss: 1.4260, Accuracy: 6706/8000 (84%)

Classification Train Epoch: 24 [0/48000 (0%)]	Loss: 0.012716, KL fake Loss: 0.009043
Classification Train Epoch: 24 [6400/48000 (13%)]	Loss: 0.016526, KL fake Loss: 0.014007
Classification Train Epoch: 24 [12800/48000 (27%)]	Loss: 0.007926, KL fake Loss: 0.007478
Classification Train Epoch: 24 [19200/48000 (40%)]	Loss: 0.016093, KL fake Loss: 0.007797
Classification Train Epoch: 24 [25600/48000 (53%)]	Loss: 0.003616, KL fake Loss: 0.007981
Classification Train Epoch: 24 [32000/48000 (67%)]	Loss: 0.044549, KL fake Loss: 0.011381
Classification Train Epoch: 24 [38400/48000 (80%)]	Loss: 0.098346, KL fake Loss: 0.020210
Classification Train Epoch: 24 [44800/48000 (93%)]	Loss: 0.015846, KL fake Loss: 0.008527

Test set: Average loss: 1.4713, Accuracy: 6706/8000 (84%)

Classification Train Epoch: 25 [0/48000 (0%)]	Loss: 0.054067, KL fake Loss: 0.007983
Classification Train Epoch: 25 [6400/48000 (13%)]	Loss: 0.007128, KL fake Loss: 0.006323
Classification Train Epoch: 25 [12800/48000 (27%)]	Loss: 0.019395, KL fake Loss: 0.005755
Classification Train Epoch: 25 [19200/48000 (40%)]	Loss: 0.028215, KL fake Loss: 0.011754
Classification Train Epoch: 25 [25600/48000 (53%)]	Loss: 0.016535, KL fake Loss: 0.020006
Classification Train Epoch: 25 [32000/48000 (67%)]	Loss: 0.029009, KL fake Loss: 0.017631
Classification Train Epoch: 25 [38400/48000 (80%)]	Loss: 0.054837, KL fake Loss: 0.010067
Classification Train Epoch: 25 [44800/48000 (93%)]	Loss: 0.099528, KL fake Loss: 0.013481

Test set: Average loss: 1.1298, Accuracy: 7131/8000 (89%)

Classification Train Epoch: 26 [0/48000 (0%)]	Loss: 0.009867, KL fake Loss: 0.009173
Classification Train Epoch: 26 [6400/48000 (13%)]	Loss: 0.012542, KL fake Loss: 0.013717
Classification Train Epoch: 26 [12800/48000 (27%)]	Loss: 0.014476, KL fake Loss: 0.009694
Classification Train Epoch: 26 [19200/48000 (40%)]	Loss: 0.052959, KL fake Loss: 0.018279
Classification Train Epoch: 26 [25600/48000 (53%)]	Loss: 0.008476, KL fake Loss: 0.008133
Classification Train Epoch: 26 [32000/48000 (67%)]	Loss: 0.003600, KL fake Loss: 0.007874
Classification Train Epoch: 26 [38400/48000 (80%)]	Loss: 0.104266, KL fake Loss: 0.084306
Classification Train Epoch: 26 [44800/48000 (93%)]	Loss: 0.017856, KL fake Loss: 0.009345

Test set: Average loss: 1.4581, Accuracy: 6747/8000 (84%)

Classification Train Epoch: 27 [0/48000 (0%)]	Loss: 0.036620, KL fake Loss: 0.005226
Classification Train Epoch: 27 [6400/48000 (13%)]	Loss: 0.069527, KL fake Loss: 0.012750
Classification Train Epoch: 27 [12800/48000 (27%)]	Loss: 0.005400, KL fake Loss: 0.013224
Classification Train Epoch: 27 [19200/48000 (40%)]	Loss: 0.043125, KL fake Loss: 0.006273
Classification Train Epoch: 27 [25600/48000 (53%)]	Loss: 0.016364, KL fake Loss: 0.011269
Classification Train Epoch: 27 [32000/48000 (67%)]	Loss: 0.090941, KL fake Loss: 0.028813
Classification Train Epoch: 27 [38400/48000 (80%)]	Loss: 0.014715, KL fake Loss: 0.006770
Classification Train Epoch: 27 [44800/48000 (93%)]	Loss: 0.037880, KL fake Loss: 0.011919

Test set: Average loss: 1.4611, Accuracy: 6030/8000 (75%)

Classification Train Epoch: 28 [0/48000 (0%)]	Loss: 0.010096, KL fake Loss: 0.021591
Classification Train Epoch: 28 [6400/48000 (13%)]	Loss: 0.012588, KL fake Loss: 0.005718
Classification Train Epoch: 28 [12800/48000 (27%)]	Loss: 0.004205, KL fake Loss: 0.015659
Classification Train Epoch: 28 [19200/48000 (40%)]	Loss: 0.024426, KL fake Loss: 0.005963
Classification Train Epoch: 28 [25600/48000 (53%)]	Loss: 0.035249, KL fake Loss: 0.011723
Classification Train Epoch: 28 [32000/48000 (67%)]	Loss: 0.040224, KL fake Loss: 0.010505
Classification Train Epoch: 28 [38400/48000 (80%)]	Loss: 0.015874, KL fake Loss: 0.008222
Classification Train Epoch: 28 [44800/48000 (93%)]	Loss: 0.012039, KL fake Loss: 0.007763

Test set: Average loss: 1.4133, Accuracy: 6884/8000 (86%)

Classification Train Epoch: 29 [0/48000 (0%)]	Loss: 0.038750, KL fake Loss: 0.013323
Classification Train Epoch: 29 [6400/48000 (13%)]	Loss: 0.044462, KL fake Loss: 0.009730
Classification Train Epoch: 29 [12800/48000 (27%)]	Loss: 0.038428, KL fake Loss: 0.009248
Classification Train Epoch: 29 [19200/48000 (40%)]	Loss: 0.103645, KL fake Loss: 0.015982
Classification Train Epoch: 29 [25600/48000 (53%)]	Loss: 0.036289, KL fake Loss: 0.012727
Classification Train Epoch: 29 [32000/48000 (67%)]	Loss: 0.006902, KL fake Loss: 0.009289
Classification Train Epoch: 29 [38400/48000 (80%)]	Loss: 0.006997, KL fake Loss: 0.008508
Classification Train Epoch: 29 [44800/48000 (93%)]	Loss: 0.008944, KL fake Loss: 0.010131

Test set: Average loss: 1.6969, Accuracy: 5595/8000 (70%)

Classification Train Epoch: 30 [0/48000 (0%)]	Loss: 0.070748, KL fake Loss: 0.010563
Classification Train Epoch: 30 [6400/48000 (13%)]	Loss: 0.101042, KL fake Loss: 0.051524
Classification Train Epoch: 30 [12800/48000 (27%)]	Loss: 0.037681, KL fake Loss: 0.132528
Classification Train Epoch: 30 [19200/48000 (40%)]	Loss: 0.058835, KL fake Loss: 0.020288
Classification Train Epoch: 30 [25600/48000 (53%)]	Loss: 0.079226, KL fake Loss: 0.015653
Classification Train Epoch: 30 [32000/48000 (67%)]	Loss: 0.030008, KL fake Loss: 0.015357
Classification Train Epoch: 30 [38400/48000 (80%)]	Loss: 0.013981, KL fake Loss: 0.009380
Classification Train Epoch: 30 [44800/48000 (93%)]	Loss: 0.053488, KL fake Loss: 0.011503

Test set: Average loss: 1.0199, Accuracy: 6764/8000 (85%)

Classification Train Epoch: 31 [0/48000 (0%)]	Loss: 0.057299, KL fake Loss: 0.012147
Classification Train Epoch: 31 [6400/48000 (13%)]	Loss: 0.102777, KL fake Loss: 0.059557
Classification Train Epoch: 31 [12800/48000 (27%)]	Loss: 0.012094, KL fake Loss: 0.006479
Classification Train Epoch: 31 [19200/48000 (40%)]	Loss: 0.031444, KL fake Loss: 0.008100
Classification Train Epoch: 31 [25600/48000 (53%)]	Loss: 0.015333, KL fake Loss: 0.005744
Classification Train Epoch: 31 [32000/48000 (67%)]	Loss: 0.002111, KL fake Loss: 0.008935
Classification Train Epoch: 31 [38400/48000 (80%)]	Loss: 0.039320, KL fake Loss: 0.008094
Classification Train Epoch: 31 [44800/48000 (93%)]	Loss: 0.025213, KL fake Loss: 0.005008

Test set: Average loss: 1.4932, Accuracy: 6997/8000 (87%)

Classification Train Epoch: 32 [0/48000 (0%)]	Loss: 0.016279, KL fake Loss: 0.007621
Classification Train Epoch: 32 [6400/48000 (13%)]	Loss: 0.004803, KL fake Loss: 0.006185
Classification Train Epoch: 32 [12800/48000 (27%)]	Loss: 0.028216, KL fake Loss: 0.005655
Classification Train Epoch: 32 [19200/48000 (40%)]	Loss: 0.036594, KL fake Loss: 0.024185
Classification Train Epoch: 32 [25600/48000 (53%)]	Loss: 0.029057, KL fake Loss: 0.007103
Classification Train Epoch: 32 [32000/48000 (67%)]	Loss: 0.038993, KL fake Loss: 0.005718
 32%|███▏      | 32/100 [1:27:14<3:05:18, 163.50s/it] 33%|███▎      | 33/100 [1:29:57<3:02:34, 163.50s/it] 34%|███▍      | 34/100 [1:32:41<2:59:51, 163.50s/it] 35%|███▌      | 35/100 [1:35:24<2:57:07, 163.50s/it] 36%|███▌      | 36/100 [1:38:08<2:54:23, 163.50s/it] 37%|███▋      | 37/100 [1:40:51<2:51:40, 163.49s/it] 38%|███▊      | 38/100 [1:43:35<2:48:56, 163.49s/it] 39%|███▉      | 39/100 [1:46:18<2:46:13, 163.49s/it] 40%|████      | 40/100 [1:49:02<2:43:31, 163.53s/it] 41%|████      | 41/100 [1:51:45<2:40:47, 163.51s/it] 42%|████▏     | 42/100 [1:54:29<2:38:03, 163.50s/it]Classification Train Epoch: 32 [38400/48000 (80%)]	Loss: 0.057864, KL fake Loss: 0.008639
Classification Train Epoch: 32 [44800/48000 (93%)]	Loss: 0.009230, KL fake Loss: 0.006485

Test set: Average loss: 1.4639, Accuracy: 6908/8000 (86%)

Classification Train Epoch: 33 [0/48000 (0%)]	Loss: 0.025170, KL fake Loss: 0.009235
Classification Train Epoch: 33 [6400/48000 (13%)]	Loss: 0.071894, KL fake Loss: 0.014868
Classification Train Epoch: 33 [12800/48000 (27%)]	Loss: 0.003261, KL fake Loss: 0.009608
Classification Train Epoch: 33 [19200/48000 (40%)]	Loss: 0.001074, KL fake Loss: 0.005872
Classification Train Epoch: 33 [25600/48000 (53%)]	Loss: 0.017657, KL fake Loss: 0.011271
Classification Train Epoch: 33 [32000/48000 (67%)]	Loss: 0.019147, KL fake Loss: 0.012547
Classification Train Epoch: 33 [38400/48000 (80%)]	Loss: 0.081123, KL fake Loss: 0.007392
Classification Train Epoch: 33 [44800/48000 (93%)]	Loss: 0.002298, KL fake Loss: 0.006111

Test set: Average loss: 1.3517, Accuracy: 7034/8000 (88%)

Classification Train Epoch: 34 [0/48000 (0%)]	Loss: 0.045614, KL fake Loss: 0.019430
Classification Train Epoch: 34 [6400/48000 (13%)]	Loss: 0.011587, KL fake Loss: 0.012520
Classification Train Epoch: 34 [12800/48000 (27%)]	Loss: 0.025480, KL fake Loss: 0.029374
Classification Train Epoch: 34 [19200/48000 (40%)]	Loss: 0.030359, KL fake Loss: 0.011733
Classification Train Epoch: 34 [25600/48000 (53%)]	Loss: 0.024982, KL fake Loss: 0.008249
Classification Train Epoch: 34 [32000/48000 (67%)]	Loss: 0.019155, KL fake Loss: 0.005302
Classification Train Epoch: 34 [38400/48000 (80%)]	Loss: 0.042156, KL fake Loss: 0.004398
Classification Train Epoch: 34 [44800/48000 (93%)]	Loss: 0.022628, KL fake Loss: 0.005546

Test set: Average loss: 1.6396, Accuracy: 6594/8000 (82%)

Classification Train Epoch: 35 [0/48000 (0%)]	Loss: 0.007121, KL fake Loss: 0.008533
Classification Train Epoch: 35 [6400/48000 (13%)]	Loss: 0.001416, KL fake Loss: 0.003651
Classification Train Epoch: 35 [12800/48000 (27%)]	Loss: 0.001802, KL fake Loss: 0.011623
Classification Train Epoch: 35 [19200/48000 (40%)]	Loss: 0.006998, KL fake Loss: 0.031019
Classification Train Epoch: 35 [25600/48000 (53%)]	Loss: 0.002572, KL fake Loss: 0.006628
Classification Train Epoch: 35 [32000/48000 (67%)]	Loss: 0.015103, KL fake Loss: 0.014151
Classification Train Epoch: 35 [38400/48000 (80%)]	Loss: 0.019266, KL fake Loss: 0.007303
Classification Train Epoch: 35 [44800/48000 (93%)]	Loss: 0.000636, KL fake Loss: 0.007042

Test set: Average loss: 1.5316, Accuracy: 6653/8000 (83%)

Classification Train Epoch: 36 [0/48000 (0%)]	Loss: 0.005364, KL fake Loss: 0.003848
Classification Train Epoch: 36 [6400/48000 (13%)]	Loss: 0.048077, KL fake Loss: 0.010577
Classification Train Epoch: 36 [12800/48000 (27%)]	Loss: 0.005876, KL fake Loss: 0.009222
Classification Train Epoch: 36 [19200/48000 (40%)]	Loss: 0.076088, KL fake Loss: 0.009002
Classification Train Epoch: 36 [25600/48000 (53%)]	Loss: 0.001780, KL fake Loss: 0.021161
Classification Train Epoch: 36 [32000/48000 (67%)]	Loss: 0.025320, KL fake Loss: 0.003797
Classification Train Epoch: 36 [38400/48000 (80%)]	Loss: 0.056753, KL fake Loss: 0.012831
Classification Train Epoch: 36 [44800/48000 (93%)]	Loss: 0.001003, KL fake Loss: 0.011150

Test set: Average loss: 1.5868, Accuracy: 6732/8000 (84%)

Classification Train Epoch: 37 [0/48000 (0%)]	Loss: 0.025300, KL fake Loss: 0.015706
Classification Train Epoch: 37 [6400/48000 (13%)]	Loss: 0.012847, KL fake Loss: 0.008041
Classification Train Epoch: 37 [12800/48000 (27%)]	Loss: 0.024516, KL fake Loss: 0.004854
Classification Train Epoch: 37 [19200/48000 (40%)]	Loss: 0.002358, KL fake Loss: 0.007315
Classification Train Epoch: 37 [25600/48000 (53%)]	Loss: 0.014850, KL fake Loss: 0.006998
Classification Train Epoch: 37 [32000/48000 (67%)]	Loss: 0.102111, KL fake Loss: 0.004401
Classification Train Epoch: 37 [38400/48000 (80%)]	Loss: 0.004151, KL fake Loss: 0.006812
Classification Train Epoch: 37 [44800/48000 (93%)]	Loss: 0.016545, KL fake Loss: 0.012464

Test set: Average loss: 0.6281, Accuracy: 6726/8000 (84%)

Classification Train Epoch: 38 [0/48000 (0%)]	Loss: 0.047510, KL fake Loss: 0.241534
Classification Train Epoch: 38 [6400/48000 (13%)]	Loss: 0.006653, KL fake Loss: 0.005398
Classification Train Epoch: 38 [12800/48000 (27%)]	Loss: 0.004062, KL fake Loss: 0.008949
Classification Train Epoch: 38 [19200/48000 (40%)]	Loss: 0.000949, KL fake Loss: 0.005512
Classification Train Epoch: 38 [25600/48000 (53%)]	Loss: 0.002784, KL fake Loss: 0.010156
Classification Train Epoch: 38 [32000/48000 (67%)]	Loss: 0.004330, KL fake Loss: 0.004731
Classification Train Epoch: 38 [38400/48000 (80%)]	Loss: 0.006806, KL fake Loss: 0.004785
Classification Train Epoch: 38 [44800/48000 (93%)]	Loss: 0.007629, KL fake Loss: 0.006555

Test set: Average loss: 1.5557, Accuracy: 6851/8000 (86%)

Classification Train Epoch: 39 [0/48000 (0%)]	Loss: 0.004606, KL fake Loss: 0.006158
Classification Train Epoch: 39 [6400/48000 (13%)]	Loss: 0.007055, KL fake Loss: 0.002932
Classification Train Epoch: 39 [12800/48000 (27%)]	Loss: 0.040505, KL fake Loss: 0.006871
Classification Train Epoch: 39 [19200/48000 (40%)]	Loss: 0.006858, KL fake Loss: 0.011211
Classification Train Epoch: 39 [25600/48000 (53%)]	Loss: 0.007308, KL fake Loss: 0.007764
Classification Train Epoch: 39 [32000/48000 (67%)]	Loss: 0.080508, KL fake Loss: 0.014540
Classification Train Epoch: 39 [38400/48000 (80%)]	Loss: 0.001051, KL fake Loss: 0.009837
Classification Train Epoch: 39 [44800/48000 (93%)]	Loss: 0.043383, KL fake Loss: 0.010750

Test set: Average loss: 1.6896, Accuracy: 6361/8000 (80%)

Classification Train Epoch: 40 [0/48000 (0%)]	Loss: 0.005790, KL fake Loss: 0.004398
Classification Train Epoch: 40 [6400/48000 (13%)]	Loss: 0.041600, KL fake Loss: 0.003305
Classification Train Epoch: 40 [12800/48000 (27%)]	Loss: 0.008677, KL fake Loss: 0.004917
Classification Train Epoch: 40 [19200/48000 (40%)]	Loss: 0.016359, KL fake Loss: 0.002954
Classification Train Epoch: 40 [25600/48000 (53%)]	Loss: 0.007598, KL fake Loss: 0.006265
Classification Train Epoch: 40 [32000/48000 (67%)]	Loss: 0.046057, KL fake Loss: 0.014496
Classification Train Epoch: 40 [38400/48000 (80%)]	Loss: 0.024387, KL fake Loss: 0.018885
Classification Train Epoch: 40 [44800/48000 (93%)]	Loss: 0.058791, KL fake Loss: 0.013414

Test set: Average loss: 1.5022, Accuracy: 6778/8000 (85%)

Classification Train Epoch: 41 [0/48000 (0%)]	Loss: 0.006491, KL fake Loss: 0.004895
Classification Train Epoch: 41 [6400/48000 (13%)]	Loss: 0.057666, KL fake Loss: 0.004882
Classification Train Epoch: 41 [12800/48000 (27%)]	Loss: 0.000612, KL fake Loss: 0.003364
Classification Train Epoch: 41 [19200/48000 (40%)]	Loss: 0.007275, KL fake Loss: 0.002963
Classification Train Epoch: 41 [25600/48000 (53%)]	Loss: 0.037104, KL fake Loss: 0.009877
Classification Train Epoch: 41 [32000/48000 (67%)]	Loss: 0.045279, KL fake Loss: 0.007806
Classification Train Epoch: 41 [38400/48000 (80%)]	Loss: 0.047190, KL fake Loss: 0.006080
Classification Train Epoch: 41 [44800/48000 (93%)]	Loss: 0.009803, KL fake Loss: 0.005846

Test set: Average loss: 1.7220, Accuracy: 5790/8000 (72%)

Classification Train Epoch: 42 [0/48000 (0%)]	Loss: 0.113507, KL fake Loss: 0.004285
Classification Train Epoch: 42 [6400/48000 (13%)]	Loss: 0.000793, KL fake Loss: 0.013578
Classification Train Epoch: 42 [12800/48000 (27%)]	Loss: 0.005890, KL fake Loss: 0.004844
Classification Train Epoch: 42 [19200/48000 (40%)]	Loss: 0.009453, KL fake Loss: 0.008698
Classification Train Epoch: 42 [25600/48000 (53%)]	Loss: 0.065066, KL fake Loss: 0.003247
Classification Train Epoch: 42 [32000/48000 (67%)]	Loss: 0.019033, KL fake Loss: 0.002405
Classification Train Epoch: 42 [38400/48000 (80%)]	Loss: 0.002606, KL fake Loss: 0.004555
Classification Train Epoch: 42 [44800/48000 (93%)]	Loss: 0.006257, KL fake Loss: 0.003832

Test set: Average loss: 1.7695, Accuracy: 6755/8000 (84%)

Classification Train Epoch: 43 [0/48000 (0%)]	Loss: 0.000949, KL fake Loss: 0.003406
Classification Train Epoch: 43 [6400/48000 (13%)]	Loss: 0.005738, KL fake Loss: 0.004115
 43%|████▎     | 43/100 [1:57:12<2:35:19, 163.49s/it] 44%|████▍     | 44/100 [1:59:56<2:32:35, 163.49s/it] 45%|████▌     | 45/100 [2:02:39<2:29:51, 163.49s/it] 46%|████▌     | 46/100 [2:05:23<2:27:08, 163.49s/it] 47%|████▋     | 47/100 [2:08:06<2:24:25, 163.50s/it] 48%|████▊     | 48/100 [2:10:49<2:21:41, 163.48s/it] 49%|████▉     | 49/100 [2:13:33<2:18:57, 163.49s/it] 50%|█████     | 50/100 [2:16:16<2:16:14, 163.49s/it] 51%|█████     | 51/100 [2:19:00<2:13:31, 163.49s/it] 52%|█████▏    | 52/100 [2:21:43<2:10:47, 163.48s/it]Classification Train Epoch: 43 [12800/48000 (27%)]	Loss: 0.003118, KL fake Loss: 0.005314
Classification Train Epoch: 43 [19200/48000 (40%)]	Loss: 0.101106, KL fake Loss: 0.026564
Classification Train Epoch: 43 [25600/48000 (53%)]	Loss: 0.047151, KL fake Loss: 0.009703
Classification Train Epoch: 43 [32000/48000 (67%)]	Loss: 0.006070, KL fake Loss: 0.018958
Classification Train Epoch: 43 [38400/48000 (80%)]	Loss: 0.022704, KL fake Loss: 0.002425
Classification Train Epoch: 43 [44800/48000 (93%)]	Loss: 0.033408, KL fake Loss: 0.005124

Test set: Average loss: 1.6227, Accuracy: 6608/8000 (83%)

Classification Train Epoch: 44 [0/48000 (0%)]	Loss: 0.002184, KL fake Loss: 0.002676
Classification Train Epoch: 44 [6400/48000 (13%)]	Loss: 0.002700, KL fake Loss: 0.004530
Classification Train Epoch: 44 [12800/48000 (27%)]	Loss: 0.005623, KL fake Loss: 0.005930
Classification Train Epoch: 44 [19200/48000 (40%)]	Loss: 0.002708, KL fake Loss: 0.002766
Classification Train Epoch: 44 [25600/48000 (53%)]	Loss: 0.002579, KL fake Loss: 0.005176
Classification Train Epoch: 44 [32000/48000 (67%)]	Loss: 0.000726, KL fake Loss: 0.004291
Classification Train Epoch: 44 [38400/48000 (80%)]	Loss: 0.001865, KL fake Loss: 0.003627
Classification Train Epoch: 44 [44800/48000 (93%)]	Loss: 0.025192, KL fake Loss: 0.011143

Test set: Average loss: 1.7757, Accuracy: 6273/8000 (78%)

Classification Train Epoch: 45 [0/48000 (0%)]	Loss: 0.001358, KL fake Loss: 0.006286
Classification Train Epoch: 45 [6400/48000 (13%)]	Loss: 0.035943, KL fake Loss: 0.006325
Classification Train Epoch: 45 [12800/48000 (27%)]	Loss: 0.003210, KL fake Loss: 0.002826
Classification Train Epoch: 45 [19200/48000 (40%)]	Loss: 0.032590, KL fake Loss: 0.122402
Classification Train Epoch: 45 [25600/48000 (53%)]	Loss: 0.007166, KL fake Loss: 0.103561
Classification Train Epoch: 45 [32000/48000 (67%)]	Loss: 0.004651, KL fake Loss: 0.018167
Classification Train Epoch: 45 [38400/48000 (80%)]	Loss: 0.024669, KL fake Loss: 0.006346
Classification Train Epoch: 45 [44800/48000 (93%)]	Loss: 0.004488, KL fake Loss: 0.015927

Test set: Average loss: 1.4799, Accuracy: 7154/8000 (89%)

Classification Train Epoch: 46 [0/48000 (0%)]	Loss: 0.017529, KL fake Loss: 0.005659
Classification Train Epoch: 46 [6400/48000 (13%)]	Loss: 0.010676, KL fake Loss: 0.002982
Classification Train Epoch: 46 [12800/48000 (27%)]	Loss: 0.024108, KL fake Loss: 0.008956
Classification Train Epoch: 46 [19200/48000 (40%)]	Loss: 0.086448, KL fake Loss: 0.006183
Classification Train Epoch: 46 [25600/48000 (53%)]	Loss: 0.001046, KL fake Loss: 0.006317
Classification Train Epoch: 46 [32000/48000 (67%)]	Loss: 0.008430, KL fake Loss: 0.009454
Classification Train Epoch: 46 [38400/48000 (80%)]	Loss: 0.053134, KL fake Loss: 0.009999
Classification Train Epoch: 46 [44800/48000 (93%)]	Loss: 0.033957, KL fake Loss: 0.006703

Test set: Average loss: 1.7419, Accuracy: 6451/8000 (81%)

Classification Train Epoch: 47 [0/48000 (0%)]	Loss: 0.006149, KL fake Loss: 0.004216
Classification Train Epoch: 47 [6400/48000 (13%)]	Loss: 0.003148, KL fake Loss: 0.005970
Classification Train Epoch: 47 [12800/48000 (27%)]	Loss: 0.003035, KL fake Loss: 0.002843
Classification Train Epoch: 47 [19200/48000 (40%)]	Loss: 0.001928, KL fake Loss: 0.004156
Classification Train Epoch: 47 [25600/48000 (53%)]	Loss: 0.053137, KL fake Loss: 0.005551
Classification Train Epoch: 47 [32000/48000 (67%)]	Loss: 0.001430, KL fake Loss: 0.005779
Classification Train Epoch: 47 [38400/48000 (80%)]	Loss: 0.005251, KL fake Loss: 0.005838
Classification Train Epoch: 47 [44800/48000 (93%)]	Loss: 0.068548, KL fake Loss: 0.002835

Test set: Average loss: 1.6873, Accuracy: 5584/8000 (70%)

Classification Train Epoch: 48 [0/48000 (0%)]	Loss: 0.007492, KL fake Loss: 0.005504
Classification Train Epoch: 48 [6400/48000 (13%)]	Loss: 0.023265, KL fake Loss: 0.004810
Classification Train Epoch: 48 [12800/48000 (27%)]	Loss: 0.002841, KL fake Loss: 0.003294
Classification Train Epoch: 48 [19200/48000 (40%)]	Loss: 0.006135, KL fake Loss: 0.173420
Classification Train Epoch: 48 [25600/48000 (53%)]	Loss: 0.000609, KL fake Loss: 0.002396
Classification Train Epoch: 48 [32000/48000 (67%)]	Loss: 0.019840, KL fake Loss: 0.004247
Classification Train Epoch: 48 [38400/48000 (80%)]	Loss: 0.049932, KL fake Loss: 0.003937
Classification Train Epoch: 48 [44800/48000 (93%)]	Loss: 0.003487, KL fake Loss: 0.015091

Test set: Average loss: 1.7758, Accuracy: 6907/8000 (86%)

Classification Train Epoch: 49 [0/48000 (0%)]	Loss: 0.008380, KL fake Loss: 0.004730
Classification Train Epoch: 49 [6400/48000 (13%)]	Loss: 0.002388, KL fake Loss: 0.005213
Classification Train Epoch: 49 [12800/48000 (27%)]	Loss: 0.003552, KL fake Loss: 0.005330
Classification Train Epoch: 49 [19200/48000 (40%)]	Loss: 0.002874, KL fake Loss: 0.005749
Classification Train Epoch: 49 [25600/48000 (53%)]	Loss: 0.012332, KL fake Loss: 0.008362
Classification Train Epoch: 49 [32000/48000 (67%)]	Loss: 0.000781, KL fake Loss: 0.010641
Classification Train Epoch: 49 [38400/48000 (80%)]	Loss: 0.038796, KL fake Loss: 0.008574
Classification Train Epoch: 49 [44800/48000 (93%)]	Loss: 0.010309, KL fake Loss: 0.014799

Test set: Average loss: 1.5628, Accuracy: 6723/8000 (84%)

Classification Train Epoch: 50 [0/48000 (0%)]	Loss: 0.007483, KL fake Loss: 0.009215
Classification Train Epoch: 50 [6400/48000 (13%)]	Loss: 0.001283, KL fake Loss: 0.004072
Classification Train Epoch: 50 [12800/48000 (27%)]	Loss: 0.001887, KL fake Loss: 0.006013
Classification Train Epoch: 50 [19200/48000 (40%)]	Loss: 0.002428, KL fake Loss: 0.006849
Classification Train Epoch: 50 [25600/48000 (53%)]	Loss: 0.014524, KL fake Loss: 0.003275
Classification Train Epoch: 50 [32000/48000 (67%)]	Loss: 0.000812, KL fake Loss: 0.005115
Classification Train Epoch: 50 [38400/48000 (80%)]	Loss: 0.043332, KL fake Loss: 0.023138
Classification Train Epoch: 50 [44800/48000 (93%)]	Loss: 0.036107, KL fake Loss: 0.031901

Test set: Average loss: 1.6801, Accuracy: 6208/8000 (78%)

Classification Train Epoch: 51 [0/48000 (0%)]	Loss: 0.031817, KL fake Loss: 0.010247
Classification Train Epoch: 51 [6400/48000 (13%)]	Loss: 0.019573, KL fake Loss: 0.006105
Classification Train Epoch: 51 [12800/48000 (27%)]	Loss: 0.001736, KL fake Loss: 0.004171
Classification Train Epoch: 51 [19200/48000 (40%)]	Loss: 0.063327, KL fake Loss: 0.004546
Classification Train Epoch: 51 [25600/48000 (53%)]	Loss: 0.034983, KL fake Loss: 0.008353
Classification Train Epoch: 51 [32000/48000 (67%)]	Loss: 0.011467, KL fake Loss: 0.002702
Classification Train Epoch: 51 [38400/48000 (80%)]	Loss: 0.001933, KL fake Loss: 0.004943
Classification Train Epoch: 51 [44800/48000 (93%)]	Loss: 0.014621, KL fake Loss: 0.005276

Test set: Average loss: 1.7167, Accuracy: 5982/8000 (75%)

Classification Train Epoch: 52 [0/48000 (0%)]	Loss: 0.033280, KL fake Loss: 0.004691
Classification Train Epoch: 52 [6400/48000 (13%)]	Loss: 0.001511, KL fake Loss: 0.001590
Classification Train Epoch: 52 [12800/48000 (27%)]	Loss: 0.023157, KL fake Loss: 0.004092
Classification Train Epoch: 52 [19200/48000 (40%)]	Loss: 0.001154, KL fake Loss: 0.004223
Classification Train Epoch: 52 [25600/48000 (53%)]	Loss: 0.002134, KL fake Loss: 0.004612
Classification Train Epoch: 52 [32000/48000 (67%)]	Loss: 0.037113, KL fake Loss: 0.005446
Classification Train Epoch: 52 [38400/48000 (80%)]	Loss: 0.001629, KL fake Loss: 0.002879
Classification Train Epoch: 52 [44800/48000 (93%)]	Loss: 0.058735, KL fake Loss: 0.005956

Test set: Average loss: 1.8181, Accuracy: 4516/8000 (56%)

Classification Train Epoch: 53 [0/48000 (0%)]	Loss: 0.048480, KL fake Loss: 0.008808
Classification Train Epoch: 53 [6400/48000 (13%)]	Loss: 0.053056, KL fake Loss: 0.004916
Classification Train Epoch: 53 [12800/48000 (27%)]	Loss: 0.000486, KL fake Loss: 0.002656
Classification Train Epoch: 53 [19200/48000 (40%)]	Loss: 0.000430, KL fake Loss: 0.002295
Classification Train Epoch: 53 [25600/48000 (53%)]	Loss: 0.101337, KL fake Loss: 0.003776
Classification Train Epoch: 53 [32000/48000 (67%)]	Loss: 0.016294, KL fake Loss: 0.004903
Classification Train Epoch: 53 [38400/48000 (80%)]	Loss: 0.000471, KL fake Loss: 0.004219
 53%|█████▎    | 53/100 [2:24:27<2:08:03, 163.48s/it] 54%|█████▍    | 54/100 [2:27:10<2:05:20, 163.48s/it] 55%|█████▌    | 55/100 [2:29:54<2:02:37, 163.49s/it] 56%|█████▌    | 56/100 [2:32:37<1:59:53, 163.48s/it] 57%|█████▋    | 57/100 [2:35:21<1:57:09, 163.48s/it] 58%|█████▊    | 58/100 [2:38:04<1:54:26, 163.48s/it] 59%|█████▉    | 59/100 [2:40:48<1:51:42, 163.49s/it] 60%|██████    | 60/100 [2:43:31<1:49:00, 163.51s/it] 61%|██████    | 61/100 [2:46:15<1:46:16, 163.50s/it] 62%|██████▏   | 62/100 [2:48:58<1:43:32, 163.50s/it] 63%|██████▎   | 63/100 [2:51:42<1:40:49, 163.50s/it]Classification Train Epoch: 53 [44800/48000 (93%)]	Loss: 0.007687, KL fake Loss: 0.007414

Test set: Average loss: 1.6934, Accuracy: 6552/8000 (82%)

Classification Train Epoch: 54 [0/48000 (0%)]	Loss: 0.020257, KL fake Loss: 0.008777
Classification Train Epoch: 54 [6400/48000 (13%)]	Loss: 0.046540, KL fake Loss: 0.004712
Classification Train Epoch: 54 [12800/48000 (27%)]	Loss: 0.008286, KL fake Loss: 0.018935
Classification Train Epoch: 54 [19200/48000 (40%)]	Loss: 0.025264, KL fake Loss: 0.019601
Classification Train Epoch: 54 [25600/48000 (53%)]	Loss: 0.001856, KL fake Loss: 0.019389
Classification Train Epoch: 54 [32000/48000 (67%)]	Loss: 0.109654, KL fake Loss: 0.072212
Classification Train Epoch: 54 [38400/48000 (80%)]	Loss: 0.137393, KL fake Loss: 0.025424
Classification Train Epoch: 54 [44800/48000 (93%)]	Loss: 0.021175, KL fake Loss: 0.009563

Test set: Average loss: 1.7824, Accuracy: 6894/8000 (86%)

Classification Train Epoch: 55 [0/48000 (0%)]	Loss: 0.001448, KL fake Loss: 0.004193
Classification Train Epoch: 55 [6400/48000 (13%)]	Loss: 0.009929, KL fake Loss: 0.004894
Classification Train Epoch: 55 [12800/48000 (27%)]	Loss: 0.025684, KL fake Loss: 0.004117
Classification Train Epoch: 55 [19200/48000 (40%)]	Loss: 0.008536, KL fake Loss: 0.004834
Classification Train Epoch: 55 [25600/48000 (53%)]	Loss: 0.043762, KL fake Loss: 0.006532
Classification Train Epoch: 55 [32000/48000 (67%)]	Loss: 0.002447, KL fake Loss: 0.004557
Classification Train Epoch: 55 [38400/48000 (80%)]	Loss: 0.024885, KL fake Loss: 0.003076
Classification Train Epoch: 55 [44800/48000 (93%)]	Loss: 0.033799, KL fake Loss: 0.722454

Test set: Average loss: 1.7596, Accuracy: 6341/8000 (79%)

Classification Train Epoch: 56 [0/48000 (0%)]	Loss: 0.000807, KL fake Loss: 0.006417
Classification Train Epoch: 56 [6400/48000 (13%)]	Loss: 0.014904, KL fake Loss: 0.197890
Classification Train Epoch: 56 [12800/48000 (27%)]	Loss: 0.020639, KL fake Loss: 0.002196
Classification Train Epoch: 56 [19200/48000 (40%)]	Loss: 0.005391, KL fake Loss: 0.003106
Classification Train Epoch: 56 [25600/48000 (53%)]	Loss: 0.001069, KL fake Loss: 0.004698
Classification Train Epoch: 56 [32000/48000 (67%)]	Loss: 0.003609, KL fake Loss: 0.001550
Classification Train Epoch: 56 [38400/48000 (80%)]	Loss: 0.004952, KL fake Loss: 0.007624
Classification Train Epoch: 56 [44800/48000 (93%)]	Loss: 0.007623, KL fake Loss: 0.010979

Test set: Average loss: 1.7776, Accuracy: 5752/8000 (72%)

Classification Train Epoch: 57 [0/48000 (0%)]	Loss: 0.003778, KL fake Loss: 0.007439
Classification Train Epoch: 57 [6400/48000 (13%)]	Loss: 0.026969, KL fake Loss: 0.005096
Classification Train Epoch: 57 [12800/48000 (27%)]	Loss: 0.007262, KL fake Loss: 0.916378
Classification Train Epoch: 57 [19200/48000 (40%)]	Loss: 0.061715, KL fake Loss: 0.010786
Classification Train Epoch: 57 [25600/48000 (53%)]	Loss: 0.009742, KL fake Loss: 0.004134
Classification Train Epoch: 57 [32000/48000 (67%)]	Loss: 0.001195, KL fake Loss: 0.002239
Classification Train Epoch: 57 [38400/48000 (80%)]	Loss: 0.026689, KL fake Loss: 0.030683
Classification Train Epoch: 57 [44800/48000 (93%)]	Loss: 0.080488, KL fake Loss: 0.007959

Test set: Average loss: 1.7101, Accuracy: 5878/8000 (73%)

Classification Train Epoch: 58 [0/48000 (0%)]	Loss: 0.020473, KL fake Loss: 0.006710
Classification Train Epoch: 58 [6400/48000 (13%)]	Loss: 0.016196, KL fake Loss: 0.009439
Classification Train Epoch: 58 [12800/48000 (27%)]	Loss: 0.000129, KL fake Loss: 0.003492
Classification Train Epoch: 58 [19200/48000 (40%)]	Loss: 0.000373, KL fake Loss: 0.005038
Classification Train Epoch: 58 [25600/48000 (53%)]	Loss: 0.000975, KL fake Loss: 0.004360
Classification Train Epoch: 58 [32000/48000 (67%)]	Loss: 0.024133, KL fake Loss: 0.063228
Classification Train Epoch: 58 [38400/48000 (80%)]	Loss: 0.003079, KL fake Loss: 0.007008
Classification Train Epoch: 58 [44800/48000 (93%)]	Loss: 0.002265, KL fake Loss: 0.009357

Test set: Average loss: 1.6235, Accuracy: 6921/8000 (87%)

Classification Train Epoch: 59 [0/48000 (0%)]	Loss: 0.008399, KL fake Loss: 0.010954
Classification Train Epoch: 59 [6400/48000 (13%)]	Loss: 0.001607, KL fake Loss: 0.002580
Classification Train Epoch: 59 [12800/48000 (27%)]	Loss: 0.002229, KL fake Loss: 0.024533
Classification Train Epoch: 59 [19200/48000 (40%)]	Loss: 0.006348, KL fake Loss: 0.008486
Classification Train Epoch: 59 [25600/48000 (53%)]	Loss: 0.002202, KL fake Loss: 0.008464
Classification Train Epoch: 59 [32000/48000 (67%)]	Loss: 0.000912, KL fake Loss: 0.006656
Classification Train Epoch: 59 [38400/48000 (80%)]	Loss: 0.014657, KL fake Loss: 0.004592
Classification Train Epoch: 59 [44800/48000 (93%)]	Loss: 0.003775, KL fake Loss: 0.005700

Test set: Average loss: 1.7101, Accuracy: 5820/8000 (73%)

Classification Train Epoch: 60 [0/48000 (0%)]	Loss: 0.026905, KL fake Loss: 0.009135
Classification Train Epoch: 60 [6400/48000 (13%)]	Loss: 0.001805, KL fake Loss: 0.109121
Classification Train Epoch: 60 [12800/48000 (27%)]	Loss: 0.000808, KL fake Loss: 0.002445
Classification Train Epoch: 60 [19200/48000 (40%)]	Loss: 0.022673, KL fake Loss: 0.002779
Classification Train Epoch: 60 [25600/48000 (53%)]	Loss: 0.029885, KL fake Loss: 0.006370
Classification Train Epoch: 60 [32000/48000 (67%)]	Loss: 0.001185, KL fake Loss: 0.003602
Classification Train Epoch: 60 [38400/48000 (80%)]	Loss: 0.005673, KL fake Loss: 0.017311
Classification Train Epoch: 60 [44800/48000 (93%)]	Loss: 0.006634, KL fake Loss: 0.004067

Test set: Average loss: 1.7406, Accuracy: 5805/8000 (73%)

Classification Train Epoch: 61 [0/48000 (0%)]	Loss: 0.006395, KL fake Loss: 0.005620
Classification Train Epoch: 61 [6400/48000 (13%)]	Loss: 0.021997, KL fake Loss: 0.002391
Classification Train Epoch: 61 [12800/48000 (27%)]	Loss: 0.000209, KL fake Loss: 0.002134
Classification Train Epoch: 61 [19200/48000 (40%)]	Loss: 0.000683, KL fake Loss: 0.001623
Classification Train Epoch: 61 [25600/48000 (53%)]	Loss: 0.006372, KL fake Loss: 0.001925
Classification Train Epoch: 61 [32000/48000 (67%)]	Loss: 0.001032, KL fake Loss: 0.001919
Classification Train Epoch: 61 [38400/48000 (80%)]	Loss: 0.001364, KL fake Loss: 0.001124
Classification Train Epoch: 61 [44800/48000 (93%)]	Loss: 0.000705, KL fake Loss: 0.001848

Test set: Average loss: 1.6845, Accuracy: 6737/8000 (84%)

Classification Train Epoch: 62 [0/48000 (0%)]	Loss: 0.030605, KL fake Loss: 0.007012
Classification Train Epoch: 62 [6400/48000 (13%)]	Loss: 0.000872, KL fake Loss: 0.001872
Classification Train Epoch: 62 [12800/48000 (27%)]	Loss: 0.000797, KL fake Loss: 0.001261
Classification Train Epoch: 62 [19200/48000 (40%)]	Loss: 0.000450, KL fake Loss: 0.001741
Classification Train Epoch: 62 [25600/48000 (53%)]	Loss: 0.000176, KL fake Loss: 0.001298
Classification Train Epoch: 62 [32000/48000 (67%)]	Loss: 0.001417, KL fake Loss: 0.005590
Classification Train Epoch: 62 [38400/48000 (80%)]	Loss: 0.001263, KL fake Loss: 0.000908
Classification Train Epoch: 62 [44800/48000 (93%)]	Loss: 0.000220, KL fake Loss: 0.001105

Test set: Average loss: 1.8080, Accuracy: 5600/8000 (70%)

Classification Train Epoch: 63 [0/48000 (0%)]	Loss: 0.000505, KL fake Loss: 0.001425
Classification Train Epoch: 63 [6400/48000 (13%)]	Loss: 0.000809, KL fake Loss: 0.001965
Classification Train Epoch: 63 [12800/48000 (27%)]	Loss: 0.001117, KL fake Loss: 0.001045
Classification Train Epoch: 63 [19200/48000 (40%)]	Loss: 0.001133, KL fake Loss: 0.001720
Classification Train Epoch: 63 [25600/48000 (53%)]	Loss: 0.000305, KL fake Loss: 0.005652
Classification Train Epoch: 63 [32000/48000 (67%)]	Loss: 0.000328, KL fake Loss: 0.004440
Classification Train Epoch: 63 [38400/48000 (80%)]	Loss: 0.000570, KL fake Loss: 0.001659
Classification Train Epoch: 63 [44800/48000 (93%)]	Loss: 0.000260, KL fake Loss: 0.003447

Test set: Average loss: 1.7247, Accuracy: 6398/8000 (80%)

Classification Train Epoch: 64 [0/48000 (0%)]	Loss: 0.000587, KL fake Loss: 0.004527
Classification Train Epoch: 64 [6400/48000 (13%)]	Loss: 0.000208, KL fake Loss: 0.002616
Classification Train Epoch: 64 [12800/48000 (27%)]	Loss: 0.000535, KL fake Loss: 0.002889
 64%|██████▍   | 64/100 [2:54:25<1:38:05, 163.49s/it] 65%|██████▌   | 65/100 [2:57:09<1:35:21, 163.48s/it] 66%|██████▌   | 66/100 [2:59:52<1:32:38, 163.48s/it] 67%|██████▋   | 67/100 [3:02:36<1:29:54, 163.48s/it] 68%|██████▊   | 68/100 [3:05:19<1:27:11, 163.48s/it] 69%|██████▉   | 69/100 [3:08:03<1:24:27, 163.48s/it] 70%|███████   | 70/100 [3:10:46<1:21:44, 163.48s/it] 71%|███████   | 71/100 [3:13:30<1:19:00, 163.48s/it] 72%|███████▏  | 72/100 [3:16:13<1:16:17, 163.48s/it] 73%|███████▎  | 73/100 [3:18:57<1:13:33, 163.47s/it]Classification Train Epoch: 64 [19200/48000 (40%)]	Loss: 0.000255, KL fake Loss: 0.001112
Classification Train Epoch: 64 [25600/48000 (53%)]	Loss: 0.000311, KL fake Loss: 0.001776
Classification Train Epoch: 64 [32000/48000 (67%)]	Loss: 0.000405, KL fake Loss: 0.004722
Classification Train Epoch: 64 [38400/48000 (80%)]	Loss: 0.000354, KL fake Loss: 0.001839
Classification Train Epoch: 64 [44800/48000 (93%)]	Loss: 0.000099, KL fake Loss: 0.001998

Test set: Average loss: 1.8291, Accuracy: 5135/8000 (64%)

Classification Train Epoch: 65 [0/48000 (0%)]	Loss: 0.000228, KL fake Loss: 0.001434
Classification Train Epoch: 65 [6400/48000 (13%)]	Loss: 0.000072, KL fake Loss: 0.000974
Classification Train Epoch: 65 [12800/48000 (27%)]	Loss: 0.000118, KL fake Loss: 0.001284
Classification Train Epoch: 65 [19200/48000 (40%)]	Loss: 0.000213, KL fake Loss: 0.001323
Classification Train Epoch: 65 [25600/48000 (53%)]	Loss: 0.000333, KL fake Loss: 0.001907
Classification Train Epoch: 65 [32000/48000 (67%)]	Loss: 0.000449, KL fake Loss: 0.001099
Classification Train Epoch: 65 [38400/48000 (80%)]	Loss: 0.000193, KL fake Loss: 0.007934
Classification Train Epoch: 65 [44800/48000 (93%)]	Loss: 0.000093, KL fake Loss: 0.001298

Test set: Average loss: 1.8611, Accuracy: 4902/8000 (61%)

Classification Train Epoch: 66 [0/48000 (0%)]	Loss: 0.000314, KL fake Loss: 0.002846
Classification Train Epoch: 66 [6400/48000 (13%)]	Loss: 0.000311, KL fake Loss: 0.001064
Classification Train Epoch: 66 [12800/48000 (27%)]	Loss: 0.000159, KL fake Loss: 0.001395
Classification Train Epoch: 66 [19200/48000 (40%)]	Loss: 0.000275, KL fake Loss: 0.001470
Classification Train Epoch: 66 [25600/48000 (53%)]	Loss: 0.000497, KL fake Loss: 0.001170
Classification Train Epoch: 66 [32000/48000 (67%)]	Loss: 0.000199, KL fake Loss: 0.003281
Classification Train Epoch: 66 [38400/48000 (80%)]	Loss: 0.000104, KL fake Loss: 0.001501
Classification Train Epoch: 66 [44800/48000 (93%)]	Loss: 0.000188, KL fake Loss: 0.000909

Test set: Average loss: 1.7863, Accuracy: 5606/8000 (70%)

Classification Train Epoch: 67 [0/48000 (0%)]	Loss: 0.000107, KL fake Loss: 0.001297
Classification Train Epoch: 67 [6400/48000 (13%)]	Loss: 0.000082, KL fake Loss: 0.001585
Classification Train Epoch: 67 [12800/48000 (27%)]	Loss: 0.000358, KL fake Loss: 0.001587
Classification Train Epoch: 67 [19200/48000 (40%)]	Loss: 0.001263, KL fake Loss: 0.001685
Classification Train Epoch: 67 [25600/48000 (53%)]	Loss: 0.000439, KL fake Loss: 0.001285
Classification Train Epoch: 67 [32000/48000 (67%)]	Loss: 0.000138, KL fake Loss: 0.001574
Classification Train Epoch: 67 [38400/48000 (80%)]	Loss: 0.000137, KL fake Loss: 0.003194
Classification Train Epoch: 67 [44800/48000 (93%)]	Loss: 0.000146, KL fake Loss: 0.001263

Test set: Average loss: 1.8979, Accuracy: 4049/8000 (51%)

Classification Train Epoch: 68 [0/48000 (0%)]	Loss: 0.000841, KL fake Loss: 0.001571
Classification Train Epoch: 68 [6400/48000 (13%)]	Loss: 0.000189, KL fake Loss: 0.001398
Classification Train Epoch: 68 [12800/48000 (27%)]	Loss: 0.000245, KL fake Loss: 0.005348
Classification Train Epoch: 68 [19200/48000 (40%)]	Loss: 0.000241, KL fake Loss: 0.001002
Classification Train Epoch: 68 [25600/48000 (53%)]	Loss: 0.000809, KL fake Loss: 0.001485
Classification Train Epoch: 68 [32000/48000 (67%)]	Loss: 0.000073, KL fake Loss: 0.001452
Classification Train Epoch: 68 [38400/48000 (80%)]	Loss: 0.000879, KL fake Loss: 0.001479
Classification Train Epoch: 68 [44800/48000 (93%)]	Loss: 0.000070, KL fake Loss: 0.001470

Test set: Average loss: 1.7844, Accuracy: 5745/8000 (72%)

Classification Train Epoch: 69 [0/48000 (0%)]	Loss: 0.000130, KL fake Loss: 0.001397
Classification Train Epoch: 69 [6400/48000 (13%)]	Loss: 0.000131, KL fake Loss: 0.002699
Classification Train Epoch: 69 [12800/48000 (27%)]	Loss: 0.000875, KL fake Loss: 0.007477
Classification Train Epoch: 69 [19200/48000 (40%)]	Loss: 0.000032, KL fake Loss: 0.001224
Classification Train Epoch: 69 [25600/48000 (53%)]	Loss: 0.000028, KL fake Loss: 0.001294
Classification Train Epoch: 69 [32000/48000 (67%)]	Loss: 0.000218, KL fake Loss: 0.001094
Classification Train Epoch: 69 [38400/48000 (80%)]	Loss: 0.000122, KL fake Loss: 0.002004
Classification Train Epoch: 69 [44800/48000 (93%)]	Loss: 0.000120, KL fake Loss: 0.001813

Test set: Average loss: 1.8036, Accuracy: 5444/8000 (68%)

Classification Train Epoch: 70 [0/48000 (0%)]	Loss: 0.001371, KL fake Loss: 0.001000
Classification Train Epoch: 70 [6400/48000 (13%)]	Loss: 0.001626, KL fake Loss: 0.001015
Classification Train Epoch: 70 [12800/48000 (27%)]	Loss: 0.001823, KL fake Loss: 0.003672
Classification Train Epoch: 70 [19200/48000 (40%)]	Loss: 0.000007, KL fake Loss: 0.001774
Classification Train Epoch: 70 [25600/48000 (53%)]	Loss: 0.000216, KL fake Loss: 0.001147
Classification Train Epoch: 70 [32000/48000 (67%)]	Loss: 0.000091, KL fake Loss: 0.002798
Classification Train Epoch: 70 [38400/48000 (80%)]	Loss: 0.000016, KL fake Loss: 0.000818
Classification Train Epoch: 70 [44800/48000 (93%)]	Loss: 0.000038, KL fake Loss: 0.001754

Test set: Average loss: 1.7568, Accuracy: 6026/8000 (75%)

Classification Train Epoch: 71 [0/48000 (0%)]	Loss: 0.000206, KL fake Loss: 0.001391
Classification Train Epoch: 71 [6400/48000 (13%)]	Loss: 0.000298, KL fake Loss: 0.001427
Classification Train Epoch: 71 [12800/48000 (27%)]	Loss: 0.000199, KL fake Loss: 0.001089
Classification Train Epoch: 71 [19200/48000 (40%)]	Loss: 0.000530, KL fake Loss: 0.000796
Classification Train Epoch: 71 [25600/48000 (53%)]	Loss: 0.001216, KL fake Loss: 0.002954
Classification Train Epoch: 71 [32000/48000 (67%)]	Loss: 0.000032, KL fake Loss: 0.001187
Classification Train Epoch: 71 [38400/48000 (80%)]	Loss: 0.000167, KL fake Loss: 0.003425
Classification Train Epoch: 71 [44800/48000 (93%)]	Loss: 0.001007, KL fake Loss: 0.013647

Test set: Average loss: 1.7925, Accuracy: 5674/8000 (71%)

Classification Train Epoch: 72 [0/48000 (0%)]	Loss: 0.001753, KL fake Loss: 0.000869
Classification Train Epoch: 72 [6400/48000 (13%)]	Loss: 0.002327, KL fake Loss: 0.001930
Classification Train Epoch: 72 [12800/48000 (27%)]	Loss: 0.000198, KL fake Loss: 0.000996
Classification Train Epoch: 72 [19200/48000 (40%)]	Loss: 0.000049, KL fake Loss: 0.008011
Classification Train Epoch: 72 [25600/48000 (53%)]	Loss: 0.000079, KL fake Loss: 0.001325
Classification Train Epoch: 72 [32000/48000 (67%)]	Loss: 0.000142, KL fake Loss: 0.004439
Classification Train Epoch: 72 [38400/48000 (80%)]	Loss: 0.000345, KL fake Loss: 0.000802
Classification Train Epoch: 72 [44800/48000 (93%)]	Loss: 0.000042, KL fake Loss: 0.005825

Test set: Average loss: 1.7560, Accuracy: 5861/8000 (73%)

Classification Train Epoch: 73 [0/48000 (0%)]	Loss: 0.000013, KL fake Loss: 0.001338
Classification Train Epoch: 73 [6400/48000 (13%)]	Loss: 0.000944, KL fake Loss: 0.001401
Classification Train Epoch: 73 [12800/48000 (27%)]	Loss: 0.000119, KL fake Loss: 0.003146
Classification Train Epoch: 73 [19200/48000 (40%)]	Loss: 0.000145, KL fake Loss: 0.000798
Classification Train Epoch: 73 [25600/48000 (53%)]	Loss: 0.000316, KL fake Loss: 0.001183
Classification Train Epoch: 73 [32000/48000 (67%)]	Loss: 0.000029, KL fake Loss: 0.001262
Classification Train Epoch: 73 [38400/48000 (80%)]	Loss: 0.000687, KL fake Loss: 0.000778
Classification Train Epoch: 73 [44800/48000 (93%)]	Loss: 0.000134, KL fake Loss: 0.002896

Test set: Average loss: 1.8525, Accuracy: 4761/8000 (60%)

Classification Train Epoch: 74 [0/48000 (0%)]	Loss: 0.000052, KL fake Loss: 0.000946
Classification Train Epoch: 74 [6400/48000 (13%)]	Loss: 0.000039, KL fake Loss: 0.001015
Classification Train Epoch: 74 [12800/48000 (27%)]	Loss: 0.000179, KL fake Loss: 0.008015
Classification Train Epoch: 74 [19200/48000 (40%)]	Loss: 0.000117, KL fake Loss: 0.000935
Classification Train Epoch: 74 [25600/48000 (53%)]	Loss: 0.000159, KL fake Loss: 0.000957
Classification Train Epoch: 74 [32000/48000 (67%)]	Loss: 0.000045, KL fake Loss: 0.001133
Classification Train Epoch: 74 [38400/48000 (80%)]	Loss: 0.000049, KL fake Loss: 0.000713
Classification Train Epoch: 74 [44800/48000 (93%)]	Loss: 0.000053, KL fake Loss: 0.000704
 74%|███████▍  | 74/100 [3:21:40<1:10:50, 163.47s/it] 75%|███████▌  | 75/100 [3:24:24<1:08:06, 163.47s/it] 76%|███████▌  | 76/100 [3:27:07<1:05:23, 163.47s/it] 77%|███████▋  | 77/100 [3:29:50<1:02:39, 163.47s/it] 78%|███████▊  | 78/100 [3:32:34<59:56, 163.47s/it]   79%|███████▉  | 79/100 [3:35:17<57:13, 163.48s/it] 80%|████████  | 80/100 [3:38:01<54:30, 163.51s/it] 81%|████████  | 81/100 [3:40:44<51:46, 163.50s/it] 82%|████████▏ | 82/100 [3:43:28<49:02, 163.49s/it] 83%|████████▎ | 83/100 [3:46:11<46:19, 163.49s/it] 84%|████████▍ | 84/100 [3:48:55<43:35, 163.49s/it]
Test set: Average loss: 1.9119, Accuracy: 3574/8000 (45%)

Classification Train Epoch: 75 [0/48000 (0%)]	Loss: 0.000201, KL fake Loss: 0.001475
Classification Train Epoch: 75 [6400/48000 (13%)]	Loss: 0.000065, KL fake Loss: 0.000626
Classification Train Epoch: 75 [12800/48000 (27%)]	Loss: 0.000193, KL fake Loss: 0.000654
Classification Train Epoch: 75 [19200/48000 (40%)]	Loss: 0.000205, KL fake Loss: 0.002348
Classification Train Epoch: 75 [25600/48000 (53%)]	Loss: 0.000046, KL fake Loss: 0.000957
Classification Train Epoch: 75 [32000/48000 (67%)]	Loss: 0.000067, KL fake Loss: 0.000591
Classification Train Epoch: 75 [38400/48000 (80%)]	Loss: 0.000031, KL fake Loss: 0.001379
Classification Train Epoch: 75 [44800/48000 (93%)]	Loss: 0.000038, KL fake Loss: 0.000943

Test set: Average loss: 1.8437, Accuracy: 4759/8000 (59%)

Classification Train Epoch: 76 [0/48000 (0%)]	Loss: 0.000042, KL fake Loss: 0.001647
Classification Train Epoch: 76 [6400/48000 (13%)]	Loss: 0.000311, KL fake Loss: 0.000619
Classification Train Epoch: 76 [12800/48000 (27%)]	Loss: 0.000062, KL fake Loss: 0.000739
Classification Train Epoch: 76 [19200/48000 (40%)]	Loss: 0.000033, KL fake Loss: 0.001024
Classification Train Epoch: 76 [25600/48000 (53%)]	Loss: 0.000219, KL fake Loss: 0.000933
Classification Train Epoch: 76 [32000/48000 (67%)]	Loss: 0.000189, KL fake Loss: 0.000609
Classification Train Epoch: 76 [38400/48000 (80%)]	Loss: 0.004948, KL fake Loss: 0.001149
Classification Train Epoch: 76 [44800/48000 (93%)]	Loss: 0.000019, KL fake Loss: 0.001120

Test set: Average loss: 1.9192, Accuracy: 3582/8000 (45%)

Classification Train Epoch: 77 [0/48000 (0%)]	Loss: 0.000020, KL fake Loss: 0.001033
Classification Train Epoch: 77 [6400/48000 (13%)]	Loss: 0.000050, KL fake Loss: 0.000884
Classification Train Epoch: 77 [12800/48000 (27%)]	Loss: 0.000197, KL fake Loss: 0.000778
Classification Train Epoch: 77 [19200/48000 (40%)]	Loss: 0.000010, KL fake Loss: 0.000849
Classification Train Epoch: 77 [25600/48000 (53%)]	Loss: 0.000033, KL fake Loss: 0.001512
Classification Train Epoch: 77 [32000/48000 (67%)]	Loss: 0.000081, KL fake Loss: 0.000626
Classification Train Epoch: 77 [38400/48000 (80%)]	Loss: 0.000082, KL fake Loss: 0.001206
Classification Train Epoch: 77 [44800/48000 (93%)]	Loss: 0.000043, KL fake Loss: 0.000455

Test set: Average loss: 1.8394, Accuracy: 4691/8000 (59%)

Classification Train Epoch: 78 [0/48000 (0%)]	Loss: 0.000022, KL fake Loss: 0.001067
Classification Train Epoch: 78 [6400/48000 (13%)]	Loss: 0.000082, KL fake Loss: 0.001133
Classification Train Epoch: 78 [12800/48000 (27%)]	Loss: 0.000110, KL fake Loss: 0.000613
Classification Train Epoch: 78 [19200/48000 (40%)]	Loss: 0.000081, KL fake Loss: 0.002224
Classification Train Epoch: 78 [25600/48000 (53%)]	Loss: 0.000015, KL fake Loss: 0.000844
Classification Train Epoch: 78 [32000/48000 (67%)]	Loss: 0.000318, KL fake Loss: 0.001178
Classification Train Epoch: 78 [38400/48000 (80%)]	Loss: 0.000054, KL fake Loss: 0.001472
Classification Train Epoch: 78 [44800/48000 (93%)]	Loss: 0.000098, KL fake Loss: 0.000742

Test set: Average loss: 1.9030, Accuracy: 3535/8000 (44%)

Classification Train Epoch: 79 [0/48000 (0%)]	Loss: 0.000020, KL fake Loss: 0.002465
Classification Train Epoch: 79 [6400/48000 (13%)]	Loss: 0.000018, KL fake Loss: 0.002254
Classification Train Epoch: 79 [12800/48000 (27%)]	Loss: 0.002169, KL fake Loss: 0.001032
Classification Train Epoch: 79 [19200/48000 (40%)]	Loss: 0.000083, KL fake Loss: 0.000672
Classification Train Epoch: 79 [25600/48000 (53%)]	Loss: 0.000211, KL fake Loss: 0.000946
Classification Train Epoch: 79 [32000/48000 (67%)]	Loss: 0.000855, KL fake Loss: 0.000935
Classification Train Epoch: 79 [38400/48000 (80%)]	Loss: 0.000024, KL fake Loss: 0.000491
Classification Train Epoch: 79 [44800/48000 (93%)]	Loss: 0.000243, KL fake Loss: 0.001208

Test set: Average loss: 1.8501, Accuracy: 4684/8000 (59%)

Classification Train Epoch: 80 [0/48000 (0%)]	Loss: 0.000017, KL fake Loss: 0.000973
Classification Train Epoch: 80 [6400/48000 (13%)]	Loss: 0.000052, KL fake Loss: 0.000641
Classification Train Epoch: 80 [12800/48000 (27%)]	Loss: 0.000013, KL fake Loss: 0.000831
Classification Train Epoch: 80 [19200/48000 (40%)]	Loss: 0.000362, KL fake Loss: 0.000796
Classification Train Epoch: 80 [25600/48000 (53%)]	Loss: 0.000032, KL fake Loss: 0.000820
Classification Train Epoch: 80 [32000/48000 (67%)]	Loss: 0.000042, KL fake Loss: 0.000713
Classification Train Epoch: 80 [38400/48000 (80%)]	Loss: 0.000092, KL fake Loss: 0.000963
Classification Train Epoch: 80 [44800/48000 (93%)]	Loss: 0.000093, KL fake Loss: 0.001936

Test set: Average loss: 1.9097, Accuracy: 2985/8000 (37%)

Classification Train Epoch: 81 [0/48000 (0%)]	Loss: 0.000073, KL fake Loss: 0.000942
Classification Train Epoch: 81 [6400/48000 (13%)]	Loss: 0.000025, KL fake Loss: 0.001206
Classification Train Epoch: 81 [12800/48000 (27%)]	Loss: 0.000078, KL fake Loss: 0.001312
Classification Train Epoch: 81 [19200/48000 (40%)]	Loss: 0.000035, KL fake Loss: 0.001055
Classification Train Epoch: 81 [25600/48000 (53%)]	Loss: 0.000041, KL fake Loss: 0.000633
Classification Train Epoch: 81 [32000/48000 (67%)]	Loss: 0.000050, KL fake Loss: 0.000733
Classification Train Epoch: 81 [38400/48000 (80%)]	Loss: 0.000024, KL fake Loss: 0.000729
Classification Train Epoch: 81 [44800/48000 (93%)]	Loss: 0.000026, KL fake Loss: 0.003504

Test set: Average loss: 1.8219, Accuracy: 5541/8000 (69%)

Classification Train Epoch: 82 [0/48000 (0%)]	Loss: 0.000029, KL fake Loss: 0.001149
Classification Train Epoch: 82 [6400/48000 (13%)]	Loss: 0.000072, KL fake Loss: 0.001867
Classification Train Epoch: 82 [12800/48000 (27%)]	Loss: 0.000092, KL fake Loss: 0.001202
Classification Train Epoch: 82 [19200/48000 (40%)]	Loss: 0.000007, KL fake Loss: 0.001262
Classification Train Epoch: 82 [25600/48000 (53%)]	Loss: 0.000439, KL fake Loss: 0.002904
Classification Train Epoch: 82 [32000/48000 (67%)]	Loss: 0.000015, KL fake Loss: 0.001061
Classification Train Epoch: 82 [38400/48000 (80%)]	Loss: 0.000015, KL fake Loss: 0.098454
Classification Train Epoch: 82 [44800/48000 (93%)]	Loss: 0.000100, KL fake Loss: 0.000620

Test set: Average loss: 1.8584, Accuracy: 4673/8000 (58%)

Classification Train Epoch: 83 [0/48000 (0%)]	Loss: 0.000208, KL fake Loss: 0.000898
Classification Train Epoch: 83 [6400/48000 (13%)]	Loss: 0.000028, KL fake Loss: 0.000729
Classification Train Epoch: 83 [12800/48000 (27%)]	Loss: 0.000011, KL fake Loss: 0.000610
Classification Train Epoch: 83 [19200/48000 (40%)]	Loss: 0.000023, KL fake Loss: 0.001097
Classification Train Epoch: 83 [25600/48000 (53%)]	Loss: 0.000033, KL fake Loss: 0.001031
Classification Train Epoch: 83 [32000/48000 (67%)]	Loss: 0.000019, KL fake Loss: 0.000446
Classification Train Epoch: 83 [38400/48000 (80%)]	Loss: 0.000261, KL fake Loss: 0.000942
Classification Train Epoch: 83 [44800/48000 (93%)]	Loss: 0.000080, KL fake Loss: 0.000842

Test set: Average loss: 1.9381, Accuracy: 2929/8000 (37%)

Classification Train Epoch: 84 [0/48000 (0%)]	Loss: 0.000040, KL fake Loss: 0.000739
Classification Train Epoch: 84 [6400/48000 (13%)]	Loss: 0.000031, KL fake Loss: 0.000965
Classification Train Epoch: 84 [12800/48000 (27%)]	Loss: 0.000015, KL fake Loss: 0.001160
Classification Train Epoch: 84 [19200/48000 (40%)]	Loss: 0.000467, KL fake Loss: 0.001104
Classification Train Epoch: 84 [25600/48000 (53%)]	Loss: 0.000041, KL fake Loss: 0.000902
Classification Train Epoch: 84 [32000/48000 (67%)]	Loss: 0.000098, KL fake Loss: 0.000932
Classification Train Epoch: 84 [38400/48000 (80%)]	Loss: 0.000022, KL fake Loss: 0.001970
Classification Train Epoch: 84 [44800/48000 (93%)]	Loss: 0.000041, KL fake Loss: 0.000652

Test set: Average loss: 1.9204, Accuracy: 3115/8000 (39%)

Classification Train Epoch: 85 [0/48000 (0%)]	Loss: 0.000020, KL fake Loss: 0.001015
Classification Train Epoch: 85 [6400/48000 (13%)]	Loss: 0.000025, KL fake Loss: 0.000501
Classification Train Epoch: 85 [12800/48000 (27%)]	Loss: 0.000009, KL fake Loss: 0.000726
Classification Train Epoch: 85 [19200/48000 (40%)]	Loss: 0.000029, KL fake Loss: 0.000574
 85%|████████▌ | 85/100 [3:51:38<40:52, 163.48s/it] 86%|████████▌ | 86/100 [3:54:22<38:08, 163.47s/it] 87%|████████▋ | 87/100 [3:57:05<35:25, 163.48s/it] 88%|████████▊ | 88/100 [3:59:49<32:41, 163.48s/it] 89%|████████▉ | 89/100 [4:02:32<29:58, 163.48s/it] 90%|█████████ | 90/100 [4:05:16<27:14, 163.48s/it] 91%|█████████ | 91/100 [4:07:59<24:31, 163.47s/it] 92%|█████████▏| 92/100 [4:10:43<21:47, 163.48s/it] 93%|█████████▎| 93/100 [4:13:26<19:04, 163.47s/it] 94%|█████████▍| 94/100 [4:16:10<16:20, 163.47s/it] 95%|█████████▌| 95/100 [4:18:53<13:37, 163.47s/it]Classification Train Epoch: 85 [25600/48000 (53%)]	Loss: 0.000149, KL fake Loss: 0.000835
Classification Train Epoch: 85 [32000/48000 (67%)]	Loss: 0.000050, KL fake Loss: 0.001077
Classification Train Epoch: 85 [38400/48000 (80%)]	Loss: 0.000015, KL fake Loss: 0.000491
Classification Train Epoch: 85 [44800/48000 (93%)]	Loss: 0.000162, KL fake Loss: 0.001071

Test set: Average loss: 1.9571, Accuracy: 2435/8000 (30%)

Classification Train Epoch: 86 [0/48000 (0%)]	Loss: 0.000100, KL fake Loss: 0.000502
Classification Train Epoch: 86 [6400/48000 (13%)]	Loss: 0.000032, KL fake Loss: 0.001103
Classification Train Epoch: 86 [12800/48000 (27%)]	Loss: 0.000105, KL fake Loss: 0.001832
Classification Train Epoch: 86 [19200/48000 (40%)]	Loss: 0.000016, KL fake Loss: 0.002630
Classification Train Epoch: 86 [25600/48000 (53%)]	Loss: 0.000022, KL fake Loss: 0.000904
Classification Train Epoch: 86 [32000/48000 (67%)]	Loss: 0.000030, KL fake Loss: 0.002667
Classification Train Epoch: 86 [38400/48000 (80%)]	Loss: 0.000014, KL fake Loss: 0.001050
Classification Train Epoch: 86 [44800/48000 (93%)]	Loss: 0.000050, KL fake Loss: 0.000826

Test set: Average loss: 1.8468, Accuracy: 4517/8000 (56%)

Classification Train Epoch: 87 [0/48000 (0%)]	Loss: 0.000210, KL fake Loss: 0.000899
Classification Train Epoch: 87 [6400/48000 (13%)]	Loss: 0.000038, KL fake Loss: 0.000665
Classification Train Epoch: 87 [12800/48000 (27%)]	Loss: 0.000928, KL fake Loss: 0.000850
Classification Train Epoch: 87 [19200/48000 (40%)]	Loss: 0.000434, KL fake Loss: 0.000545
Classification Train Epoch: 87 [25600/48000 (53%)]	Loss: 0.000003, KL fake Loss: 0.000655
Classification Train Epoch: 87 [32000/48000 (67%)]	Loss: 0.000017, KL fake Loss: 0.000614
Classification Train Epoch: 87 [38400/48000 (80%)]	Loss: 0.000014, KL fake Loss: 0.000907
Classification Train Epoch: 87 [44800/48000 (93%)]	Loss: 0.000237, KL fake Loss: 0.002324

Test set: Average loss: 1.8953, Accuracy: 3816/8000 (48%)

Classification Train Epoch: 88 [0/48000 (0%)]	Loss: 0.000058, KL fake Loss: 0.000930
Classification Train Epoch: 88 [6400/48000 (13%)]	Loss: 0.000028, KL fake Loss: 0.000713
Classification Train Epoch: 88 [12800/48000 (27%)]	Loss: 0.000011, KL fake Loss: 0.006504
Classification Train Epoch: 88 [19200/48000 (40%)]	Loss: 0.000011, KL fake Loss: 0.000996
Classification Train Epoch: 88 [25600/48000 (53%)]	Loss: 0.000128, KL fake Loss: 0.002028
Classification Train Epoch: 88 [32000/48000 (67%)]	Loss: 0.000127, KL fake Loss: 0.000474
Classification Train Epoch: 88 [38400/48000 (80%)]	Loss: 0.000052, KL fake Loss: 0.000920
Classification Train Epoch: 88 [44800/48000 (93%)]	Loss: 0.000087, KL fake Loss: 0.000558

Test set: Average loss: 1.8948, Accuracy: 4113/8000 (51%)

Classification Train Epoch: 89 [0/48000 (0%)]	Loss: 0.000081, KL fake Loss: 0.000773
Classification Train Epoch: 89 [6400/48000 (13%)]	Loss: 0.000044, KL fake Loss: 0.000524
Classification Train Epoch: 89 [12800/48000 (27%)]	Loss: 0.000776, KL fake Loss: 0.000706
Classification Train Epoch: 89 [19200/48000 (40%)]	Loss: 0.000038, KL fake Loss: 0.000701
Classification Train Epoch: 89 [25600/48000 (53%)]	Loss: 0.000014, KL fake Loss: 0.000873
Classification Train Epoch: 89 [32000/48000 (67%)]	Loss: 0.000030, KL fake Loss: 0.001062
Classification Train Epoch: 89 [38400/48000 (80%)]	Loss: 0.000037, KL fake Loss: 0.000850
Classification Train Epoch: 89 [44800/48000 (93%)]	Loss: 0.000052, KL fake Loss: 0.000644

Test set: Average loss: 1.9092, Accuracy: 3837/8000 (48%)

Classification Train Epoch: 90 [0/48000 (0%)]	Loss: 0.000247, KL fake Loss: 0.000494
Classification Train Epoch: 90 [6400/48000 (13%)]	Loss: 0.000096, KL fake Loss: 0.000533
Classification Train Epoch: 90 [12800/48000 (27%)]	Loss: 0.000017, KL fake Loss: 0.000744
Classification Train Epoch: 90 [19200/48000 (40%)]	Loss: 0.000168, KL fake Loss: 0.000532
Classification Train Epoch: 90 [25600/48000 (53%)]	Loss: 0.000031, KL fake Loss: 0.000766
Classification Train Epoch: 90 [32000/48000 (67%)]	Loss: 0.000038, KL fake Loss: 0.000823
Classification Train Epoch: 90 [38400/48000 (80%)]	Loss: 0.000007, KL fake Loss: 0.001412
Classification Train Epoch: 90 [44800/48000 (93%)]	Loss: 0.000025, KL fake Loss: 0.000485

Test set: Average loss: 1.9116, Accuracy: 3658/8000 (46%)

Classification Train Epoch: 91 [0/48000 (0%)]	Loss: 0.000042, KL fake Loss: 0.000874
Classification Train Epoch: 91 [6400/48000 (13%)]	Loss: 0.000003, KL fake Loss: 0.000532
Classification Train Epoch: 91 [12800/48000 (27%)]	Loss: 0.000008, KL fake Loss: 0.001297
Classification Train Epoch: 91 [19200/48000 (40%)]	Loss: 0.000020, KL fake Loss: 0.000707
Classification Train Epoch: 91 [25600/48000 (53%)]	Loss: 0.000013, KL fake Loss: 0.000728
Classification Train Epoch: 91 [32000/48000 (67%)]	Loss: 0.000009, KL fake Loss: 0.000458
Classification Train Epoch: 91 [38400/48000 (80%)]	Loss: 0.000089, KL fake Loss: 0.000497
Classification Train Epoch: 91 [44800/48000 (93%)]	Loss: 0.000004, KL fake Loss: 0.000507

Test set: Average loss: 1.9439, Accuracy: 3010/8000 (38%)

Classification Train Epoch: 92 [0/48000 (0%)]	Loss: 0.000010, KL fake Loss: 0.000634
Classification Train Epoch: 92 [6400/48000 (13%)]	Loss: 0.000014, KL fake Loss: 0.000801
Classification Train Epoch: 92 [12800/48000 (27%)]	Loss: 0.000071, KL fake Loss: 0.000591
Classification Train Epoch: 92 [19200/48000 (40%)]	Loss: 0.000022, KL fake Loss: 0.000923
Classification Train Epoch: 92 [25600/48000 (53%)]	Loss: 0.000009, KL fake Loss: 0.000521
Classification Train Epoch: 92 [32000/48000 (67%)]	Loss: 0.000298, KL fake Loss: 0.000754
Classification Train Epoch: 92 [38400/48000 (80%)]	Loss: 0.000059, KL fake Loss: 0.006272
Classification Train Epoch: 92 [44800/48000 (93%)]	Loss: 0.000026, KL fake Loss: 0.002309

Test set: Average loss: 1.7805, Accuracy: 5909/8000 (74%)

Classification Train Epoch: 93 [0/48000 (0%)]	Loss: 0.000006, KL fake Loss: 0.000886
Classification Train Epoch: 93 [6400/48000 (13%)]	Loss: 0.000020, KL fake Loss: 0.000628
Classification Train Epoch: 93 [12800/48000 (27%)]	Loss: 0.000101, KL fake Loss: 0.001129
Classification Train Epoch: 93 [19200/48000 (40%)]	Loss: 0.000066, KL fake Loss: 0.007131
Classification Train Epoch: 93 [25600/48000 (53%)]	Loss: 0.000041, KL fake Loss: 0.000772
Classification Train Epoch: 93 [32000/48000 (67%)]	Loss: 0.000018, KL fake Loss: 0.002059
Classification Train Epoch: 93 [38400/48000 (80%)]	Loss: 0.000061, KL fake Loss: 0.001568
Classification Train Epoch: 93 [44800/48000 (93%)]	Loss: 0.000246, KL fake Loss: 0.000771

Test set: Average loss: 1.8605, Accuracy: 4571/8000 (57%)

Classification Train Epoch: 94 [0/48000 (0%)]	Loss: 0.000020, KL fake Loss: 0.001077
Classification Train Epoch: 94 [6400/48000 (13%)]	Loss: 0.000244, KL fake Loss: 0.000745
Classification Train Epoch: 94 [12800/48000 (27%)]	Loss: 0.000040, KL fake Loss: 0.000979
Classification Train Epoch: 94 [19200/48000 (40%)]	Loss: 0.000380, KL fake Loss: 0.000722
Classification Train Epoch: 94 [25600/48000 (53%)]	Loss: 0.000022, KL fake Loss: 0.000659
Classification Train Epoch: 94 [32000/48000 (67%)]	Loss: 0.000014, KL fake Loss: 0.001109
Classification Train Epoch: 94 [38400/48000 (80%)]	Loss: 0.000008, KL fake Loss: 0.000721
Classification Train Epoch: 94 [44800/48000 (93%)]	Loss: 0.000014, KL fake Loss: 0.000671

Test set: Average loss: 1.9482, Accuracy: 2845/8000 (36%)

Classification Train Epoch: 95 [0/48000 (0%)]	Loss: 0.000023, KL fake Loss: 0.001736
Classification Train Epoch: 95 [6400/48000 (13%)]	Loss: 0.000327, KL fake Loss: 0.000704
Classification Train Epoch: 95 [12800/48000 (27%)]	Loss: 0.000006, KL fake Loss: 0.000880
Classification Train Epoch: 95 [19200/48000 (40%)]	Loss: 0.000023, KL fake Loss: 0.000850
Classification Train Epoch: 95 [25600/48000 (53%)]	Loss: 0.000031, KL fake Loss: 0.000732
Classification Train Epoch: 95 [32000/48000 (67%)]	Loss: 0.000031, KL fake Loss: 0.000744
Classification Train Epoch: 95 [38400/48000 (80%)]	Loss: 0.000284, KL fake Loss: 0.000547
Classification Train Epoch: 95 [44800/48000 (93%)]	Loss: 0.000092, KL fake Loss: 0.000512

Test set: Average loss: 1.9690, Accuracy: 2650/8000 (33%)

 96%|█████████▌| 96/100 [4:21:37<10:53, 163.47s/it] 97%|█████████▋| 97/100 [4:24:20<08:10, 163.47s/it] 98%|█████████▊| 98/100 [4:27:04<05:26, 163.47s/it] 99%|█████████▉| 99/100 [4:29:47<02:43, 163.48s/it]100%|██████████| 100/100 [4:32:31<00:00, 163.51s/it]100%|██████████| 100/100 [4:32:31<00:00, 163.51s/it]
Classification Train Epoch: 96 [0/48000 (0%)]	Loss: 0.000094, KL fake Loss: 0.002053
Classification Train Epoch: 96 [6400/48000 (13%)]	Loss: 0.000029, KL fake Loss: 0.005751
Classification Train Epoch: 96 [12800/48000 (27%)]	Loss: 0.000053, KL fake Loss: 0.087846
Classification Train Epoch: 96 [19200/48000 (40%)]	Loss: 0.000118, KL fake Loss: 0.000748
Classification Train Epoch: 96 [25600/48000 (53%)]	Loss: 0.000007, KL fake Loss: 0.000671
Classification Train Epoch: 96 [32000/48000 (67%)]	Loss: 0.000019, KL fake Loss: 0.000560
Classification Train Epoch: 96 [38400/48000 (80%)]	Loss: 0.000092, KL fake Loss: 0.000727
Classification Train Epoch: 96 [44800/48000 (93%)]	Loss: 0.000026, KL fake Loss: 0.000816

Test set: Average loss: 1.8290, Accuracy: 5233/8000 (65%)

Classification Train Epoch: 97 [0/48000 (0%)]	Loss: 0.000422, KL fake Loss: 0.000594
Classification Train Epoch: 97 [6400/48000 (13%)]	Loss: 0.000022, KL fake Loss: 0.000597
Classification Train Epoch: 97 [12800/48000 (27%)]	Loss: 0.000182, KL fake Loss: 0.000686
Classification Train Epoch: 97 [19200/48000 (40%)]	Loss: 0.000019, KL fake Loss: 0.000863
Classification Train Epoch: 97 [25600/48000 (53%)]	Loss: 0.000183, KL fake Loss: 0.000638
Classification Train Epoch: 97 [32000/48000 (67%)]	Loss: 0.000044, KL fake Loss: 0.000715
Classification Train Epoch: 97 [38400/48000 (80%)]	Loss: 0.000115, KL fake Loss: 0.000489
Classification Train Epoch: 97 [44800/48000 (93%)]	Loss: 0.000008, KL fake Loss: 0.000586

Test set: Average loss: 1.9392, Accuracy: 3053/8000 (38%)

Classification Train Epoch: 98 [0/48000 (0%)]	Loss: 0.000227, KL fake Loss: 0.000609
Classification Train Epoch: 98 [6400/48000 (13%)]	Loss: 0.000011, KL fake Loss: 0.000499
Classification Train Epoch: 98 [12800/48000 (27%)]	Loss: 0.000030, KL fake Loss: 0.000627
Classification Train Epoch: 98 [19200/48000 (40%)]	Loss: 0.000029, KL fake Loss: 0.000693
Classification Train Epoch: 98 [25600/48000 (53%)]	Loss: 0.000024, KL fake Loss: 0.000686
Classification Train Epoch: 98 [32000/48000 (67%)]	Loss: 0.000100, KL fake Loss: 0.000614
Classification Train Epoch: 98 [38400/48000 (80%)]	Loss: 0.000091, KL fake Loss: 0.000453
Classification Train Epoch: 98 [44800/48000 (93%)]	Loss: 0.000004, KL fake Loss: 0.000942

Test set: Average loss: 1.9095, Accuracy: 4031/8000 (50%)

Classification Train Epoch: 99 [0/48000 (0%)]	Loss: 0.000011, KL fake Loss: 0.000439
Classification Train Epoch: 99 [6400/48000 (13%)]	Loss: 0.000020, KL fake Loss: 0.000555
Classification Train Epoch: 99 [12800/48000 (27%)]	Loss: 0.000041, KL fake Loss: 0.004210
Classification Train Epoch: 99 [19200/48000 (40%)]	Loss: 0.000005, KL fake Loss: 0.000742
Classification Train Epoch: 99 [25600/48000 (53%)]	Loss: 0.000045, KL fake Loss: 0.001137
Classification Train Epoch: 99 [32000/48000 (67%)]	Loss: 0.000009, KL fake Loss: 0.000622
Classification Train Epoch: 99 [38400/48000 (80%)]	Loss: 0.000018, KL fake Loss: 0.000554
Classification Train Epoch: 99 [44800/48000 (93%)]	Loss: 0.000012, KL fake Loss: 0.000715

Test set: Average loss: 1.8242, Accuracy: 5298/8000 (66%)

Classification Train Epoch: 100 [0/48000 (0%)]	Loss: 0.000026, KL fake Loss: 0.056989
Classification Train Epoch: 100 [6400/48000 (13%)]	Loss: 0.000007, KL fake Loss: 0.000798
Classification Train Epoch: 100 [12800/48000 (27%)]	Loss: 0.000012, KL fake Loss: 0.000531
Classification Train Epoch: 100 [19200/48000 (40%)]	Loss: 0.000071, KL fake Loss: 0.000587
Classification Train Epoch: 100 [25600/48000 (53%)]	Loss: 0.000017, KL fake Loss: 0.000325
Classification Train Epoch: 100 [32000/48000 (67%)]	Loss: 0.000022, KL fake Loss: 0.000550
Classification Train Epoch: 100 [38400/48000 (80%)]	Loss: 0.000012, KL fake Loss: 0.000360
Classification Train Epoch: 100 [44800/48000 (93%)]	Loss: 0.000039, KL fake Loss: 0.000671

Test set: Average loss: 1.9677, Accuracy: 2869/8000 (36%)

Namespace(batch_size=128, no_cuda=False, seed=1, dataset='FashionMNIST', dataroot='./data', imageSize=32, outf='./results/joint_confidence_loss/FM-0.1/', out_dataset='FashionMNIST', num_classes=8, num_channels=1, pre_trained_net='results/joint_confidence_loss/FM-0.1/model_epoch_100.pth')
Random Seed:  1
Load model
DenseNet3(
  (conv1): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock(
    (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock(
    (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(342, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (fc): Linear(in_features=342, out_features=8, bias=True)
  (softmax): Softmax(dim=1)
)ic| len(dset): 60000
ic| len(dset): 10000
ic| len(dset): 60000
ic| len(dset): 10000

load target data:  FashionMNIST
load non target data:  FashionMNIST
generate log from in-distribution data

 Final Accuracy: 2869/8000 (35.86%)

generate log  from out-of-distribution data
calculate metrics
  Performance of Baseline detector
TNR at TPR 95%:             1.858%
TNR at TPR 99%:             0.098%
AUROC:                     45.154%
Detection acc:             50.913%
AUPR In:                   48.275%
AUPR Out:                  44.841%

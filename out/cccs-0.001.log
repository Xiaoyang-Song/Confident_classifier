ic| len(dset): 60000
ic| len(dset): 10000
Namespace(batch_size=64, epochs=100, lr=0.001, no_cuda=False, seed=1, log_interval=100, dataset='FashionMNIST', dataroot='./data', imageSize=32, outf='./results/joint_confidence_loss/FM-0.001/', wd=0.0, droprate=0.1, decreasing_lr='60', num_classes=8, beta=0.001, num_channels=1)
Random Seed:  1
load InD data for Experiment:  FashionMNIST
Load model
DenseNet3(
  (conv1): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock(
    (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock(
    (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(342, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (fc): Linear(in_features=342, out_features=8, bias=True)
  (softmax): Softmax(dim=1)
)
load GAN
Setup optimizer
0.001
  0%|          | 0/100 [00:00<?, ?it/s]/home/xysong/.conda/envs/OoD/lib/python3.9/site-packages/torch/nn/functional.py:2904: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
 15%|█▌        | 15/100 [53:04<5:00:40, 212.24s/it]  1%|          | 1/100 [02:43<4:30:23, 163.88s/it] 16%|█▌        | 16/100 [56:36<4:57:07, 212.23s/it]  2%|▏         | 2/100 [05:27<4:27:21, 163.68s/it]  3%|▎         | 3/100 [08:10<4:24:31, 163.62s/it] 17%|█▋        | 17/100 [1:00:08<4:53:34, 212.23s/it]  4%|▍         | 4/100 [10:54<4:21:45, 163.60s/it] 18%|█▊        | 18/100 [1:03:41<4:50:01, 212.22s/it]  5%|▌         | 5/100 [13:38<4:18:59, 163.58s/it] 19%|█▉        | 19/100 [1:07:13<4:46:29, 212.22s/it]  6%|▌         | 6/100 [16:21<4:16:15, 163.57s/it]  7%|▋         | 7/100 [19:05<4:13:32, 163.57s/it] 20%|██        | 20/100 [1:10:45<4:43:00, 212.25s/it]  8%|▊         | 8/100 [21:48<4:10:48, 163.57s/it] 21%|██        | 21/100 [1:14:17<4:39:27, 212.25s/it]Classification Train Epoch: 11 [32000/50000 (64%)]	Loss: 0.212548, KL fake Loss: 0.014723
Classification Train Epoch: 11 [38400/50000 (77%)]	Loss: 0.087734, KL fake Loss: 0.027861
Classification Train Epoch: 11 [44800/50000 (90%)]	Loss: 0.207916, KL fake Loss: 0.015446

Test set: Average loss: 1.9253, Accuracy: 3887/10000 (39%)

Classification Train Epoch: 12 [0/50000 (0%)]	Loss: 0.038124, KL fake Loss: 0.016972
Classification Train Epoch: 12 [6400/50000 (13%)]	Loss: 0.131757, KL fake Loss: 0.009217
Classification Train Epoch: 12 [12800/50000 (26%)]	Loss: 0.033016, KL fake Loss: 0.020963
Classification Train Epoch: 12 [19200/50000 (38%)]	Loss: 0.043659, KL fake Loss: 0.025143
Classification Train Epoch: 12 [25600/50000 (51%)]	Loss: 0.328693, KL fake Loss: 0.029168
Classification Train Epoch: 12 [32000/50000 (64%)]	Loss: 0.207482, KL fake Loss: 0.025201
Classification Train Epoch: 12 [38400/50000 (77%)]	Loss: 0.312318, KL fake Loss: 0.031879
Classification Train Epoch: 12 [44800/50000 (90%)]	Loss: 0.218671, KL fake Loss: 0.017583

Test set: Average loss: 1.8837, Accuracy: 4104/10000 (41%)

Classification Train Epoch: 13 [0/50000 (0%)]	Loss: 0.092734, KL fake Loss: 0.030693
Classification Train Epoch: 13 [6400/50000 (13%)]	Loss: 0.055350, KL fake Loss: 0.013512
Classification Train Epoch: 13 [12800/50000 (26%)]	Loss: 0.055193, KL fake Loss: 0.018116
Classification Train Epoch: 13 [19200/50000 (38%)]	Loss: 0.213527, KL fake Loss: 0.013054
Classification Train Epoch: 13 [25600/50000 (51%)]	Loss: 0.264165, KL fake Loss: 0.011227
Classification Train Epoch: 13 [32000/50000 (64%)]	Loss: 0.165991, KL fake Loss: 0.019629
Classification Train Epoch: 13 [38400/50000 (77%)]	Loss: 0.093782, KL fake Loss: 0.036366
Classification Train Epoch: 13 [44800/50000 (90%)]	Loss: 0.029329, KL fake Loss: 0.043721

Test set: Average loss: 1.8773, Accuracy: 3484/10000 (35%)

Classification Train Epoch: 14 [0/50000 (0%)]	Loss: 0.036734, KL fake Loss: 0.016485
Classification Train Epoch: 14 [6400/50000 (13%)]	Loss: 0.109243, KL fake Loss: 0.024739
Classification Train Epoch: 14 [12800/50000 (26%)]	Loss: 0.100811, KL fake Loss: 0.020435
Classification Train Epoch: 14 [19200/50000 (38%)]	Loss: 0.143626, KL fake Loss: 0.018819
Classification Train Epoch: 14 [25600/50000 (51%)]	Loss: 0.089578, KL fake Loss: 0.021518
Classification Train Epoch: 14 [32000/50000 (64%)]	Loss: 0.205877, KL fake Loss: 0.017332
Classification Train Epoch: 14 [38400/50000 (77%)]	Loss: 0.159497, KL fake Loss: 0.027776
Classification Train Epoch: 14 [44800/50000 (90%)]	Loss: 0.143936, KL fake Loss: 0.016066

Test set: Average loss: 1.9714, Accuracy: 3290/10000 (33%)

Classification Train Epoch: 15 [0/50000 (0%)]	Loss: 0.111445, KL fake Loss: 0.016437
Classification Train Epoch: 15 [6400/50000 (13%)]	Loss: 0.217456, KL fake Loss: 0.010326
Classification Train Epoch: 15 [12800/50000 (26%)]	Loss: 0.093905, KL fake Loss: 0.012918
Classification Train Epoch: 15 [19200/50000 (38%)]	Loss: 0.036011, KL fake Loss: 0.010561
Classification Train Epoch: 15 [25600/50000 (51%)]	Loss: 0.066022, KL fake Loss: 0.013232
Classification Train Epoch: 15 [32000/50000 (64%)]	Loss: 0.062861, KL fake Loss: 0.016368
Classification Train Epoch: 15 [38400/50000 (77%)]	Loss: 0.029519, KL fake Loss: 0.010704
Classification Train Epoch: 15 [44800/50000 (90%)]	Loss: 0.035024, KL fake Loss: 0.006918

Test set: Average loss: 2.0151, Accuracy: 3560/10000 (36%)

Classification Train Epoch: 16 [0/50000 (0%)]	Loss: 0.104750, KL fake Loss: 0.014336
Classification Train Epoch: 16 [6400/50000 (13%)]	Loss: 0.028896, KL fake Loss: 0.008817
Classification Train Epoch: 16 [12800/50000 (26%)]	Loss: 0.034766, KL fake Loss: 0.018757
Classification Train Epoch: 16 [19200/50000 (38%)]	Loss: 0.032019, KL fake Loss: 0.011946
Classification Train Epoch: 16 [25600/50000 (51%)]	Loss: 0.096190, KL fake Loss: 0.017579
Classification Train Epoch: 16 [32000/50000 (64%)]	Loss: 0.068666, KL fake Loss: 0.014191
Classification Train Epoch: 16 [38400/50000 (77%)]	Loss: 0.151366, KL fake Loss: 0.022159
Classification Train Epoch: 16 [44800/50000 (90%)]	Loss: 0.128800, KL fake Loss: 0.011501

Test set: Average loss: 2.1978, Accuracy: 3548/10000 (35%)

Classification Train Epoch: 17 [0/50000 (0%)]	Loss: 0.102388, KL fake Loss: 0.015075
Classification Train Epoch: 17 [6400/50000 (13%)]	Loss: 0.011901, KL fake Loss: 0.010504
Classification Train Epoch: 17 [12800/50000 (26%)]	Loss: 0.027869, KL fake Loss: 0.013717
Classification Train Epoch: 17 [19200/50000 (38%)]	Loss: 0.169307, KL fake Loss: 0.035905
Classification Train Epoch: 17 [25600/50000 (51%)]	Loss: 0.030996, KL fake Loss: 0.036016
Classification Train Epoch: 17 [32000/50000 (64%)]	Loss: 0.040640, KL fake Loss: 0.019544
Classification Train Epoch: 17 [38400/50000 (77%)]	Loss: 0.049673, KL fake Loss: 0.014851
Classification Train Epoch: 17 [44800/50000 (90%)]	Loss: 0.168949, KL fake Loss: 0.020936

Test set: Average loss: 2.1464, Accuracy: 3302/10000 (33%)

Classification Train Epoch: 18 [0/50000 (0%)]	Loss: 0.066585, KL fake Loss: 0.014126
Classification Train Epoch: 18 [6400/50000 (13%)]	Loss: 0.036012, KL fake Loss: 0.017168
Classification Train Epoch: 18 [12800/50000 (26%)]	Loss: 0.051865, KL fake Loss: 0.014084
Classification Train Epoch: 18 [19200/50000 (38%)]	Loss: 0.032401, KL fake Loss: 0.015350
Classification Train Epoch: 18 [25600/50000 (51%)]	Loss: 0.084339, KL fake Loss: 0.019780
Classification Train Epoch: 18 [32000/50000 (64%)]	Loss: 0.041913, KL fake Loss: 0.028846
Classification Train Epoch: 18 [38400/50000 (77%)]	Loss: 0.057104, KL fake Loss: 0.025432
Classification Train Epoch: 18 [44800/50000 (90%)]	Loss: 0.055473, KL fake Loss: 0.009174

Test set: Average loss: 2.6505, Accuracy: 2537/10000 (25%)

Classification Train Epoch: 19 [0/50000 (0%)]	Loss: 0.038920, KL fake Loss: 0.028954
Classification Train Epoch: 19 [6400/50000 (13%)]	Loss: 0.113915, KL fake Loss: 0.013616
Classification Train Epoch: 19 [12800/50000 (26%)]	Loss: 0.016072, KL fake Loss: 0.019596
Classification Train Epoch: 19 [19200/50000 (38%)]	Loss: 0.049767, KL fake Loss: 0.009648
Classification Train Epoch: 19 [25600/50000 (51%)]	Loss: 0.059924, KL fake Loss: 0.014099
Classification Train Epoch: 19 [32000/50000 (64%)]	Loss: 0.028816, KL fake Loss: 0.017449
Classification Train Epoch: 19 [38400/50000 (77%)]	Loss: 0.075112, KL fake Loss: 0.028391
Classification Train Epoch: 19 [44800/50000 (90%)]	Loss: 0.141547, KL fake Loss: 0.015269

Test set: Average loss: 2.4581, Accuracy: 3107/10000 (31%)

Classification Train Epoch: 20 [0/50000 (0%)]	Loss: 0.088004, KL fake Loss: 0.031567
Classification Train Epoch: 20 [6400/50000 (13%)]	Loss: 0.131056, KL fake Loss: 0.021417
Classification Train Epoch: 20 [12800/50000 (26%)]	Loss: 0.038886, KL fake Loss: 0.032516
Classification Train Epoch: 20 [19200/50000 (38%)]	Loss: 0.051937, KL fake Loss: 0.018010
Classification Train Epoch: 20 [25600/50000 (51%)]	Loss: 0.065714, KL fake Loss: 0.012976
Classification Train Epoch: 20 [32000/50000 (64%)]	Loss: 0.020957, KL fake Loss: 0.014167
Classification Train Epoch: 20 [38400/50000 (77%)]	Loss: 0.039768, KL fake Loss: 0.020167
Classification Train Epoch: 20 [44800/50000 (90%)]	Loss: 0.124288, KL fake Loss: 0.016133

Test set: Average loss: 2.4407, Accuracy: 2729/10000 (27%)

Classification Train Epoch: 21 [0/50000 (0%)]	Loss: 0.028835, KL fake Loss: 0.010954
Classification Train Epoch: 21 [6400/50000 (13%)]	Loss: 0.062966, KL fake Loss: 0.015413
Classification Train Epoch: 21 [12800/50000 (26%)]	Loss: 0.060733, KL fake Loss: 0.023096
Classification Train Epoch: 21 [19200/50000 (38%)]	Loss: 0.102889, KL fake Loss: 0.005575
Classification Train Epoch: 21 [25600/50000 (51%)]	Loss: 0.114294, KL fake Loss: 0.010848
Classification Train Epoch: 21 [32000/50000 (64%)]	Loss: 0.053676, KL fake Loss: 0.017315
Classification Train Epoch: 21 [38400/50000 (77%)]	Loss: 0.106819, KL fake Loss: 0.021671
Classification Train Epoch: 21 [44800/50000 (90%)]	Loss: 0.189596, KL fake Loss: 0.020567

Test set: Average loss: 2.4711, Accuracy: 2478/10000 (25%)

Classification Train Epoch: 22 [0/50000 (0%)]	Loss: 0.037395, KL fake Loss: 0.025905
  9%|▉         | 9/100 [24:32<4:08:04, 163.57s/it] 22%|██▏       | 22/100 [1:17:50<4:35:55, 212.25s/it] 10%|█         | 10/100 [27:15<4:05:21, 163.57s/it]Classification Train Epoch: 1 [0/48000 (0%)]	Loss: 2.131171, KL fake Loss: 0.036354
Classification Train Epoch: 1 [6400/48000 (13%)]	Loss: 0.556221, KL fake Loss: 1.490043
Classification Train Epoch: 1 [12800/48000 (27%)]	Loss: 0.375855, KL fake Loss: 2.384501
Classification Train Epoch: 1 [19200/48000 (40%)]	Loss: 0.457674, KL fake Loss: 2.534861
Classification Train Epoch: 1 [25600/48000 (53%)]	Loss: 0.261945, KL fake Loss: 2.849358
Classification Train Epoch: 1 [32000/48000 (67%)]	Loss: 0.224552, KL fake Loss: 3.249392
Classification Train Epoch: 1 [38400/48000 (80%)]	Loss: 0.315633, KL fake Loss: 2.954937
Classification Train Epoch: 1 [44800/48000 (93%)]	Loss: 0.398113, KL fake Loss: 3.328030

Test set: Average loss: 0.3556, Accuracy: 6995/8000 (87%)

Classification Train Epoch: 2 [0/48000 (0%)]	Loss: 0.395531, KL fake Loss: 3.477937
Classification Train Epoch: 2 [6400/48000 (13%)]	Loss: 0.262431, KL fake Loss: 3.818824
Classification Train Epoch: 2 [12800/48000 (27%)]	Loss: 0.187248, KL fake Loss: 4.097095
Classification Train Epoch: 2 [19200/48000 (40%)]	Loss: 0.406755, KL fake Loss: 4.086260
Classification Train Epoch: 2 [25600/48000 (53%)]	Loss: 0.188053, KL fake Loss: 4.038288
Classification Train Epoch: 2 [32000/48000 (67%)]	Loss: 0.231965, KL fake Loss: 3.666029
Classification Train Epoch: 2 [38400/48000 (80%)]	Loss: 0.087828, KL fake Loss: 3.839978
Classification Train Epoch: 2 [44800/48000 (93%)]	Loss: 0.295340, KL fake Loss: 3.767070

Test set: Average loss: 0.3714, Accuracy: 6954/8000 (87%)

Classification Train Epoch: 3 [0/48000 (0%)]	Loss: 0.312946, KL fake Loss: 4.705142
Classification Train Epoch: 3 [6400/48000 (13%)]	Loss: 0.289796, KL fake Loss: 4.375064
Classification Train Epoch: 3 [12800/48000 (27%)]	Loss: 0.200672, KL fake Loss: 4.355386
Classification Train Epoch: 3 [19200/48000 (40%)]	Loss: 0.370133, KL fake Loss: 4.570182
Classification Train Epoch: 3 [25600/48000 (53%)]	Loss: 0.268563, KL fake Loss: 4.409597
Classification Train Epoch: 3 [32000/48000 (67%)]	Loss: 0.226402, KL fake Loss: 4.669416
Classification Train Epoch: 3 [38400/48000 (80%)]	Loss: 0.272224, KL fake Loss: 4.417255
Classification Train Epoch: 3 [44800/48000 (93%)]	Loss: 0.107048, KL fake Loss: 4.210593

Test set: Average loss: 0.2919, Accuracy: 7187/8000 (90%)

Classification Train Epoch: 4 [0/48000 (0%)]	Loss: 0.321899, KL fake Loss: 4.806856
Classification Train Epoch: 4 [6400/48000 (13%)]	Loss: 0.139781, KL fake Loss: 4.274414
Classification Train Epoch: 4 [12800/48000 (27%)]	Loss: 0.186286, KL fake Loss: 4.883650
Classification Train Epoch: 4 [19200/48000 (40%)]	Loss: 0.109179, KL fake Loss: 4.819000
Classification Train Epoch: 4 [25600/48000 (53%)]	Loss: 0.328260, KL fake Loss: 4.702056
Classification Train Epoch: 4 [32000/48000 (67%)]	Loss: 0.366311, KL fake Loss: 4.619808
Classification Train Epoch: 4 [38400/48000 (80%)]	Loss: 0.118383, KL fake Loss: 4.714013
Classification Train Epoch: 4 [44800/48000 (93%)]	Loss: 0.227780, KL fake Loss: 5.149653

Test set: Average loss: 0.2311, Accuracy: 7343/8000 (92%)

Classification Train Epoch: 5 [0/48000 (0%)]	Loss: 0.166883, KL fake Loss: 4.990814
Classification Train Epoch: 5 [6400/48000 (13%)]	Loss: 0.153077, KL fake Loss: 4.640681
Classification Train Epoch: 5 [12800/48000 (27%)]	Loss: 0.205211, KL fake Loss: 5.103571
Classification Train Epoch: 5 [19200/48000 (40%)]	Loss: 0.236348, KL fake Loss: 5.266685
Classification Train Epoch: 5 [25600/48000 (53%)]	Loss: 0.132925, KL fake Loss: 5.180274
Classification Train Epoch: 5 [32000/48000 (67%)]	Loss: 0.247294, KL fake Loss: 4.930861
Classification Train Epoch: 5 [38400/48000 (80%)]	Loss: 0.090043, KL fake Loss: 5.432752
Classification Train Epoch: 5 [44800/48000 (93%)]	Loss: 0.184427, KL fake Loss: 5.399434

Test set: Average loss: 0.2298, Accuracy: 7356/8000 (92%)

Classification Train Epoch: 6 [0/48000 (0%)]	Loss: 0.202899, KL fake Loss: 5.163062
Classification Train Epoch: 6 [6400/48000 (13%)]	Loss: 0.151288, KL fake Loss: 4.885067
Classification Train Epoch: 6 [12800/48000 (27%)]	Loss: 0.205639, KL fake Loss: 5.096790
Classification Train Epoch: 6 [19200/48000 (40%)]	Loss: 0.078763, KL fake Loss: 4.981892
Classification Train Epoch: 6 [25600/48000 (53%)]	Loss: 0.142035, KL fake Loss: 5.593181
Classification Train Epoch: 6 [32000/48000 (67%)]	Loss: 0.300814, KL fake Loss: 4.907858
Classification Train Epoch: 6 [38400/48000 (80%)]	Loss: 0.228237, KL fake Loss: 5.462407
Classification Train Epoch: 6 [44800/48000 (93%)]	Loss: 0.118081, KL fake Loss: 5.255136

Test set: Average loss: 0.2422, Accuracy: 7315/8000 (91%)

Classification Train Epoch: 7 [0/48000 (0%)]	Loss: 0.192325, KL fake Loss: 5.039639
Classification Train Epoch: 7 [6400/48000 (13%)]	Loss: 0.064926, KL fake Loss: 4.933039
Classification Train Epoch: 7 [12800/48000 (27%)]	Loss: 0.122188, KL fake Loss: 5.714104
Classification Train Epoch: 7 [19200/48000 (40%)]	Loss: 0.289094, KL fake Loss: 5.011048
Classification Train Epoch: 7 [25600/48000 (53%)]	Loss: 0.070294, KL fake Loss: 5.214157
Classification Train Epoch: 7 [32000/48000 (67%)]	Loss: 0.072905, KL fake Loss: 5.741677
Classification Train Epoch: 7 [38400/48000 (80%)]	Loss: 0.299719, KL fake Loss: 5.177258
Classification Train Epoch: 7 [44800/48000 (93%)]	Loss: 0.119178, KL fake Loss: 5.562861

Test set: Average loss: 0.2450, Accuracy: 7320/8000 (92%)

Classification Train Epoch: 8 [0/48000 (0%)]	Loss: 0.171530, KL fake Loss: 5.408843
Classification Train Epoch: 8 [6400/48000 (13%)]	Loss: 0.158908, KL fake Loss: 5.802119
Classification Train Epoch: 8 [12800/48000 (27%)]	Loss: 0.133060, KL fake Loss: 5.399985
Classification Train Epoch: 8 [19200/48000 (40%)]	Loss: 0.164112, KL fake Loss: 5.485814
Classification Train Epoch: 8 [25600/48000 (53%)]	Loss: 0.155644, KL fake Loss: 5.601627
Classification Train Epoch: 8 [32000/48000 (67%)]	Loss: 0.222168, KL fake Loss: 5.223139
Classification Train Epoch: 8 [38400/48000 (80%)]	Loss: 0.154149, KL fake Loss: 5.909665
Classification Train Epoch: 8 [44800/48000 (93%)]	Loss: 0.122327, KL fake Loss: 5.525035

Test set: Average loss: 0.2253, Accuracy: 7363/8000 (92%)

Classification Train Epoch: 9 [0/48000 (0%)]	Loss: 0.062020, KL fake Loss: 5.008094
Classification Train Epoch: 9 [6400/48000 (13%)]	Loss: 0.255997, KL fake Loss: 5.698804
Classification Train Epoch: 9 [12800/48000 (27%)]	Loss: 0.116755, KL fake Loss: 5.333490
Classification Train Epoch: 9 [19200/48000 (40%)]	Loss: 0.121967, KL fake Loss: 5.819992
Classification Train Epoch: 9 [25600/48000 (53%)]	Loss: 0.152849, KL fake Loss: 5.806088
Classification Train Epoch: 9 [32000/48000 (67%)]	Loss: 0.094562, KL fake Loss: 5.863798
Classification Train Epoch: 9 [38400/48000 (80%)]	Loss: 0.125823, KL fake Loss: 5.986809
Classification Train Epoch: 9 [44800/48000 (93%)]	Loss: 0.065940, KL fake Loss: 6.075541

Test set: Average loss: 0.2442, Accuracy: 7351/8000 (92%)

Classification Train Epoch: 10 [0/48000 (0%)]	Loss: 0.231770, KL fake Loss: 5.689782
Classification Train Epoch: 10 [6400/48000 (13%)]	Loss: 0.031301, KL fake Loss: 5.611119
Classification Train Epoch: 10 [12800/48000 (27%)]	Loss: 0.123534, KL fake Loss: 6.012808
Classification Train Epoch: 10 [19200/48000 (40%)]	Loss: 0.020557, KL fake Loss: 5.502139
Classification Train Epoch: 10 [25600/48000 (53%)]	Loss: 0.181329, KL fake Loss: 5.637462
Classification Train Epoch: 10 [32000/48000 (67%)]	Loss: 0.190229, KL fake Loss: 5.504747
Classification Train Epoch: 10 [38400/48000 (80%)]	Loss: 0.077147, KL fake Loss: 5.705241
Classification Train Epoch: 10 [44800/48000 (93%)]	Loss: 0.189438, KL fake Loss: 6.077808

Test set: Average loss: 0.2418, Accuracy: 7365/8000 (92%)

Classification Train Epoch: 11 [0/48000 (0%)]	Loss: 0.150465, KL fake Loss: 6.019071
Classification Train Epoch: 11 [6400/48000 (13%)]	Loss: 0.071651, KL fake Loss: 5.854670
Classification Train Epoch: 11 [12800/48000 (27%)]	Loss: 0.072501, KL fake Loss: 6.284957
Classification Train Epoch: 11 [19200/48000 (40%)]	Loss: 0.073068, KL fake Loss: 5.806089
Classification Train Epoch: 11 [25600/48000 (53%)]	Loss: 0.030961, KL fake Loss: 5.979211
 23%|██▎       | 23/100 [1:21:22<4:32:22, 212.24s/it] 11%|█         | 11/100 [29:59<4:02:37, 163.57s/it] 12%|█▏        | 12/100 [32:43<3:59:54, 163.57s/it] 24%|██▍       | 24/100 [1:24:54<4:28:49, 212.24s/it] 13%|█▎        | 13/100 [35:26<3:57:10, 163.57s/it] 25%|██▌       | 25/100 [1:28:26<4:25:16, 212.23s/it] 14%|█▍        | 14/100 [38:10<3:54:26, 163.57s/it] 26%|██▌       | 26/100 [1:31:59<4:21:44, 212.23s/it] 15%|█▌        | 15/100 [40:53<3:51:43, 163.58s/it] 16%|█▌        | 16/100 [43:37<3:49:00, 163.58s/it] 27%|██▋       | 27/100 [1:35:31<4:18:12, 212.22s/it] 17%|█▋        | 17/100 [46:20<3:46:16, 163.58s/it] 28%|██▊       | 28/100 [1:39:03<4:14:39, 212.22s/it] 18%|█▊        | 18/100 [49:04<3:43:33, 163.58s/it] 29%|██▉       | 29/100 [1:42:35<4:11:06, 212.21s/it] 19%|█▉        | 19/100 [51:48<3:40:50, 163.59s/it] 20%|██        | 20/100 [54:31<3:38:09, 163.61s/it] 30%|███       | 30/100 [1:46:07<4:07:35, 212.22s/it] 21%|██        | 21/100 [57:15<3:35:25, 163.61s/it]Classification Train Epoch: 11 [32000/48000 (67%)]	Loss: 0.115375, KL fake Loss: 5.630680
Classification Train Epoch: 11 [38400/48000 (80%)]	Loss: 0.279487, KL fake Loss: 6.101155
Classification Train Epoch: 11 [44800/48000 (93%)]	Loss: 0.044965, KL fake Loss: 5.934929

Test set: Average loss: 0.2771, Accuracy: 7340/8000 (92%)

Classification Train Epoch: 12 [0/48000 (0%)]	Loss: 0.034007, KL fake Loss: 6.313941
Classification Train Epoch: 12 [6400/48000 (13%)]	Loss: 0.051397, KL fake Loss: 6.087794
Classification Train Epoch: 12 [12800/48000 (27%)]	Loss: 0.202580, KL fake Loss: 6.546753
Classification Train Epoch: 12 [19200/48000 (40%)]	Loss: 0.037425, KL fake Loss: 6.044616
Classification Train Epoch: 12 [25600/48000 (53%)]	Loss: 0.100200, KL fake Loss: 6.370005
Classification Train Epoch: 12 [32000/48000 (67%)]	Loss: 0.221144, KL fake Loss: 6.490998
Classification Train Epoch: 12 [38400/48000 (80%)]	Loss: 0.081554, KL fake Loss: 6.449890
Classification Train Epoch: 12 [44800/48000 (93%)]	Loss: 0.220322, KL fake Loss: 6.123002

Test set: Average loss: 0.2631, Accuracy: 7351/8000 (92%)

Classification Train Epoch: 13 [0/48000 (0%)]	Loss: 0.056615, KL fake Loss: 5.888749
Classification Train Epoch: 13 [6400/48000 (13%)]	Loss: 0.025789, KL fake Loss: 6.148477
Classification Train Epoch: 13 [12800/48000 (27%)]	Loss: 0.038247, KL fake Loss: 6.403306
Classification Train Epoch: 13 [19200/48000 (40%)]	Loss: 0.037145, KL fake Loss: 6.080561
Classification Train Epoch: 13 [25600/48000 (53%)]	Loss: 0.019260, KL fake Loss: 6.328706
Classification Train Epoch: 13 [32000/48000 (67%)]	Loss: 0.111649, KL fake Loss: 6.096788
Classification Train Epoch: 13 [38400/48000 (80%)]	Loss: 0.087574, KL fake Loss: 6.063999
Classification Train Epoch: 13 [44800/48000 (93%)]	Loss: 0.225682, KL fake Loss: 6.631804

Test set: Average loss: 0.2725, Accuracy: 7367/8000 (92%)

Classification Train Epoch: 14 [0/48000 (0%)]	Loss: 0.048589, KL fake Loss: 5.893189
Classification Train Epoch: 14 [6400/48000 (13%)]	Loss: 0.026617, KL fake Loss: 6.380300
Classification Train Epoch: 14 [12800/48000 (27%)]	Loss: 0.041603, KL fake Loss: 6.348706
Classification Train Epoch: 14 [19200/48000 (40%)]	Loss: 0.028372, KL fake Loss: 6.593184
Classification Train Epoch: 14 [25600/48000 (53%)]	Loss: 0.098428, KL fake Loss: 6.325336
Classification Train Epoch: 14 [32000/48000 (67%)]	Loss: 0.027978, KL fake Loss: 6.836706
Classification Train Epoch: 14 [38400/48000 (80%)]	Loss: 0.064608, KL fake Loss: 7.120091
Classification Train Epoch: 14 [44800/48000 (93%)]	Loss: 0.073836, KL fake Loss: 7.134224

Test set: Average loss: 0.2735, Accuracy: 7396/8000 (92%)

Classification Train Epoch: 15 [0/48000 (0%)]	Loss: 0.089756, KL fake Loss: 6.359237
Classification Train Epoch: 15 [6400/48000 (13%)]	Loss: 0.065230, KL fake Loss: 6.864017
Classification Train Epoch: 15 [12800/48000 (27%)]	Loss: 0.030924, KL fake Loss: 6.373890
Classification Train Epoch: 15 [19200/48000 (40%)]	Loss: 0.064182, KL fake Loss: 6.396185
Classification Train Epoch: 15 [25600/48000 (53%)]	Loss: 0.014698, KL fake Loss: 6.740658
Classification Train Epoch: 15 [32000/48000 (67%)]	Loss: 0.009329, KL fake Loss: 6.755937
Classification Train Epoch: 15 [38400/48000 (80%)]	Loss: 0.066333, KL fake Loss: 7.013794
Classification Train Epoch: 15 [44800/48000 (93%)]	Loss: 0.057702, KL fake Loss: 6.700243

Test set: Average loss: 0.2695, Accuracy: 7398/8000 (92%)

Classification Train Epoch: 16 [0/48000 (0%)]	Loss: 0.079159, KL fake Loss: 6.574121
Classification Train Epoch: 16 [6400/48000 (13%)]	Loss: 0.080141, KL fake Loss: 7.306125
Classification Train Epoch: 16 [12800/48000 (27%)]	Loss: 0.025194, KL fake Loss: 6.996763
Classification Train Epoch: 16 [19200/48000 (40%)]	Loss: 0.058227, KL fake Loss: 7.242005
Classification Train Epoch: 16 [25600/48000 (53%)]	Loss: 0.077799, KL fake Loss: 7.207328
Classification Train Epoch: 16 [32000/48000 (67%)]	Loss: 0.147027, KL fake Loss: 6.636949
Classification Train Epoch: 16 [38400/48000 (80%)]	Loss: 0.029031, KL fake Loss: 6.545029
Classification Train Epoch: 16 [44800/48000 (93%)]	Loss: 0.077309, KL fake Loss: 6.644652

Test set: Average loss: 0.3070, Accuracy: 7335/8000 (92%)

Classification Train Epoch: 17 [0/48000 (0%)]	Loss: 0.043401, KL fake Loss: 6.915112
Classification Train Epoch: 17 [6400/48000 (13%)]	Loss: 0.011013, KL fake Loss: 6.900146
Classification Train Epoch: 17 [12800/48000 (27%)]	Loss: 0.021602, KL fake Loss: 6.744467
Classification Train Epoch: 17 [19200/48000 (40%)]	Loss: 0.007734, KL fake Loss: 7.577024
Classification Train Epoch: 17 [25600/48000 (53%)]	Loss: 0.042928, KL fake Loss: 7.150088
Classification Train Epoch: 17 [32000/48000 (67%)]	Loss: 0.188995, KL fake Loss: 7.258967
Classification Train Epoch: 17 [38400/48000 (80%)]	Loss: 0.053307, KL fake Loss: 6.896343
Classification Train Epoch: 17 [44800/48000 (93%)]	Loss: 0.020866, KL fake Loss: 6.795304

Test set: Average loss: 0.2816, Accuracy: 7422/8000 (93%)

Classification Train Epoch: 18 [0/48000 (0%)]	Loss: 0.015306, KL fake Loss: 6.969993
Classification Train Epoch: 18 [6400/48000 (13%)]	Loss: 0.020126, KL fake Loss: 6.628673
Classification Train Epoch: 18 [12800/48000 (27%)]	Loss: 0.053383, KL fake Loss: 6.852743
Classification Train Epoch: 18 [19200/48000 (40%)]	Loss: 0.050425, KL fake Loss: 7.285569
Classification Train Epoch: 18 [25600/48000 (53%)]	Loss: 0.083938, KL fake Loss: 6.997233
Classification Train Epoch: 18 [32000/48000 (67%)]	Loss: 0.082503, KL fake Loss: 7.011636
Classification Train Epoch: 18 [38400/48000 (80%)]	Loss: 0.077378, KL fake Loss: 7.317613
Classification Train Epoch: 18 [44800/48000 (93%)]	Loss: 0.038173, KL fake Loss: 7.079915

Test set: Average loss: 0.2754, Accuracy: 7429/8000 (93%)

Classification Train Epoch: 19 [0/48000 (0%)]	Loss: 0.026203, KL fake Loss: 6.637044
Classification Train Epoch: 19 [6400/48000 (13%)]	Loss: 0.012837, KL fake Loss: 7.453934
Classification Train Epoch: 19 [12800/48000 (27%)]	Loss: 0.041031, KL fake Loss: 7.369463
Classification Train Epoch: 19 [19200/48000 (40%)]	Loss: 0.046929, KL fake Loss: 7.151077
Classification Train Epoch: 19 [25600/48000 (53%)]	Loss: 0.043331, KL fake Loss: 6.678772
Classification Train Epoch: 19 [32000/48000 (67%)]	Loss: 0.059038, KL fake Loss: 6.960159
Classification Train Epoch: 19 [38400/48000 (80%)]	Loss: 0.031190, KL fake Loss: 7.327844
Classification Train Epoch: 19 [44800/48000 (93%)]	Loss: 0.066641, KL fake Loss: 6.821121

Test set: Average loss: 0.3199, Accuracy: 7386/8000 (92%)

Classification Train Epoch: 20 [0/48000 (0%)]	Loss: 0.022915, KL fake Loss: 6.824987
Classification Train Epoch: 20 [6400/48000 (13%)]	Loss: 0.004114, KL fake Loss: 7.429123
Classification Train Epoch: 20 [12800/48000 (27%)]	Loss: 0.009987, KL fake Loss: 7.157975
Classification Train Epoch: 20 [19200/48000 (40%)]	Loss: 0.013025, KL fake Loss: 7.488052
Classification Train Epoch: 20 [25600/48000 (53%)]	Loss: 0.075771, KL fake Loss: 7.390622
Classification Train Epoch: 20 [32000/48000 (67%)]	Loss: 0.024295, KL fake Loss: 6.754093
Classification Train Epoch: 20 [38400/48000 (80%)]	Loss: 0.006800, KL fake Loss: 7.569916
Classification Train Epoch: 20 [44800/48000 (93%)]	Loss: 0.039897, KL fake Loss: 7.310348

Test set: Average loss: 0.3271, Accuracy: 7366/8000 (92%)

Classification Train Epoch: 21 [0/48000 (0%)]	Loss: 0.009560, KL fake Loss: 7.243668
Classification Train Epoch: 21 [6400/48000 (13%)]	Loss: 0.030400, KL fake Loss: 6.737086
Classification Train Epoch: 21 [12800/48000 (27%)]	Loss: 0.038412, KL fake Loss: 7.929675
Classification Train Epoch: 21 [19200/48000 (40%)]	Loss: 0.062570, KL fake Loss: 7.453944
Classification Train Epoch: 21 [25600/48000 (53%)]	Loss: 0.025498, KL fake Loss: 6.907094
Classification Train Epoch: 21 [32000/48000 (67%)]	Loss: 0.043710, KL fake Loss: 7.469588
Classification Train Epoch: 21 [38400/48000 (80%)]	Loss: 0.017566, KL fake Loss: 7.024342
Classification Train Epoch: 21 [44800/48000 (93%)]	Loss: 0.004679, KL fake Loss: 7.730324

Test set: Average loss: 0.3238, Accuracy: 7398/8000 (92%)

Classification Train Epoch: 22 [0/48000 (0%)]	Loss: 0.005462, KL fake Loss: 6.999725
 31%|███       | 31/100 [1:49:40<4:04:03, 212.22s/it] 22%|██▏       | 22/100 [59:59<3:32:41, 163.60s/it]Classification Train Epoch: 22 [6400/50000 (13%)]	Loss: 0.059122, KL fake Loss: 0.007879
Classification Train Epoch: 22 [12800/50000 (26%)]	Loss: 0.016593, KL fake Loss: 0.010911
Classification Train Epoch: 22 [19200/50000 (38%)]	Loss: 0.043109, KL fake Loss: 0.014579
Classification Train Epoch: 22 [25600/50000 (51%)]	Loss: 0.019642, KL fake Loss: 0.012708
Classification Train Epoch: 22 [32000/50000 (64%)]	Loss: 0.044048, KL fake Loss: 0.010868
Classification Train Epoch: 22 [38400/50000 (77%)]	Loss: 0.010424, KL fake Loss: 0.029373
Classification Train Epoch: 22 [44800/50000 (90%)]	Loss: 0.010377, KL fake Loss: 0.013515

Test set: Average loss: 2.1927, Accuracy: 3283/10000 (33%)

Classification Train Epoch: 23 [0/50000 (0%)]	Loss: 0.026754, KL fake Loss: 0.026531
Classification Train Epoch: 23 [6400/50000 (13%)]	Loss: 0.092070, KL fake Loss: 0.019306
Classification Train Epoch: 23 [12800/50000 (26%)]	Loss: 0.008570, KL fake Loss: 0.014391
Classification Train Epoch: 23 [19200/50000 (38%)]	Loss: 0.021208, KL fake Loss: 0.012623
Classification Train Epoch: 23 [25600/50000 (51%)]	Loss: 0.111746, KL fake Loss: 0.014728
Classification Train Epoch: 23 [32000/50000 (64%)]	Loss: 0.015674, KL fake Loss: 0.015415
Classification Train Epoch: 23 [38400/50000 (77%)]	Loss: 0.047789, KL fake Loss: 0.011144
Classification Train Epoch: 23 [44800/50000 (90%)]	Loss: 0.156557, KL fake Loss: 0.021948

Test set: Average loss: 2.0186, Accuracy: 3266/10000 (33%)

Classification Train Epoch: 24 [0/50000 (0%)]	Loss: 0.073440, KL fake Loss: 0.015398
Classification Train Epoch: 24 [6400/50000 (13%)]	Loss: 0.040302, KL fake Loss: 0.017704
Classification Train Epoch: 24 [12800/50000 (26%)]	Loss: 0.078107, KL fake Loss: 0.007898
Classification Train Epoch: 24 [19200/50000 (38%)]	Loss: 0.015011, KL fake Loss: 0.012308
Classification Train Epoch: 24 [25600/50000 (51%)]	Loss: 0.082534, KL fake Loss: 0.010304
Classification Train Epoch: 24 [32000/50000 (64%)]	Loss: 0.020545, KL fake Loss: 0.024481
Classification Train Epoch: 24 [38400/50000 (77%)]	Loss: 0.037092, KL fake Loss: 0.008315
Classification Train Epoch: 24 [44800/50000 (90%)]	Loss: 0.122024, KL fake Loss: 0.022596

Test set: Average loss: 2.6998, Accuracy: 2362/10000 (24%)

Classification Train Epoch: 25 [0/50000 (0%)]	Loss: 0.011176, KL fake Loss: 0.016036
Classification Train Epoch: 25 [6400/50000 (13%)]	Loss: 0.074920, KL fake Loss: 0.010419
Classification Train Epoch: 25 [12800/50000 (26%)]	Loss: 0.033940, KL fake Loss: 0.006489
Classification Train Epoch: 25 [19200/50000 (38%)]	Loss: 0.060606, KL fake Loss: 0.018535
Classification Train Epoch: 25 [25600/50000 (51%)]	Loss: 0.012503, KL fake Loss: 0.016920
Classification Train Epoch: 25 [32000/50000 (64%)]	Loss: 0.050701, KL fake Loss: 0.022004
Classification Train Epoch: 25 [38400/50000 (77%)]	Loss: 0.049616, KL fake Loss: 0.009388
Classification Train Epoch: 25 [44800/50000 (90%)]	Loss: 0.015988, KL fake Loss: 0.015975

Test set: Average loss: 3.0048, Accuracy: 2713/10000 (27%)

Classification Train Epoch: 26 [0/50000 (0%)]	Loss: 0.043663, KL fake Loss: 0.017236
Classification Train Epoch: 26 [6400/50000 (13%)]	Loss: 0.013331, KL fake Loss: 0.010997
Classification Train Epoch: 26 [12800/50000 (26%)]	Loss: 0.010651, KL fake Loss: 0.019883
Classification Train Epoch: 26 [19200/50000 (38%)]	Loss: 0.030822, KL fake Loss: 0.019278
Classification Train Epoch: 26 [25600/50000 (51%)]	Loss: 0.132877, KL fake Loss: 0.006033
Classification Train Epoch: 26 [32000/50000 (64%)]	Loss: 0.153687, KL fake Loss: 0.013509
Classification Train Epoch: 26 [38400/50000 (77%)]	Loss: 0.060972, KL fake Loss: 0.018798
Classification Train Epoch: 26 [44800/50000 (90%)]	Loss: 0.037859, KL fake Loss: 0.016293

Test set: Average loss: 2.6764, Accuracy: 2504/10000 (25%)

Classification Train Epoch: 27 [0/50000 (0%)]	Loss: 0.056126, KL fake Loss: 0.028571
Classification Train Epoch: 27 [6400/50000 (13%)]	Loss: 0.044877, KL fake Loss: 0.005932
Classification Train Epoch: 27 [12800/50000 (26%)]	Loss: 0.136370, KL fake Loss: 0.010490
Classification Train Epoch: 27 [19200/50000 (38%)]	Loss: 0.025780, KL fake Loss: 0.028208
Classification Train Epoch: 27 [25600/50000 (51%)]	Loss: 0.057868, KL fake Loss: 0.024943
Classification Train Epoch: 27 [32000/50000 (64%)]	Loss: 0.080385, KL fake Loss: 0.005965
Classification Train Epoch: 27 [38400/50000 (77%)]	Loss: 0.023471, KL fake Loss: 0.014971
Classification Train Epoch: 27 [44800/50000 (90%)]	Loss: 0.020332, KL fake Loss: 0.008057

Test set: Average loss: 2.2011, Accuracy: 2904/10000 (29%)

Classification Train Epoch: 28 [0/50000 (0%)]	Loss: 0.013604, KL fake Loss: 0.017607
Classification Train Epoch: 28 [6400/50000 (13%)]	Loss: 0.122392, KL fake Loss: 0.005253
Classification Train Epoch: 28 [12800/50000 (26%)]	Loss: 0.007049, KL fake Loss: 0.006523
Classification Train Epoch: 28 [19200/50000 (38%)]	Loss: 0.138648, KL fake Loss: 0.013646
Classification Train Epoch: 28 [25600/50000 (51%)]	Loss: 0.013411, KL fake Loss: 0.006710
Classification Train Epoch: 28 [32000/50000 (64%)]	Loss: 0.064914, KL fake Loss: 0.006021
Classification Train Epoch: 28 [38400/50000 (77%)]	Loss: 0.065002, KL fake Loss: 0.007575
Classification Train Epoch: 28 [44800/50000 (90%)]	Loss: 0.024070, KL fake Loss: 0.016946

Test set: Average loss: 2.3962, Accuracy: 2770/10000 (28%)

Classification Train Epoch: 29 [0/50000 (0%)]	Loss: 0.073640, KL fake Loss: 0.015466
Classification Train Epoch: 29 [6400/50000 (13%)]	Loss: 0.010866, KL fake Loss: 0.005567
Classification Train Epoch: 29 [12800/50000 (26%)]	Loss: 0.018944, KL fake Loss: 0.014177
Classification Train Epoch: 29 [19200/50000 (38%)]	Loss: 0.010028, KL fake Loss: 0.013488
Classification Train Epoch: 29 [25600/50000 (51%)]	Loss: 0.010574, KL fake Loss: 0.008498
Classification Train Epoch: 29 [32000/50000 (64%)]	Loss: 0.077551, KL fake Loss: 0.020689
Classification Train Epoch: 29 [38400/50000 (77%)]	Loss: 0.057300, KL fake Loss: 0.010237
Classification Train Epoch: 29 [44800/50000 (90%)]	Loss: 0.043589, KL fake Loss: 0.009027

Test set: Average loss: 3.2194, Accuracy: 3488/10000 (35%)

Classification Train Epoch: 30 [0/50000 (0%)]	Loss: 0.010808, KL fake Loss: 0.016757
Classification Train Epoch: 30 [6400/50000 (13%)]	Loss: 0.245578, KL fake Loss: 0.018643
Classification Train Epoch: 30 [12800/50000 (26%)]	Loss: 0.054345, KL fake Loss: 0.013037
Classification Train Epoch: 30 [19200/50000 (38%)]	Loss: 0.007375, KL fake Loss: 0.023855
Classification Train Epoch: 30 [25600/50000 (51%)]	Loss: 0.016977, KL fake Loss: 0.012272
Classification Train Epoch: 30 [32000/50000 (64%)]	Loss: 0.051948, KL fake Loss: 0.005084
Classification Train Epoch: 30 [38400/50000 (77%)]	Loss: 0.046057, KL fake Loss: 0.009659
Classification Train Epoch: 30 [44800/50000 (90%)]	Loss: 0.103799, KL fake Loss: 0.017131

Test set: Average loss: 2.2344, Accuracy: 2794/10000 (28%)

Classification Train Epoch: 31 [0/50000 (0%)]	Loss: 0.004634, KL fake Loss: 0.009267
Classification Train Epoch: 31 [6400/50000 (13%)]	Loss: 0.010150, KL fake Loss: 0.010479
Classification Train Epoch: 31 [12800/50000 (26%)]	Loss: 0.004656, KL fake Loss: 0.013047
Classification Train Epoch: 31 [19200/50000 (38%)]	Loss: 0.142937, KL fake Loss: 0.018890
Classification Train Epoch: 31 [25600/50000 (51%)]	Loss: 0.019433, KL fake Loss: 0.005207
Classification Train Epoch: 31 [32000/50000 (64%)]	Loss: 0.031893, KL fake Loss: 0.016516
Classification Train Epoch: 31 [38400/50000 (77%)]	Loss: 0.060763, KL fake Loss: 0.021261
Classification Train Epoch: 31 [44800/50000 (90%)]	Loss: 0.020980, KL fake Loss: 0.012300

Test set: Average loss: 2.0419, Accuracy: 3184/10000 (32%)

Classification Train Epoch: 32 [0/50000 (0%)]	Loss: 0.094798, KL fake Loss: 0.017790
Classification Train Epoch: 32 [6400/50000 (13%)]	Loss: 0.055447, KL fake Loss: 0.008000
Classification Train Epoch: 32 [12800/50000 (26%)]	Loss: 0.014325, KL fake Loss: 0.007497
Classification Train Epoch: 32 [19200/50000 (38%)]	Loss: 0.101985, KL fake Loss: 0.016557
Classification Train Epoch: 32 [25600/50000 (51%)]	Loss: 0.006884, KL fake Loss: 0.010393
 32%|███▏      | 32/100 [1:53:12<4:00:30, 212.22s/it] 23%|██▎       | 23/100 [1:02:42<3:29:56, 163.59s/it] 33%|███▎      | 33/100 [1:56:44<3:56:58, 212.22s/it] 24%|██▍       | 24/100 [1:05:26<3:27:13, 163.59s/it] 25%|██▌       | 25/100 [1:08:09<3:24:29, 163.59s/it] 34%|███▍      | 34/100 [2:00:16<3:53:26, 212.21s/it] 26%|██▌       | 26/100 [1:10:53<3:21:45, 163.59s/it] 35%|███▌      | 35/100 [2:03:48<3:49:53, 212.21s/it] 27%|██▋       | 27/100 [1:13:36<3:19:02, 163.59s/it] 36%|███▌      | 36/100 [2:07:21<3:46:21, 212.22s/it] 28%|██▊       | 28/100 [1:16:20<3:16:18, 163.59s/it] 29%|██▉       | 29/100 [1:19:04<3:13:34, 163.59s/it] 37%|███▋      | 37/100 [2:10:53<3:42:49, 212.22s/it] 30%|███       | 30/100 [1:21:47<3:10:51, 163.59s/it] 38%|███▊      | 38/100 [2:14:25<3:39:17, 212.22s/it] 31%|███       | 31/100 [1:24:31<3:08:07, 163.59s/it] 39%|███▉      | 39/100 [2:17:57<3:35:45, 212.23s/it]Classification Train Epoch: 22 [6400/48000 (13%)]	Loss: 0.074819, KL fake Loss: 7.261186
Classification Train Epoch: 22 [12800/48000 (27%)]	Loss: 0.072604, KL fake Loss: 6.960002
Classification Train Epoch: 22 [19200/48000 (40%)]	Loss: 0.013618, KL fake Loss: 7.655914
Classification Train Epoch: 22 [25600/48000 (53%)]	Loss: 0.053550, KL fake Loss: 7.606717
Classification Train Epoch: 22 [32000/48000 (67%)]	Loss: 0.112288, KL fake Loss: 7.259984
Classification Train Epoch: 22 [38400/48000 (80%)]	Loss: 0.075058, KL fake Loss: 7.576579
Classification Train Epoch: 22 [44800/48000 (93%)]	Loss: 0.068299, KL fake Loss: 6.928946

Test set: Average loss: 0.3209, Accuracy: 7414/8000 (93%)

Classification Train Epoch: 23 [0/48000 (0%)]	Loss: 0.013794, KL fake Loss: 7.792893
Classification Train Epoch: 23 [6400/48000 (13%)]	Loss: 0.011853, KL fake Loss: 7.575777
Classification Train Epoch: 23 [12800/48000 (27%)]	Loss: 0.008091, KL fake Loss: 6.876118
Classification Train Epoch: 23 [19200/48000 (40%)]	Loss: 0.036429, KL fake Loss: 7.611551
Classification Train Epoch: 23 [25600/48000 (53%)]	Loss: 0.005967, KL fake Loss: 6.950741
Classification Train Epoch: 23 [32000/48000 (67%)]	Loss: 0.068807, KL fake Loss: 7.245286
Classification Train Epoch: 23 [38400/48000 (80%)]	Loss: 0.058681, KL fake Loss: 7.074072
Classification Train Epoch: 23 [44800/48000 (93%)]	Loss: 0.032084, KL fake Loss: 6.932616

Test set: Average loss: 0.3577, Accuracy: 7375/8000 (92%)

Classification Train Epoch: 24 [0/48000 (0%)]	Loss: 0.056297, KL fake Loss: 6.947131
Classification Train Epoch: 24 [6400/48000 (13%)]	Loss: 0.022375, KL fake Loss: 6.986901
Classification Train Epoch: 24 [12800/48000 (27%)]	Loss: 0.027207, KL fake Loss: 7.981112
Classification Train Epoch: 24 [19200/48000 (40%)]	Loss: 0.028775, KL fake Loss: 7.069019
Classification Train Epoch: 24 [25600/48000 (53%)]	Loss: 0.018471, KL fake Loss: 7.251715
Classification Train Epoch: 24 [32000/48000 (67%)]	Loss: 0.084704, KL fake Loss: 7.397136
Classification Train Epoch: 24 [38400/48000 (80%)]	Loss: 0.081677, KL fake Loss: 7.634434
Classification Train Epoch: 24 [44800/48000 (93%)]	Loss: 0.041393, KL fake Loss: 7.596530

Test set: Average loss: 0.3053, Accuracy: 7436/8000 (93%)

Classification Train Epoch: 25 [0/48000 (0%)]	Loss: 0.004834, KL fake Loss: 7.483318
Classification Train Epoch: 25 [6400/48000 (13%)]	Loss: 0.021595, KL fake Loss: 7.204426
Classification Train Epoch: 25 [12800/48000 (27%)]	Loss: 0.024390, KL fake Loss: 6.398804
Classification Train Epoch: 25 [19200/48000 (40%)]	Loss: 0.013319, KL fake Loss: 7.291982
Classification Train Epoch: 25 [25600/48000 (53%)]	Loss: 0.004288, KL fake Loss: 8.310438
Classification Train Epoch: 25 [32000/48000 (67%)]	Loss: 0.014244, KL fake Loss: 6.934627
Classification Train Epoch: 25 [38400/48000 (80%)]	Loss: 0.004738, KL fake Loss: 7.406069
Classification Train Epoch: 25 [44800/48000 (93%)]	Loss: 0.011968, KL fake Loss: 7.139097

Test set: Average loss: 0.3271, Accuracy: 7405/8000 (93%)

Classification Train Epoch: 26 [0/48000 (0%)]	Loss: 0.020061, KL fake Loss: 7.211157
Classification Train Epoch: 26 [6400/48000 (13%)]	Loss: 0.010883, KL fake Loss: 7.736678
Classification Train Epoch: 26 [12800/48000 (27%)]	Loss: 0.003172, KL fake Loss: 7.722345
Classification Train Epoch: 26 [19200/48000 (40%)]	Loss: 0.043911, KL fake Loss: 7.340430
Classification Train Epoch: 26 [25600/48000 (53%)]	Loss: 0.049934, KL fake Loss: 8.056471
Classification Train Epoch: 26 [32000/48000 (67%)]	Loss: 0.039827, KL fake Loss: 6.886519
Classification Train Epoch: 26 [38400/48000 (80%)]	Loss: 0.032536, KL fake Loss: 7.593212
Classification Train Epoch: 26 [44800/48000 (93%)]	Loss: 0.017482, KL fake Loss: 7.622677

Test set: Average loss: 0.3174, Accuracy: 7417/8000 (93%)

Classification Train Epoch: 27 [0/48000 (0%)]	Loss: 0.005039, KL fake Loss: 6.721786
Classification Train Epoch: 27 [6400/48000 (13%)]	Loss: 0.009657, KL fake Loss: 7.759183
Classification Train Epoch: 27 [12800/48000 (27%)]	Loss: 0.008709, KL fake Loss: 7.326456
Classification Train Epoch: 27 [19200/48000 (40%)]	Loss: 0.016482, KL fake Loss: 7.692933
Classification Train Epoch: 27 [25600/48000 (53%)]	Loss: 0.002863, KL fake Loss: 7.421565
Classification Train Epoch: 27 [32000/48000 (67%)]	Loss: 0.023277, KL fake Loss: 7.963960
Classification Train Epoch: 27 [38400/48000 (80%)]	Loss: 0.014507, KL fake Loss: 7.261745
Classification Train Epoch: 27 [44800/48000 (93%)]	Loss: 0.026968, KL fake Loss: 7.680142

Test set: Average loss: 0.3632, Accuracy: 7373/8000 (92%)

Classification Train Epoch: 28 [0/48000 (0%)]	Loss: 0.008816, KL fake Loss: 7.757343
Classification Train Epoch: 28 [6400/48000 (13%)]	Loss: 0.015236, KL fake Loss: 7.405360
Classification Train Epoch: 28 [12800/48000 (27%)]	Loss: 0.011357, KL fake Loss: 7.950626
Classification Train Epoch: 28 [19200/48000 (40%)]	Loss: 0.036599, KL fake Loss: 7.345437
Classification Train Epoch: 28 [25600/48000 (53%)]	Loss: 0.005371, KL fake Loss: 7.627120
Classification Train Epoch: 28 [32000/48000 (67%)]	Loss: 0.033256, KL fake Loss: 7.465103
Classification Train Epoch: 28 [38400/48000 (80%)]	Loss: 0.053865, KL fake Loss: 7.470652
Classification Train Epoch: 28 [44800/48000 (93%)]	Loss: 0.026162, KL fake Loss: 6.838239

Test set: Average loss: 0.3347, Accuracy: 7422/8000 (93%)

Classification Train Epoch: 29 [0/48000 (0%)]	Loss: 0.004727, KL fake Loss: 7.450818
Classification Train Epoch: 29 [6400/48000 (13%)]	Loss: 0.005240, KL fake Loss: 6.954442
Classification Train Epoch: 29 [12800/48000 (27%)]	Loss: 0.018909, KL fake Loss: 7.418345
Classification Train Epoch: 29 [19200/48000 (40%)]	Loss: 0.003695, KL fake Loss: 7.612665
Classification Train Epoch: 29 [25600/48000 (53%)]	Loss: 0.055276, KL fake Loss: 7.263842
Classification Train Epoch: 29 [32000/48000 (67%)]	Loss: 0.007963, KL fake Loss: 7.605268
Classification Train Epoch: 29 [38400/48000 (80%)]	Loss: 0.003810, KL fake Loss: 7.540555
Classification Train Epoch: 29 [44800/48000 (93%)]	Loss: 0.025875, KL fake Loss: 7.218158

Test set: Average loss: 0.3836, Accuracy: 7362/8000 (92%)

Classification Train Epoch: 30 [0/48000 (0%)]	Loss: 0.092102, KL fake Loss: 8.237277
Classification Train Epoch: 30 [6400/48000 (13%)]	Loss: 0.023713, KL fake Loss: 7.550103
Classification Train Epoch: 30 [12800/48000 (27%)]	Loss: 0.005415, KL fake Loss: 7.388480
Classification Train Epoch: 30 [19200/48000 (40%)]	Loss: 0.045734, KL fake Loss: 7.676534
Classification Train Epoch: 30 [25600/48000 (53%)]	Loss: 0.002009, KL fake Loss: 7.699098
Classification Train Epoch: 30 [32000/48000 (67%)]	Loss: 0.005704, KL fake Loss: 7.487596
Classification Train Epoch: 30 [38400/48000 (80%)]	Loss: 0.011553, KL fake Loss: 7.208930
Classification Train Epoch: 30 [44800/48000 (93%)]	Loss: 0.016876, KL fake Loss: 7.459477

Test set: Average loss: 0.3513, Accuracy: 7362/8000 (92%)

Classification Train Epoch: 31 [0/48000 (0%)]	Loss: 0.016275, KL fake Loss: 7.335032
Classification Train Epoch: 31 [6400/48000 (13%)]	Loss: 0.011968, KL fake Loss: 7.849485
Classification Train Epoch: 31 [12800/48000 (27%)]	Loss: 0.001473, KL fake Loss: 7.560694
Classification Train Epoch: 31 [19200/48000 (40%)]	Loss: 0.008186, KL fake Loss: 7.216271
Classification Train Epoch: 31 [25600/48000 (53%)]	Loss: 0.004373, KL fake Loss: 7.142106
Classification Train Epoch: 31 [32000/48000 (67%)]	Loss: 0.003425, KL fake Loss: 7.354597
Classification Train Epoch: 31 [38400/48000 (80%)]	Loss: 0.010792, KL fake Loss: 7.843150
Classification Train Epoch: 31 [44800/48000 (93%)]	Loss: 0.020580, KL fake Loss: 7.400200

Test set: Average loss: 0.3480, Accuracy: 7365/8000 (92%)

Classification Train Epoch: 32 [0/48000 (0%)]	Loss: 0.004352, KL fake Loss: 7.744890
Classification Train Epoch: 32 [6400/48000 (13%)]	Loss: 0.005695, KL fake Loss: 7.480346
Classification Train Epoch: 32 [12800/48000 (27%)]	Loss: 0.005825, KL fake Loss: 7.652772
Classification Train Epoch: 32 [19200/48000 (40%)]	Loss: 0.002468, KL fake Loss: 7.245917
Classification Train Epoch: 32 [25600/48000 (53%)]	Loss: 0.013023, KL fake Loss: 7.438377
Classification Train Epoch: 32 [32000/48000 (67%)]	Loss: 0.012988, KL fake Loss: 7.270454
 32%|███▏      | 32/100 [1:27:14<3:05:23, 163.58s/it] 33%|███▎      | 33/100 [1:29:58<3:02:40, 163.59s/it] 40%|████      | 40/100 [2:21:30<3:32:15, 212.26s/it] 34%|███▍      | 34/100 [1:32:42<2:59:56, 163.58s/it] 41%|████      | 41/100 [2:25:02<3:28:42, 212.25s/it] 35%|███▌      | 35/100 [1:35:25<2:57:12, 163.58s/it] 42%|████▏     | 42/100 [2:28:34<3:25:10, 212.24s/it]Classification Train Epoch: 32 [32000/50000 (64%)]	Loss: 0.024679, KL fake Loss: 0.013850
Classification Train Epoch: 32 [38400/50000 (77%)]	Loss: 0.091308, KL fake Loss: 0.005818
Classification Train Epoch: 32 [44800/50000 (90%)]	Loss: 0.154839, KL fake Loss: 0.009113

Test set: Average loss: 2.2706, Accuracy: 2624/10000 (26%)

Classification Train Epoch: 33 [0/50000 (0%)]	Loss: 0.080044, KL fake Loss: 0.021919
Classification Train Epoch: 33 [6400/50000 (13%)]	Loss: 0.061210, KL fake Loss: 0.006730
Classification Train Epoch: 33 [12800/50000 (26%)]	Loss: 0.038553, KL fake Loss: 0.018667
Classification Train Epoch: 33 [19200/50000 (38%)]	Loss: 0.003940, KL fake Loss: 0.015842
Classification Train Epoch: 33 [25600/50000 (51%)]	Loss: 0.004774, KL fake Loss: 0.013366
Classification Train Epoch: 33 [32000/50000 (64%)]	Loss: 0.035584, KL fake Loss: 0.016029
Classification Train Epoch: 33 [38400/50000 (77%)]	Loss: 0.009229, KL fake Loss: 0.012540
Classification Train Epoch: 33 [44800/50000 (90%)]	Loss: 0.047780, KL fake Loss: 0.007604

Test set: Average loss: 2.5328, Accuracy: 2484/10000 (25%)

Classification Train Epoch: 34 [0/50000 (0%)]	Loss: 0.030297, KL fake Loss: 0.014000
Classification Train Epoch: 34 [6400/50000 (13%)]	Loss: 0.049686, KL fake Loss: 0.007250
Classification Train Epoch: 34 [12800/50000 (26%)]	Loss: 0.003168, KL fake Loss: 0.015840
Classification Train Epoch: 34 [19200/50000 (38%)]	Loss: 0.023644, KL fake Loss: 0.009950
Classification Train Epoch: 34 [25600/50000 (51%)]	Loss: 0.027648, KL fake Loss: 0.003626
Classification Train Epoch: 34 [32000/50000 (64%)]	Loss: 0.012800, KL fake Loss: 0.009814
Classification Train Epoch: 34 [38400/50000 (77%)]	Loss: 0.111176, KL fake Loss: 0.014962
Classification Train Epoch: 34 [44800/50000 (90%)]	Loss: 0.005267, KL fake Loss: 0.016134

Test set: Average loss: 2.1656, Accuracy: 2774/10000 (28%)

Classification Train Epoch: 35 [0/50000 (0%)]	Loss: 0.041815, KL fake Loss: 0.017986
Classification Train Epoch: 35 [6400/50000 (13%)]	Loss: 0.029392, KL fake Loss: 0.010073
Classification Train Epoch: 35 [12800/50000 (26%)]	Loss: 0.003924, KL fake Loss: 0.006554
Classification Train Epoch: 35 [19200/50000 (38%)]	Loss: 0.007096, KL fake Loss: 0.012036
Classification Train Epoch: 35 [25600/50000 (51%)]	Loss: 0.053484, KL fake Loss: 0.011297
Classification Train Epoch: 35 [32000/50000 (64%)]	Loss: 0.119355, KL fake Loss: 0.025966
Classification Train Epoch: 35 [38400/50000 (77%)]	Loss: 0.067618, KL fake Loss: 0.014566
Classification Train Epoch: 35 [44800/50000 (90%)]	Loss: 0.030195, KL fake Loss: 0.020958

Test set: Average loss: 2.2739, Accuracy: 2792/10000 (28%)

Classification Train Epoch: 36 [0/50000 (0%)]	Loss: 0.060573, KL fake Loss: 0.010015
Classification Train Epoch: 36 [6400/50000 (13%)]	Loss: 0.104646, KL fake Loss: 0.005010
Classification Train Epoch: 36 [12800/50000 (26%)]	Loss: 0.031694, KL fake Loss: 0.017375
Classification Train Epoch: 36 [19200/50000 (38%)]	Loss: 0.004099, KL fake Loss: 0.005323
Classification Train Epoch: 36 [25600/50000 (51%)]	Loss: 0.165409, KL fake Loss: 0.012415
Classification Train Epoch: 36 [32000/50000 (64%)]	Loss: 0.021716, KL fake Loss: 0.022786
Classification Train Epoch: 36 [38400/50000 (77%)]	Loss: 0.065827, KL fake Loss: 0.022556
Classification Train Epoch: 36 [44800/50000 (90%)]	Loss: 0.024035, KL fake Loss: 0.015838

Test set: Average loss: 2.6011, Accuracy: 2119/10000 (21%)

Classification Train Epoch: 37 [0/50000 (0%)]	Loss: 0.001923, KL fake Loss: 0.029023
Classification Train Epoch: 37 [6400/50000 (13%)]	Loss: 0.012297, KL fake Loss: 0.009895
Classification Train Epoch: 37 [12800/50000 (26%)]	Loss: 0.136666, KL fake Loss: 0.006087
Classification Train Epoch: 37 [19200/50000 (38%)]	Loss: 0.009040, KL fake Loss: 0.005281
Classification Train Epoch: 37 [25600/50000 (51%)]	Loss: 0.043292, KL fake Loss: 0.008172
Classification Train Epoch: 37 [32000/50000 (64%)]	Loss: 0.005140, KL fake Loss: 0.011927
Classification Train Epoch: 37 [38400/50000 (77%)]	Loss: 0.003628, KL fake Loss: 0.017454
Classification Train Epoch: 37 [44800/50000 (90%)]	Loss: 0.019573, KL fake Loss: 0.004286

Test set: Average loss: 2.3956, Accuracy: 2806/10000 (28%)

Classification Train Epoch: 38 [0/50000 (0%)]	Loss: 0.026382, KL fake Loss: 0.012080
Classification Train Epoch: 38 [6400/50000 (13%)]	Loss: 0.015300, KL fake Loss: 0.006230
Classification Train Epoch: 38 [12800/50000 (26%)]	Loss: 0.132361, KL fake Loss: 0.014744
Classification Train Epoch: 38 [19200/50000 (38%)]	Loss: 0.030167, KL fake Loss: 0.016200
Classification Train Epoch: 38 [25600/50000 (51%)]	Loss: 0.037276, KL fake Loss: 0.004817
Classification Train Epoch: 38 [32000/50000 (64%)]	Loss: 0.008725, KL fake Loss: 0.007758
Classification Train Epoch: 38 [38400/50000 (77%)]	Loss: 0.051911, KL fake Loss: 0.034046
Classification Train Epoch: 38 [44800/50000 (90%)]	Loss: 0.138823, KL fake Loss: 0.019720

Test set: Average loss: 2.3531, Accuracy: 2616/10000 (26%)

Classification Train Epoch: 39 [0/50000 (0%)]	Loss: 0.054008, KL fake Loss: 0.011531
Classification Train Epoch: 39 [6400/50000 (13%)]	Loss: 0.000848, KL fake Loss: 0.009078
Classification Train Epoch: 39 [12800/50000 (26%)]	Loss: 0.004957, KL fake Loss: 0.005982
Classification Train Epoch: 39 [19200/50000 (38%)]	Loss: 0.114171, KL fake Loss: 0.015183
Classification Train Epoch: 39 [25600/50000 (51%)]	Loss: 0.002070, KL fake Loss: 0.011900
Classification Train Epoch: 39 [32000/50000 (64%)]	Loss: 0.023938, KL fake Loss: 0.007467
Classification Train Epoch: 39 [38400/50000 (77%)]	Loss: 0.127137, KL fake Loss: 0.005061
Classification Train Epoch: 39 [44800/50000 (90%)]	Loss: 0.070764, KL fake Loss: 0.012646

Test set: Average loss: 2.1816, Accuracy: 3740/10000 (37%)

Classification Train Epoch: 40 [0/50000 (0%)]	Loss: 0.036897, KL fake Loss: 0.009113
Classification Train Epoch: 40 [6400/50000 (13%)]	Loss: 0.080659, KL fake Loss: 0.010913
Classification Train Epoch: 40 [12800/50000 (26%)]	Loss: 0.048726, KL fake Loss: 0.017732
Classification Train Epoch: 40 [19200/50000 (38%)]	Loss: 0.036544, KL fake Loss: 0.004482
Classification Train Epoch: 40 [25600/50000 (51%)]	Loss: 0.002757, KL fake Loss: 0.008141
Classification Train Epoch: 40 [32000/50000 (64%)]	Loss: 0.004317, KL fake Loss: 0.018597
Classification Train Epoch: 40 [38400/50000 (77%)]	Loss: 0.009667, KL fake Loss: 0.016748
Classification Train Epoch: 40 [44800/50000 (90%)]	Loss: 0.043062, KL fake Loss: 0.011882

Test set: Average loss: 2.5360, Accuracy: 2877/10000 (29%)

Classification Train Epoch: 41 [0/50000 (0%)]	Loss: 0.030584, KL fake Loss: 0.012922
Classification Train Epoch: 41 [6400/50000 (13%)]	Loss: 0.006836, KL fake Loss: 0.011330
Classification Train Epoch: 41 [12800/50000 (26%)]	Loss: 0.036951, KL fake Loss: 0.007242
Classification Train Epoch: 41 [19200/50000 (38%)]	Loss: 0.003390, KL fake Loss: 0.004964
Classification Train Epoch: 41 [25600/50000 (51%)]	Loss: 0.010207, KL fake Loss: 0.018651
Classification Train Epoch: 41 [32000/50000 (64%)]	Loss: 0.000850, KL fake Loss: 0.016361
Classification Train Epoch: 41 [38400/50000 (77%)]	Loss: 0.039111, KL fake Loss: 0.009908
Classification Train Epoch: 41 [44800/50000 (90%)]	Loss: 0.037018, KL fake Loss: 0.011285

Test set: Average loss: 2.3025, Accuracy: 2287/10000 (23%)

Classification Train Epoch: 42 [0/50000 (0%)]	Loss: 0.124632, KL fake Loss: 0.013523
Classification Train Epoch: 42 [6400/50000 (13%)]	Loss: 0.005110, KL fake Loss: 0.009608
Classification Train Epoch: 42 [12800/50000 (26%)]	Loss: 0.000765, KL fake Loss: 0.009752
Classification Train Epoch: 42 [19200/50000 (38%)]	Loss: 0.005886, KL fake Loss: 0.004945
Classification Train Epoch: 42 [25600/50000 (51%)]	Loss: 0.001129, KL fake Loss: 0.007740
Classification Train Epoch: 42 [32000/50000 (64%)]	Loss: 0.048983, KL fake Loss: 0.010565
Classification Train Epoch: 42 [38400/50000 (77%)]	Loss: 0.051032, KL fake Loss: 0.012410
Classification Train Epoch: 42 [44800/50000 (90%)]	Loss: 0.047403, KL fake Loss: 0.012030

Test set: Average loss: 2.1464, Accuracy: 2730/10000 (27%)

Classification Train Epoch: 43 [0/50000 (0%)]	Loss: 0.004406, KL fake Loss: 0.010959
 36%|███▌      | 36/100 [1:38:09<2:54:29, 163.58s/it] 43%|████▎     | 43/100 [2:32:06<3:21:37, 212.24s/it] 37%|███▋      | 37/100 [1:40:52<2:51:45, 163.58s/it] 38%|███▊      | 38/100 [1:43:36<2:49:01, 163.57s/it] 44%|████▍     | 44/100 [2:35:39<3:18:05, 212.24s/it] 39%|███▉      | 39/100 [1:46:19<2:46:18, 163.58s/it] 45%|████▌     | 45/100 [2:39:11<3:14:33, 212.24s/it] 40%|████      | 40/100 [1:49:03<2:43:36, 163.61s/it] 46%|████▌     | 46/100 [2:42:43<3:11:01, 212.24s/it] 41%|████      | 41/100 [1:51:47<2:40:52, 163.60s/it] 42%|████▏     | 42/100 [1:54:30<2:38:08, 163.59s/it] 47%|████▋     | 47/100 [2:46:15<3:07:28, 212.24s/it]Classification Train Epoch: 32 [38400/48000 (80%)]	Loss: 0.004315, KL fake Loss: 7.633996
Classification Train Epoch: 32 [44800/48000 (93%)]	Loss: 0.052824, KL fake Loss: 7.704761

Test set: Average loss: 0.3519, Accuracy: 7399/8000 (92%)

Classification Train Epoch: 33 [0/48000 (0%)]	Loss: 0.036953, KL fake Loss: 7.724569
Classification Train Epoch: 33 [6400/48000 (13%)]	Loss: 0.201010, KL fake Loss: 7.573421
Classification Train Epoch: 33 [12800/48000 (27%)]	Loss: 0.004226, KL fake Loss: 7.669556
Classification Train Epoch: 33 [19200/48000 (40%)]	Loss: 0.019036, KL fake Loss: 7.293540
Classification Train Epoch: 33 [25600/48000 (53%)]	Loss: 0.019276, KL fake Loss: 7.526673
Classification Train Epoch: 33 [32000/48000 (67%)]	Loss: 0.004462, KL fake Loss: 7.339836
Classification Train Epoch: 33 [38400/48000 (80%)]	Loss: 0.020613, KL fake Loss: 7.258388
Classification Train Epoch: 33 [44800/48000 (93%)]	Loss: 0.000779, KL fake Loss: 6.960445

Test set: Average loss: 0.3754, Accuracy: 7369/8000 (92%)

Classification Train Epoch: 34 [0/48000 (0%)]	Loss: 0.032218, KL fake Loss: 6.996252
Classification Train Epoch: 34 [6400/48000 (13%)]	Loss: 0.013086, KL fake Loss: 6.920880
Classification Train Epoch: 34 [12800/48000 (27%)]	Loss: 0.036504, KL fake Loss: 7.210224
Classification Train Epoch: 34 [19200/48000 (40%)]	Loss: 0.002198, KL fake Loss: 7.549747
Classification Train Epoch: 34 [25600/48000 (53%)]	Loss: 0.042072, KL fake Loss: 7.252707
Classification Train Epoch: 34 [32000/48000 (67%)]	Loss: 0.023382, KL fake Loss: 6.808560
Classification Train Epoch: 34 [38400/48000 (80%)]	Loss: 0.001634, KL fake Loss: 7.178863
Classification Train Epoch: 34 [44800/48000 (93%)]	Loss: 0.013029, KL fake Loss: 7.123954

Test set: Average loss: 0.3906, Accuracy: 7364/8000 (92%)

Classification Train Epoch: 35 [0/48000 (0%)]	Loss: 0.037288, KL fake Loss: 7.016958
Classification Train Epoch: 35 [6400/48000 (13%)]	Loss: 0.002140, KL fake Loss: 7.549364
Classification Train Epoch: 35 [12800/48000 (27%)]	Loss: 0.010760, KL fake Loss: 7.228585
Classification Train Epoch: 35 [19200/48000 (40%)]	Loss: 0.004525, KL fake Loss: 6.882189
Classification Train Epoch: 35 [25600/48000 (53%)]	Loss: 0.004576, KL fake Loss: 7.355220
Classification Train Epoch: 35 [32000/48000 (67%)]	Loss: 0.023697, KL fake Loss: 6.687786
Classification Train Epoch: 35 [38400/48000 (80%)]	Loss: 0.036349, KL fake Loss: 6.787525
Classification Train Epoch: 35 [44800/48000 (93%)]	Loss: 0.075195, KL fake Loss: 7.622696

Test set: Average loss: 0.3745, Accuracy: 7405/8000 (93%)

Classification Train Epoch: 36 [0/48000 (0%)]	Loss: 0.001373, KL fake Loss: 7.580359
Classification Train Epoch: 36 [6400/48000 (13%)]	Loss: 0.033417, KL fake Loss: 7.642792
Classification Train Epoch: 36 [12800/48000 (27%)]	Loss: 0.003069, KL fake Loss: 7.089706
Classification Train Epoch: 36 [19200/48000 (40%)]	Loss: 0.007501, KL fake Loss: 7.370201
Classification Train Epoch: 36 [25600/48000 (53%)]	Loss: 0.008155, KL fake Loss: 7.034537
Classification Train Epoch: 36 [32000/48000 (67%)]	Loss: 0.007411, KL fake Loss: 7.706453
Classification Train Epoch: 36 [38400/48000 (80%)]	Loss: 0.007342, KL fake Loss: 7.451683
Classification Train Epoch: 36 [44800/48000 (93%)]	Loss: 0.003497, KL fake Loss: 7.372261

Test set: Average loss: 0.3634, Accuracy: 7410/8000 (93%)

Classification Train Epoch: 37 [0/48000 (0%)]	Loss: 0.013097, KL fake Loss: 7.798960
Classification Train Epoch: 37 [6400/48000 (13%)]	Loss: 0.050453, KL fake Loss: 7.402836
Classification Train Epoch: 37 [12800/48000 (27%)]	Loss: 0.022606, KL fake Loss: 6.821213
Classification Train Epoch: 37 [19200/48000 (40%)]	Loss: 0.006478, KL fake Loss: 7.033466
Classification Train Epoch: 37 [25600/48000 (53%)]	Loss: 0.004481, KL fake Loss: 7.000037
Classification Train Epoch: 37 [32000/48000 (67%)]	Loss: 0.068519, KL fake Loss: 6.865623
Classification Train Epoch: 37 [38400/48000 (80%)]	Loss: 0.008390, KL fake Loss: 7.068597
Classification Train Epoch: 37 [44800/48000 (93%)]	Loss: 0.005526, KL fake Loss: 7.717949

Test set: Average loss: 0.3519, Accuracy: 7409/8000 (93%)

Classification Train Epoch: 38 [0/48000 (0%)]	Loss: 0.004742, KL fake Loss: 7.362850
Classification Train Epoch: 38 [6400/48000 (13%)]	Loss: 0.003843, KL fake Loss: 7.295807
Classification Train Epoch: 38 [12800/48000 (27%)]	Loss: 0.007959, KL fake Loss: 7.161716
Classification Train Epoch: 38 [19200/48000 (40%)]	Loss: 0.002885, KL fake Loss: 6.689563
Classification Train Epoch: 38 [25600/48000 (53%)]	Loss: 0.008735, KL fake Loss: 7.370694
Classification Train Epoch: 38 [32000/48000 (67%)]	Loss: 0.000874, KL fake Loss: 7.294181
Classification Train Epoch: 38 [38400/48000 (80%)]	Loss: 0.001668, KL fake Loss: 7.315771
Classification Train Epoch: 38 [44800/48000 (93%)]	Loss: 0.011129, KL fake Loss: 6.774981

Test set: Average loss: 0.3926, Accuracy: 7363/8000 (92%)

Classification Train Epoch: 39 [0/48000 (0%)]	Loss: 0.008334, KL fake Loss: 7.583042
Classification Train Epoch: 39 [6400/48000 (13%)]	Loss: 0.024763, KL fake Loss: 6.920303
Classification Train Epoch: 39 [12800/48000 (27%)]	Loss: 0.018235, KL fake Loss: 7.973762
Classification Train Epoch: 39 [19200/48000 (40%)]	Loss: 0.090459, KL fake Loss: 7.482008
Classification Train Epoch: 39 [25600/48000 (53%)]	Loss: 0.070194, KL fake Loss: 7.438892
Classification Train Epoch: 39 [32000/48000 (67%)]	Loss: 0.047910, KL fake Loss: 7.316922
Classification Train Epoch: 39 [38400/48000 (80%)]	Loss: 0.064979, KL fake Loss: 7.718604
Classification Train Epoch: 39 [44800/48000 (93%)]	Loss: 0.012823, KL fake Loss: 7.051643

Test set: Average loss: 0.3556, Accuracy: 7398/8000 (92%)

Classification Train Epoch: 40 [0/48000 (0%)]	Loss: 0.002047, KL fake Loss: 7.108024
Classification Train Epoch: 40 [6400/48000 (13%)]	Loss: 0.003081, KL fake Loss: 7.568825
Classification Train Epoch: 40 [12800/48000 (27%)]	Loss: 0.033525, KL fake Loss: 7.290775
Classification Train Epoch: 40 [19200/48000 (40%)]	Loss: 0.004804, KL fake Loss: 7.662035
Classification Train Epoch: 40 [25600/48000 (53%)]	Loss: 0.007699, KL fake Loss: 7.376801
Classification Train Epoch: 40 [32000/48000 (67%)]	Loss: 0.000806, KL fake Loss: 6.871501
Classification Train Epoch: 40 [38400/48000 (80%)]	Loss: 0.003090, KL fake Loss: 7.526383
Classification Train Epoch: 40 [44800/48000 (93%)]	Loss: 0.011041, KL fake Loss: 7.291946

Test set: Average loss: 0.3519, Accuracy: 7449/8000 (93%)

Classification Train Epoch: 41 [0/48000 (0%)]	Loss: 0.000679, KL fake Loss: 8.010072
Classification Train Epoch: 41 [6400/48000 (13%)]	Loss: 0.005834, KL fake Loss: 7.504949
Classification Train Epoch: 41 [12800/48000 (27%)]	Loss: 0.000612, KL fake Loss: 7.395001
Classification Train Epoch: 41 [19200/48000 (40%)]	Loss: 0.014361, KL fake Loss: 7.747675
Classification Train Epoch: 41 [25600/48000 (53%)]	Loss: 0.005949, KL fake Loss: 7.123863
Classification Train Epoch: 41 [32000/48000 (67%)]	Loss: 0.048226, KL fake Loss: 7.165523
Classification Train Epoch: 41 [38400/48000 (80%)]	Loss: 0.149910, KL fake Loss: 6.685187
Classification Train Epoch: 41 [44800/48000 (93%)]	Loss: 0.014394, KL fake Loss: 7.418205

Test set: Average loss: 0.3890, Accuracy: 7380/8000 (92%)

Classification Train Epoch: 42 [0/48000 (0%)]	Loss: 0.036881, KL fake Loss: 7.394595
Classification Train Epoch: 42 [6400/48000 (13%)]	Loss: 0.003266, KL fake Loss: 7.478611
Classification Train Epoch: 42 [12800/48000 (27%)]	Loss: 0.048690, KL fake Loss: 6.990586
Classification Train Epoch: 42 [19200/48000 (40%)]	Loss: 0.005281, KL fake Loss: 6.606860
Classification Train Epoch: 42 [25600/48000 (53%)]	Loss: 0.032363, KL fake Loss: 7.053169
Classification Train Epoch: 42 [32000/48000 (67%)]	Loss: 0.007127, KL fake Loss: 7.751576
Classification Train Epoch: 42 [38400/48000 (80%)]	Loss: 0.020574, KL fake Loss: 6.900044
Classification Train Epoch: 42 [44800/48000 (93%)]	Loss: 0.027290, KL fake Loss: 7.602309

Test set: Average loss: 0.3661, Accuracy: 7403/8000 (93%)

Classification Train Epoch: 43 [0/48000 (0%)]	Loss: 0.030691, KL fake Loss: 7.370497
Classification Train Epoch: 43 [6400/48000 (13%)]	Loss: 0.001241, KL fake Loss: 7.464775
 43%|████▎     | 43/100 [1:57:14<2:35:24, 163.58s/it] 48%|████▊     | 48/100 [2:49:48<3:03:55, 212.23s/it] 44%|████▍     | 44/100 [1:59:57<2:32:40, 163.58s/it] 49%|████▉     | 49/100 [2:53:20<3:00:23, 212.22s/it] 45%|████▌     | 45/100 [2:02:41<2:29:56, 163.58s/it] 50%|█████     | 50/100 [2:56:52<2:56:50, 212.22s/it] 46%|████▌     | 46/100 [2:05:25<2:27:13, 163.58s/it] 47%|████▋     | 47/100 [2:08:08<2:24:29, 163.58s/it] 51%|█████     | 51/100 [3:00:24<2:53:18, 212.22s/it] 48%|████▊     | 48/100 [2:10:52<2:21:46, 163.58s/it] 52%|█████▏    | 52/100 [3:03:56<2:49:46, 212.22s/it] 49%|████▉     | 49/100 [2:13:35<2:19:02, 163.58s/it]Classification Train Epoch: 43 [6400/50000 (13%)]	Loss: 0.009435, KL fake Loss: 0.006837
Classification Train Epoch: 43 [12800/50000 (26%)]	Loss: 0.002335, KL fake Loss: 0.006287
Classification Train Epoch: 43 [19200/50000 (38%)]	Loss: 0.004411, KL fake Loss: 0.003324
Classification Train Epoch: 43 [25600/50000 (51%)]	Loss: 0.007109, KL fake Loss: 0.005706
Classification Train Epoch: 43 [32000/50000 (64%)]	Loss: 0.045365, KL fake Loss: 0.007503
Classification Train Epoch: 43 [38400/50000 (77%)]	Loss: 0.019430, KL fake Loss: 0.035950
Classification Train Epoch: 43 [44800/50000 (90%)]	Loss: 0.024286, KL fake Loss: 0.011605

Test set: Average loss: 2.8342, Accuracy: 2291/10000 (23%)

Classification Train Epoch: 44 [0/50000 (0%)]	Loss: 0.009807, KL fake Loss: 0.009872
Classification Train Epoch: 44 [6400/50000 (13%)]	Loss: 0.034255, KL fake Loss: 0.020234
Classification Train Epoch: 44 [12800/50000 (26%)]	Loss: 0.042141, KL fake Loss: 0.016989
Classification Train Epoch: 44 [19200/50000 (38%)]	Loss: 0.003250, KL fake Loss: 0.004078
Classification Train Epoch: 44 [25600/50000 (51%)]	Loss: 0.019807, KL fake Loss: 0.014143
Classification Train Epoch: 44 [32000/50000 (64%)]	Loss: 0.006207, KL fake Loss: 0.008880
Classification Train Epoch: 44 [38400/50000 (77%)]	Loss: 0.026728, KL fake Loss: 0.021761
Classification Train Epoch: 44 [44800/50000 (90%)]	Loss: 0.026236, KL fake Loss: 0.010038

Test set: Average loss: 2.4922, Accuracy: 3234/10000 (32%)

Classification Train Epoch: 45 [0/50000 (0%)]	Loss: 0.011388, KL fake Loss: 0.026922
Classification Train Epoch: 45 [6400/50000 (13%)]	Loss: 0.050828, KL fake Loss: 0.005991
Classification Train Epoch: 45 [12800/50000 (26%)]	Loss: 0.004232, KL fake Loss: 0.006531
Classification Train Epoch: 45 [19200/50000 (38%)]	Loss: 0.014839, KL fake Loss: 0.005776
Classification Train Epoch: 45 [25600/50000 (51%)]	Loss: 0.057936, KL fake Loss: 0.009462
Classification Train Epoch: 45 [32000/50000 (64%)]	Loss: 0.034658, KL fake Loss: 0.012378
Classification Train Epoch: 45 [38400/50000 (77%)]	Loss: 0.020499, KL fake Loss: 0.003519
Classification Train Epoch: 45 [44800/50000 (90%)]	Loss: 0.026070, KL fake Loss: 0.007817

Test set: Average loss: 2.2707, Accuracy: 2457/10000 (25%)

Classification Train Epoch: 46 [0/50000 (0%)]	Loss: 0.005396, KL fake Loss: 0.008739
Classification Train Epoch: 46 [6400/50000 (13%)]	Loss: 0.011087, KL fake Loss: 0.005859
Classification Train Epoch: 46 [12800/50000 (26%)]	Loss: 0.022156, KL fake Loss: 0.007669
Classification Train Epoch: 46 [19200/50000 (38%)]	Loss: 0.058753, KL fake Loss: 0.006583
Classification Train Epoch: 46 [25600/50000 (51%)]	Loss: 0.003295, KL fake Loss: 0.030934
Classification Train Epoch: 46 [32000/50000 (64%)]	Loss: 0.011625, KL fake Loss: 0.019114
Classification Train Epoch: 46 [38400/50000 (77%)]	Loss: 0.021430, KL fake Loss: 0.009835
Classification Train Epoch: 46 [44800/50000 (90%)]	Loss: 0.010451, KL fake Loss: 0.008763

Test set: Average loss: 1.8338, Accuracy: 3555/10000 (36%)

Classification Train Epoch: 47 [0/50000 (0%)]	Loss: 0.052388, KL fake Loss: 0.017182
Classification Train Epoch: 47 [6400/50000 (13%)]	Loss: 0.001920, KL fake Loss: 0.017974
Classification Train Epoch: 47 [12800/50000 (26%)]	Loss: 0.038205, KL fake Loss: 0.008857
Classification Train Epoch: 47 [19200/50000 (38%)]	Loss: 0.003717, KL fake Loss: 0.007755
Classification Train Epoch: 47 [25600/50000 (51%)]	Loss: 0.029632, KL fake Loss: 0.002979
Classification Train Epoch: 47 [32000/50000 (64%)]	Loss: 0.036561, KL fake Loss: 0.011603
Classification Train Epoch: 47 [38400/50000 (77%)]	Loss: 0.014556, KL fake Loss: 0.005346
Classification Train Epoch: 47 [44800/50000 (90%)]	Loss: 0.048107, KL fake Loss: 0.003293

Test set: Average loss: 1.9494, Accuracy: 3209/10000 (32%)

Classification Train Epoch: 48 [0/50000 (0%)]	Loss: 0.001081, KL fake Loss: 0.015431
Classification Train Epoch: 48 [6400/50000 (13%)]	Loss: 0.003374, KL fake Loss: 0.004774
Classification Train Epoch: 48 [12800/50000 (26%)]	Loss: 0.017576, KL fake Loss: 0.011112
Classification Train Epoch: 48 [19200/50000 (38%)]	Loss: 0.009611, KL fake Loss: 0.005161
Classification Train Epoch: 48 [25600/50000 (51%)]	Loss: 0.031227, KL fake Loss: 0.014547
Classification Train Epoch: 48 [32000/50000 (64%)]	Loss: 0.011505, KL fake Loss: 0.006728
Classification Train Epoch: 48 [38400/50000 (77%)]	Loss: 0.017078, KL fake Loss: 0.007057
Classification Train Epoch: 48 [44800/50000 (90%)]	Loss: 0.007583, KL fake Loss: 0.006622

Test set: Average loss: 2.0814, Accuracy: 2919/10000 (29%)

Classification Train Epoch: 49 [0/50000 (0%)]	Loss: 0.004206, KL fake Loss: 0.009964
Classification Train Epoch: 49 [6400/50000 (13%)]	Loss: 0.018444, KL fake Loss: 0.004388
Classification Train Epoch: 49 [12800/50000 (26%)]	Loss: 0.001966, KL fake Loss: 0.003959
Classification Train Epoch: 49 [19200/50000 (38%)]	Loss: 0.004380, KL fake Loss: 0.014429
Classification Train Epoch: 49 [25600/50000 (51%)]	Loss: 0.003855, KL fake Loss: 0.003469
Classification Train Epoch: 49 [32000/50000 (64%)]	Loss: 0.015892, KL fake Loss: 0.008330
Classification Train Epoch: 49 [38400/50000 (77%)]	Loss: 0.015564, KL fake Loss: 0.005619
Classification Train Epoch: 49 [44800/50000 (90%)]	Loss: 0.009565, KL fake Loss: 0.010261

Test set: Average loss: 1.9669, Accuracy: 3385/10000 (34%)

Classification Train Epoch: 50 [0/50000 (0%)]	Loss: 0.003482, KL fake Loss: 0.009933
Classification Train Epoch: 50 [6400/50000 (13%)]	Loss: 0.025705, KL fake Loss: 0.012240
Classification Train Epoch: 50 [12800/50000 (26%)]	Loss: 0.026511, KL fake Loss: 0.009006
Classification Train Epoch: 50 [19200/50000 (38%)]	Loss: 0.024557, KL fake Loss: 0.016175
Classification Train Epoch: 50 [25600/50000 (51%)]	Loss: 0.016167, KL fake Loss: 0.013042
Classification Train Epoch: 50 [32000/50000 (64%)]	Loss: 0.011267, KL fake Loss: 0.026201
Classification Train Epoch: 50 [38400/50000 (77%)]	Loss: 0.049643, KL fake Loss: 0.006367
Classification Train Epoch: 50 [44800/50000 (90%)]	Loss: 0.018679, KL fake Loss: 0.011713

Test set: Average loss: 2.3532, Accuracy: 2374/10000 (24%)

Classification Train Epoch: 51 [0/50000 (0%)]	Loss: 0.004912, KL fake Loss: 0.005830
Classification Train Epoch: 51 [6400/50000 (13%)]	Loss: 0.001613, KL fake Loss: 0.011859
Classification Train Epoch: 51 [12800/50000 (26%)]	Loss: 0.002373, KL fake Loss: 0.012254
Classification Train Epoch: 51 [19200/50000 (38%)]	Loss: 0.043843, KL fake Loss: 0.009535
Classification Train Epoch: 51 [25600/50000 (51%)]	Loss: 0.001194, KL fake Loss: 0.009820
Classification Train Epoch: 51 [32000/50000 (64%)]	Loss: 0.004224, KL fake Loss: 0.010040
Classification Train Epoch: 51 [38400/50000 (77%)]	Loss: 0.008615, KL fake Loss: 0.007663
Classification Train Epoch: 51 [44800/50000 (90%)]	Loss: 0.048743, KL fake Loss: 0.007929

Test set: Average loss: 1.8192, Accuracy: 3786/10000 (38%)

Classification Train Epoch: 52 [0/50000 (0%)]	Loss: 0.046136, KL fake Loss: 0.011906
Classification Train Epoch: 52 [6400/50000 (13%)]	Loss: 0.002780, KL fake Loss: 0.003023
Classification Train Epoch: 52 [12800/50000 (26%)]	Loss: 0.000546, KL fake Loss: 0.007177
Classification Train Epoch: 52 [19200/50000 (38%)]	Loss: 0.084344, KL fake Loss: 0.008043
Classification Train Epoch: 52 [25600/50000 (51%)]	Loss: 0.013010, KL fake Loss: 0.008531
Classification Train Epoch: 52 [32000/50000 (64%)]	Loss: 0.005208, KL fake Loss: 0.014678
Classification Train Epoch: 52 [38400/50000 (77%)]	Loss: 0.034333, KL fake Loss: 0.005085
Classification Train Epoch: 52 [44800/50000 (90%)]	Loss: 0.103691, KL fake Loss: 0.017971

Test set: Average loss: 1.9894, Accuracy: 3118/10000 (31%)

Classification Train Epoch: 53 [0/50000 (0%)]	Loss: 0.001437, KL fake Loss: 0.012478
Classification Train Epoch: 53 [6400/50000 (13%)]	Loss: 0.009275, KL fake Loss: 0.010004
Classification Train Epoch: 53 [12800/50000 (26%)]	Loss: 0.001585, KL fake Loss: 0.007494
Classification Train Epoch: 53 [19200/50000 (38%)]	Loss: 0.021145, KL fake Loss: 0.007143
Classification Train Epoch: 53 [25600/50000 (51%)]	Loss: 0.010423, KL fake Loss: 0.005619
 53%|█████▎    | 53/100 [3:07:29<2:46:14, 212.23s/it] 50%|█████     | 50/100 [2:16:19<2:16:19, 163.58s/it] 51%|█████     | 51/100 [2:19:02<2:13:35, 163.58s/it] 54%|█████▍    | 54/100 [3:11:01<2:42:43, 212.24s/it] 52%|█████▏    | 52/100 [2:21:46<2:10:51, 163.58s/it] 55%|█████▌    | 55/100 [3:14:33<2:39:10, 212.24s/it]Classification Train Epoch: 43 [12800/48000 (27%)]	Loss: 0.001861, KL fake Loss: 7.450893
Classification Train Epoch: 43 [19200/48000 (40%)]	Loss: 0.001182, KL fake Loss: 6.556173
Classification Train Epoch: 43 [25600/48000 (53%)]	Loss: 0.000829, KL fake Loss: 6.869984
Classification Train Epoch: 43 [32000/48000 (67%)]	Loss: 0.005515, KL fake Loss: 6.797587
Classification Train Epoch: 43 [38400/48000 (80%)]	Loss: 0.019541, KL fake Loss: 7.210528
Classification Train Epoch: 43 [44800/48000 (93%)]	Loss: 0.016374, KL fake Loss: 7.423959

Test set: Average loss: 0.3898, Accuracy: 7386/8000 (92%)

Classification Train Epoch: 44 [0/48000 (0%)]	Loss: 0.009387, KL fake Loss: 7.607340
Classification Train Epoch: 44 [6400/48000 (13%)]	Loss: 0.013632, KL fake Loss: 7.135039
Classification Train Epoch: 44 [12800/48000 (27%)]	Loss: 0.031734, KL fake Loss: 6.976309
Classification Train Epoch: 44 [19200/48000 (40%)]	Loss: 0.008279, KL fake Loss: 7.316700
Classification Train Epoch: 44 [25600/48000 (53%)]	Loss: 0.002135, KL fake Loss: 6.970280
Classification Train Epoch: 44 [32000/48000 (67%)]	Loss: 0.066262, KL fake Loss: 7.958041
Classification Train Epoch: 44 [38400/48000 (80%)]	Loss: 0.035876, KL fake Loss: 7.553808
Classification Train Epoch: 44 [44800/48000 (93%)]	Loss: 0.003847, KL fake Loss: 7.792573

Test set: Average loss: 0.3887, Accuracy: 7374/8000 (92%)

Classification Train Epoch: 45 [0/48000 (0%)]	Loss: 0.010242, KL fake Loss: 7.097882
Classification Train Epoch: 45 [6400/48000 (13%)]	Loss: 0.024935, KL fake Loss: 7.596008
Classification Train Epoch: 45 [12800/48000 (27%)]	Loss: 0.005112, KL fake Loss: 7.727712
Classification Train Epoch: 45 [19200/48000 (40%)]	Loss: 0.029869, KL fake Loss: 7.683193
Classification Train Epoch: 45 [25600/48000 (53%)]	Loss: 0.009674, KL fake Loss: 7.342362
Classification Train Epoch: 45 [32000/48000 (67%)]	Loss: 0.009079, KL fake Loss: 6.864348
Classification Train Epoch: 45 [38400/48000 (80%)]	Loss: 0.002435, KL fake Loss: 7.399776
Classification Train Epoch: 45 [44800/48000 (93%)]	Loss: 0.001283, KL fake Loss: 7.108107

Test set: Average loss: 0.3699, Accuracy: 7400/8000 (92%)

Classification Train Epoch: 46 [0/48000 (0%)]	Loss: 0.001381, KL fake Loss: 6.832356
Classification Train Epoch: 46 [6400/48000 (13%)]	Loss: 0.035040, KL fake Loss: 7.080788
Classification Train Epoch: 46 [12800/48000 (27%)]	Loss: 0.001462, KL fake Loss: 6.964392
Classification Train Epoch: 46 [19200/48000 (40%)]	Loss: 0.004922, KL fake Loss: 7.136532
Classification Train Epoch: 46 [25600/48000 (53%)]	Loss: 0.003609, KL fake Loss: 7.157705
Classification Train Epoch: 46 [32000/48000 (67%)]	Loss: 0.005988, KL fake Loss: 7.667358
Classification Train Epoch: 46 [38400/48000 (80%)]	Loss: 0.007468, KL fake Loss: 6.511844
Classification Train Epoch: 46 [44800/48000 (93%)]	Loss: 0.011305, KL fake Loss: 7.796171

Test set: Average loss: 0.3943, Accuracy: 7377/8000 (92%)

Classification Train Epoch: 47 [0/48000 (0%)]	Loss: 0.074576, KL fake Loss: 7.173350
Classification Train Epoch: 47 [6400/48000 (13%)]	Loss: 0.003072, KL fake Loss: 7.618819
Classification Train Epoch: 47 [12800/48000 (27%)]	Loss: 0.017938, KL fake Loss: 7.365294
Classification Train Epoch: 47 [19200/48000 (40%)]	Loss: 0.002447, KL fake Loss: 7.557367
Classification Train Epoch: 47 [25600/48000 (53%)]	Loss: 0.068266, KL fake Loss: 7.307540
Classification Train Epoch: 47 [32000/48000 (67%)]	Loss: 0.018244, KL fake Loss: 7.446464
Classification Train Epoch: 47 [38400/48000 (80%)]	Loss: 0.005235, KL fake Loss: 6.736169
Classification Train Epoch: 47 [44800/48000 (93%)]	Loss: 0.003449, KL fake Loss: 7.296562

Test set: Average loss: 0.3812, Accuracy: 7392/8000 (92%)

Classification Train Epoch: 48 [0/48000 (0%)]	Loss: 0.003996, KL fake Loss: 7.079161
Classification Train Epoch: 48 [6400/48000 (13%)]	Loss: 0.006686, KL fake Loss: 7.111416
Classification Train Epoch: 48 [12800/48000 (27%)]	Loss: 0.003246, KL fake Loss: 7.025242
Classification Train Epoch: 48 [19200/48000 (40%)]	Loss: 0.002216, KL fake Loss: 7.589223
Classification Train Epoch: 48 [25600/48000 (53%)]	Loss: 0.002648, KL fake Loss: 7.254994
Classification Train Epoch: 48 [32000/48000 (67%)]	Loss: 0.001864, KL fake Loss: 7.578753
Classification Train Epoch: 48 [38400/48000 (80%)]	Loss: 0.001344, KL fake Loss: 6.873052
Classification Train Epoch: 48 [44800/48000 (93%)]	Loss: 0.001148, KL fake Loss: 7.080270

Test set: Average loss: 0.4004, Accuracy: 7408/8000 (93%)

Classification Train Epoch: 49 [0/48000 (0%)]	Loss: 0.002849, KL fake Loss: 7.927563
Classification Train Epoch: 49 [6400/48000 (13%)]	Loss: 0.002217, KL fake Loss: 7.181064
Classification Train Epoch: 49 [12800/48000 (27%)]	Loss: 0.006798, KL fake Loss: 6.993959
Classification Train Epoch: 49 [19200/48000 (40%)]	Loss: 0.004353, KL fake Loss: 6.954336
Classification Train Epoch: 49 [25600/48000 (53%)]	Loss: 0.033703, KL fake Loss: 7.289984
Classification Train Epoch: 49 [32000/48000 (67%)]	Loss: 0.030689, KL fake Loss: 7.238181
Classification Train Epoch: 49 [38400/48000 (80%)]	Loss: 0.007810, KL fake Loss: 7.437222
Classification Train Epoch: 49 [44800/48000 (93%)]	Loss: 0.006497, KL fake Loss: 6.974739

Test set: Average loss: 0.3995, Accuracy: 7393/8000 (92%)

Classification Train Epoch: 50 [0/48000 (0%)]	Loss: 0.002452, KL fake Loss: 7.089920
Classification Train Epoch: 50 [6400/48000 (13%)]	Loss: 0.006020, KL fake Loss: 7.286572
Classification Train Epoch: 50 [12800/48000 (27%)]	Loss: 0.004107, KL fake Loss: 7.472475
Classification Train Epoch: 50 [19200/48000 (40%)]	Loss: 0.001971, KL fake Loss: 7.331383
Classification Train Epoch: 50 [25600/48000 (53%)]	Loss: 0.087068, KL fake Loss: 7.180033
Classification Train Epoch: 50 [32000/48000 (67%)]	Loss: 0.006399, KL fake Loss: 7.155234
Classification Train Epoch: 50 [38400/48000 (80%)]	Loss: 0.029047, KL fake Loss: 7.017903
Classification Train Epoch: 50 [44800/48000 (93%)]	Loss: 0.006615, KL fake Loss: 6.904497

Test set: Average loss: 0.3725, Accuracy: 7417/8000 (93%)

Classification Train Epoch: 51 [0/48000 (0%)]	Loss: 0.076010, KL fake Loss: 7.183012
Classification Train Epoch: 51 [6400/48000 (13%)]	Loss: 0.002496, KL fake Loss: 7.551895
Classification Train Epoch: 51 [12800/48000 (27%)]	Loss: 0.027677, KL fake Loss: 6.995538
Classification Train Epoch: 51 [19200/48000 (40%)]	Loss: 0.082198, KL fake Loss: 6.929725
Classification Train Epoch: 51 [25600/48000 (53%)]	Loss: 0.001117, KL fake Loss: 7.076007
Classification Train Epoch: 51 [32000/48000 (67%)]	Loss: 0.010464, KL fake Loss: 6.833511
Classification Train Epoch: 51 [38400/48000 (80%)]	Loss: 0.027585, KL fake Loss: 7.295972
Classification Train Epoch: 51 [44800/48000 (93%)]	Loss: 0.051744, KL fake Loss: 6.521661

Test set: Average loss: 0.3642, Accuracy: 7419/8000 (93%)

Classification Train Epoch: 52 [0/48000 (0%)]	Loss: 0.003875, KL fake Loss: 7.092994
Classification Train Epoch: 52 [6400/48000 (13%)]	Loss: 0.000599, KL fake Loss: 6.843089
Classification Train Epoch: 52 [12800/48000 (27%)]	Loss: 0.110678, KL fake Loss: 6.958757
Classification Train Epoch: 52 [19200/48000 (40%)]	Loss: 0.007177, KL fake Loss: 7.267373
Classification Train Epoch: 52 [25600/48000 (53%)]	Loss: 0.004818, KL fake Loss: 6.927924
Classification Train Epoch: 52 [32000/48000 (67%)]	Loss: 0.049350, KL fake Loss: 7.005346
Classification Train Epoch: 52 [38400/48000 (80%)]	Loss: 0.002114, KL fake Loss: 6.507102
Classification Train Epoch: 52 [44800/48000 (93%)]	Loss: 0.002933, KL fake Loss: 6.548489

Test set: Average loss: 0.3844, Accuracy: 7411/8000 (93%)

Classification Train Epoch: 53 [0/48000 (0%)]	Loss: 0.012855, KL fake Loss: 6.983936
Classification Train Epoch: 53 [6400/48000 (13%)]	Loss: 0.004027, KL fake Loss: 6.746030
Classification Train Epoch: 53 [12800/48000 (27%)]	Loss: 0.002988, KL fake Loss: 6.799088
Classification Train Epoch: 53 [19200/48000 (40%)]	Loss: 0.018814, KL fake Loss: 6.747201
Classification Train Epoch: 53 [25600/48000 (53%)]	Loss: 0.038192, KL fake Loss: 7.086563
Classification Train Epoch: 53 [32000/48000 (67%)]	Loss: 0.011056, KL fake Loss: 6.902241
Classification Train Epoch: 53 [38400/48000 (80%)]	Loss: 0.009746, KL fake Loss: 6.883338
 53%|█████▎    | 53/100 [2:24:30<2:08:08, 163.58s/it] 56%|█████▌    | 56/100 [3:18:05<2:35:38, 212.24s/it] 54%|█████▍    | 54/100 [2:27:13<2:05:24, 163.58s/it] 55%|█████▌    | 55/100 [2:29:57<2:02:40, 163.57s/it] 57%|█████▋    | 57/100 [3:21:38<2:32:06, 212.24s/it] 56%|█████▌    | 56/100 [2:32:40<1:59:57, 163.57s/it] 58%|█████▊    | 58/100 [3:25:10<2:28:33, 212.24s/it] 57%|█████▋    | 57/100 [2:35:24<1:57:13, 163.58s/it] 59%|█████▉    | 59/100 [3:28:42<2:25:01, 212.24s/it] 58%|█████▊    | 58/100 [2:38:07<1:54:30, 163.58s/it] 60%|██████    | 60/100 [3:32:14<2:21:30, 212.26s/it] 59%|█████▉    | 59/100 [2:40:51<1:51:46, 163.58s/it] 60%|██████    | 60/100 [2:43:35<1:49:04, 163.62s/it] 61%|██████    | 61/100 [3:35:47<2:17:57, 212.25s/it] 61%|██████    | 61/100 [2:46:18<1:46:20, 163.60s/it] 62%|██████▏   | 62/100 [3:39:19<2:14:25, 212.24s/it] 62%|██████▏   | 62/100 [2:49:02<1:43:36, 163.60s/it] 63%|██████▎   | 63/100 [3:42:51<2:10:52, 212.23s/it] 63%|██████▎   | 63/100 [2:51:46<1:40:53, 163.60s/it]Classification Train Epoch: 53 [32000/50000 (64%)]	Loss: 0.000765, KL fake Loss: 0.002633
Classification Train Epoch: 53 [38400/50000 (77%)]	Loss: 0.005299, KL fake Loss: 0.004504
Classification Train Epoch: 53 [44800/50000 (90%)]	Loss: 0.030179, KL fake Loss: 0.009342

Test set: Average loss: 2.0548, Accuracy: 2939/10000 (29%)

Classification Train Epoch: 54 [0/50000 (0%)]	Loss: 0.001674, KL fake Loss: 0.006968
Classification Train Epoch: 54 [6400/50000 (13%)]	Loss: 0.023695, KL fake Loss: 0.006859
Classification Train Epoch: 54 [12800/50000 (26%)]	Loss: 0.019069, KL fake Loss: 0.010036
Classification Train Epoch: 54 [19200/50000 (38%)]	Loss: 0.062732, KL fake Loss: 0.011783
Classification Train Epoch: 54 [25600/50000 (51%)]	Loss: 0.035710, KL fake Loss: 0.018986
Classification Train Epoch: 54 [32000/50000 (64%)]	Loss: 0.087644, KL fake Loss: 0.008004
Classification Train Epoch: 54 [38400/50000 (77%)]	Loss: 0.010865, KL fake Loss: 0.020516
Classification Train Epoch: 54 [44800/50000 (90%)]	Loss: 0.022147, KL fake Loss: 0.013871

Test set: Average loss: 2.0752, Accuracy: 3044/10000 (30%)

Classification Train Epoch: 55 [0/50000 (0%)]	Loss: 0.003219, KL fake Loss: 0.012290
Classification Train Epoch: 55 [6400/50000 (13%)]	Loss: 0.039506, KL fake Loss: 0.004823
Classification Train Epoch: 55 [12800/50000 (26%)]	Loss: 0.012915, KL fake Loss: 0.003082
Classification Train Epoch: 55 [19200/50000 (38%)]	Loss: 0.061483, KL fake Loss: 0.006233
Classification Train Epoch: 55 [25600/50000 (51%)]	Loss: 0.080682, KL fake Loss: 0.014428
Classification Train Epoch: 55 [32000/50000 (64%)]	Loss: 0.027161, KL fake Loss: 0.011483
Classification Train Epoch: 55 [38400/50000 (77%)]	Loss: 0.002007, KL fake Loss: 0.012665
Classification Train Epoch: 55 [44800/50000 (90%)]	Loss: 0.058056, KL fake Loss: 0.003167

Test set: Average loss: 2.2274, Accuracy: 2793/10000 (28%)

Classification Train Epoch: 56 [0/50000 (0%)]	Loss: 0.010039, KL fake Loss: 0.013746
Classification Train Epoch: 56 [6400/50000 (13%)]	Loss: 0.001743, KL fake Loss: 0.007135
Classification Train Epoch: 56 [12800/50000 (26%)]	Loss: 0.000389, KL fake Loss: 0.015867
Classification Train Epoch: 56 [19200/50000 (38%)]	Loss: 0.025757, KL fake Loss: 0.011021
Classification Train Epoch: 56 [25600/50000 (51%)]	Loss: 0.007016, KL fake Loss: 0.002613
Classification Train Epoch: 56 [32000/50000 (64%)]	Loss: 0.002633, KL fake Loss: 0.007080
Classification Train Epoch: 56 [38400/50000 (77%)]	Loss: 0.013700, KL fake Loss: 0.007178
Classification Train Epoch: 56 [44800/50000 (90%)]	Loss: 0.000896, KL fake Loss: 0.003003

Test set: Average loss: 1.9301, Accuracy: 3461/10000 (35%)

Classification Train Epoch: 57 [0/50000 (0%)]	Loss: 0.004591, KL fake Loss: 0.019581
Classification Train Epoch: 57 [6400/50000 (13%)]	Loss: 0.049847, KL fake Loss: 0.004833
Classification Train Epoch: 57 [12800/50000 (26%)]	Loss: 0.076063, KL fake Loss: 0.011169
Classification Train Epoch: 57 [19200/50000 (38%)]	Loss: 0.007496, KL fake Loss: 0.004910
Classification Train Epoch: 57 [25600/50000 (51%)]	Loss: 0.006603, KL fake Loss: 0.015307
Classification Train Epoch: 57 [32000/50000 (64%)]	Loss: 0.009328, KL fake Loss: 0.010165
Classification Train Epoch: 57 [38400/50000 (77%)]	Loss: 0.027409, KL fake Loss: 0.006329
Classification Train Epoch: 57 [44800/50000 (90%)]	Loss: 0.002633, KL fake Loss: 0.008814

Test set: Average loss: 2.1308, Accuracy: 2902/10000 (29%)

Classification Train Epoch: 58 [0/50000 (0%)]	Loss: 0.009130, KL fake Loss: 0.014770
Classification Train Epoch: 58 [6400/50000 (13%)]	Loss: 0.049233, KL fake Loss: 0.005646
Classification Train Epoch: 58 [12800/50000 (26%)]	Loss: 0.003226, KL fake Loss: 0.007387
Classification Train Epoch: 58 [19200/50000 (38%)]	Loss: 0.035985, KL fake Loss: 0.007325
Classification Train Epoch: 58 [25600/50000 (51%)]	Loss: 0.026319, KL fake Loss: 0.006526
Classification Train Epoch: 58 [32000/50000 (64%)]	Loss: 0.001855, KL fake Loss: 0.009886
Classification Train Epoch: 58 [38400/50000 (77%)]	Loss: 0.070055, KL fake Loss: 0.010486
Classification Train Epoch: 58 [44800/50000 (90%)]	Loss: 0.077508, KL fake Loss: 0.009883

Test set: Average loss: 1.9884, Accuracy: 2805/10000 (28%)

Classification Train Epoch: 59 [0/50000 (0%)]	Loss: 0.138168, KL fake Loss: 0.013814
Classification Train Epoch: 59 [6400/50000 (13%)]	Loss: 0.079794, KL fake Loss: 0.007921
Classification Train Epoch: 59 [12800/50000 (26%)]	Loss: 0.005001, KL fake Loss: 0.016811
Classification Train Epoch: 59 [19200/50000 (38%)]	Loss: 0.002704, KL fake Loss: 0.004099
Classification Train Epoch: 59 [25600/50000 (51%)]	Loss: 0.000630, KL fake Loss: 0.001939
Classification Train Epoch: 59 [32000/50000 (64%)]	Loss: 0.004246, KL fake Loss: 0.010794
Classification Train Epoch: 59 [38400/50000 (77%)]	Loss: 0.010354, KL fake Loss: 0.014455
Classification Train Epoch: 59 [44800/50000 (90%)]	Loss: 0.003668, KL fake Loss: 0.011111

Test set: Average loss: 2.6312, Accuracy: 2841/10000 (28%)

Classification Train Epoch: 60 [0/50000 (0%)]	Loss: 0.014528, KL fake Loss: 0.011330
Classification Train Epoch: 60 [6400/50000 (13%)]	Loss: 0.001454, KL fake Loss: 0.006188
Classification Train Epoch: 60 [12800/50000 (26%)]	Loss: 0.007568, KL fake Loss: 0.007251
Classification Train Epoch: 60 [19200/50000 (38%)]	Loss: 0.007068, KL fake Loss: 0.015760
Classification Train Epoch: 60 [25600/50000 (51%)]	Loss: 0.040183, KL fake Loss: 0.003704
Classification Train Epoch: 60 [32000/50000 (64%)]	Loss: 0.022294, KL fake Loss: 0.004421
Classification Train Epoch: 60 [38400/50000 (77%)]	Loss: 0.007289, KL fake Loss: 0.005674
Classification Train Epoch: 60 [44800/50000 (90%)]	Loss: 0.019928, KL fake Loss: 0.006462

Test set: Average loss: 1.9812, Accuracy: 2635/10000 (26%)

Classification Train Epoch: 61 [0/50000 (0%)]	Loss: 0.000452, KL fake Loss: 0.009506
Classification Train Epoch: 61 [6400/50000 (13%)]	Loss: 0.005231, KL fake Loss: 0.001735
Classification Train Epoch: 61 [12800/50000 (26%)]	Loss: 0.000744, KL fake Loss: 0.000268
Classification Train Epoch: 61 [19200/50000 (38%)]	Loss: 0.000723, KL fake Loss: 0.002090
Classification Train Epoch: 61 [25600/50000 (51%)]	Loss: 0.000968, KL fake Loss: 0.000572
Classification Train Epoch: 61 [32000/50000 (64%)]	Loss: 0.001352, KL fake Loss: 0.002910
Classification Train Epoch: 61 [38400/50000 (77%)]	Loss: 0.000571, KL fake Loss: 0.000266
Classification Train Epoch: 61 [44800/50000 (90%)]	Loss: 0.028669, KL fake Loss: 0.000358

Test set: Average loss: 1.9445, Accuracy: 3010/10000 (30%)

Classification Train Epoch: 62 [0/50000 (0%)]	Loss: 0.001907, KL fake Loss: 0.001291
Classification Train Epoch: 62 [6400/50000 (13%)]	Loss: 0.008968, KL fake Loss: 0.000093
Classification Train Epoch: 62 [12800/50000 (26%)]	Loss: 0.001888, KL fake Loss: 0.000210
Classification Train Epoch: 62 [19200/50000 (38%)]	Loss: 0.005412, KL fake Loss: 0.000547
Classification Train Epoch: 62 [25600/50000 (51%)]	Loss: 0.019425, KL fake Loss: 0.000197
Classification Train Epoch: 62 [32000/50000 (64%)]	Loss: 0.000500, KL fake Loss: 0.000215
Classification Train Epoch: 62 [38400/50000 (77%)]	Loss: 0.001016, KL fake Loss: 0.000902
Classification Train Epoch: 62 [44800/50000 (90%)]	Loss: 0.005668, KL fake Loss: 0.004096

Test set: Average loss: 2.0946, Accuracy: 3085/10000 (31%)

Classification Train Epoch: 63 [0/50000 (0%)]	Loss: 0.000852, KL fake Loss: 0.000105
Classification Train Epoch: 63 [6400/50000 (13%)]	Loss: 0.000330, KL fake Loss: 0.003922
Classification Train Epoch: 63 [12800/50000 (26%)]	Loss: 0.000134, KL fake Loss: 0.001092
Classification Train Epoch: 63 [19200/50000 (38%)]	Loss: 0.000834, KL fake Loss: 0.002989
Classification Train Epoch: 63 [25600/50000 (51%)]	Loss: 0.000817, KL fake Loss: 0.000374
Classification Train Epoch: 63 [32000/50000 (64%)]	Loss: 0.001628, KL fake Loss: 0.000326
Classification Train Epoch: 63 [38400/50000 (77%)]	Loss: 0.000251, KL fake Loss: 0.000790
Classification Train Epoch: 63 [44800/50000 (90%)]	Loss: 0.000931, KL fake Loss: 0.000704

Test set: Average loss: 1.9864, Accuracy: 2813/10000 (28%)

Classification Train Epoch: 64 [0/50000 (0%)]	Loss: 0.000525, KL fake Loss: 0.001325
Classification Train Epoch: 53 [44800/48000 (93%)]	Loss: 0.004964, KL fake Loss: 6.858565

Test set: Average loss: 0.3816, Accuracy: 7406/8000 (93%)

Classification Train Epoch: 54 [0/48000 (0%)]	Loss: 0.001785, KL fake Loss: 7.242305
Classification Train Epoch: 54 [6400/48000 (13%)]	Loss: 0.006363, KL fake Loss: 6.574452
Classification Train Epoch: 54 [12800/48000 (27%)]	Loss: 0.012059, KL fake Loss: 6.611296
Classification Train Epoch: 54 [19200/48000 (40%)]	Loss: 0.011753, KL fake Loss: 7.097659
Classification Train Epoch: 54 [25600/48000 (53%)]	Loss: 0.000531, KL fake Loss: 7.095372
Classification Train Epoch: 54 [32000/48000 (67%)]	Loss: 0.008280, KL fake Loss: 6.689403
Classification Train Epoch: 54 [38400/48000 (80%)]	Loss: 0.024592, KL fake Loss: 6.767689
Classification Train Epoch: 54 [44800/48000 (93%)]	Loss: 0.041571, KL fake Loss: 6.761647

Test set: Average loss: 0.3736, Accuracy: 7422/8000 (93%)

Classification Train Epoch: 55 [0/48000 (0%)]	Loss: 0.001969, KL fake Loss: 7.174432
Classification Train Epoch: 55 [6400/48000 (13%)]	Loss: 0.008524, KL fake Loss: 7.124226
Classification Train Epoch: 55 [12800/48000 (27%)]	Loss: 0.036341, KL fake Loss: 7.186407
Classification Train Epoch: 55 [19200/48000 (40%)]	Loss: 0.007824, KL fake Loss: 6.744858
Classification Train Epoch: 55 [25600/48000 (53%)]	Loss: 0.009213, KL fake Loss: 6.918309
Classification Train Epoch: 55 [32000/48000 (67%)]	Loss: 0.001284, KL fake Loss: 7.517367
Classification Train Epoch: 55 [38400/48000 (80%)]	Loss: 0.004077, KL fake Loss: 6.635324
Classification Train Epoch: 55 [44800/48000 (93%)]	Loss: 0.002407, KL fake Loss: 7.586513

Test set: Average loss: 0.3756, Accuracy: 7447/8000 (93%)

Classification Train Epoch: 56 [0/48000 (0%)]	Loss: 0.025399, KL fake Loss: 6.738451
Classification Train Epoch: 56 [6400/48000 (13%)]	Loss: 0.002095, KL fake Loss: 6.463832
Classification Train Epoch: 56 [12800/48000 (27%)]	Loss: 0.002564, KL fake Loss: 6.987613
Classification Train Epoch: 56 [19200/48000 (40%)]	Loss: 0.001084, KL fake Loss: 6.964803
Classification Train Epoch: 56 [25600/48000 (53%)]	Loss: 0.014904, KL fake Loss: 7.658628
Classification Train Epoch: 56 [32000/48000 (67%)]	Loss: 0.015088, KL fake Loss: 6.767639
Classification Train Epoch: 56 [38400/48000 (80%)]	Loss: 0.001274, KL fake Loss: 6.864021
Classification Train Epoch: 56 [44800/48000 (93%)]	Loss: 0.061961, KL fake Loss: 6.828972

Test set: Average loss: 0.3945, Accuracy: 7381/8000 (92%)

Classification Train Epoch: 57 [0/48000 (0%)]	Loss: 0.042965, KL fake Loss: 7.454370
Classification Train Epoch: 57 [6400/48000 (13%)]	Loss: 0.022893, KL fake Loss: 7.472128
Classification Train Epoch: 57 [12800/48000 (27%)]	Loss: 0.005058, KL fake Loss: 7.631965
Classification Train Epoch: 57 [19200/48000 (40%)]	Loss: 0.012436, KL fake Loss: 7.083969
Classification Train Epoch: 57 [25600/48000 (53%)]	Loss: 0.047668, KL fake Loss: 7.066692
Classification Train Epoch: 57 [32000/48000 (67%)]	Loss: 0.014802, KL fake Loss: 6.884096
Classification Train Epoch: 57 [38400/48000 (80%)]	Loss: 0.004164, KL fake Loss: 6.776913
Classification Train Epoch: 57 [44800/48000 (93%)]	Loss: 0.001554, KL fake Loss: 6.622404

Test set: Average loss: 0.3929, Accuracy: 7406/8000 (93%)

Classification Train Epoch: 58 [0/48000 (0%)]	Loss: 0.003080, KL fake Loss: 7.216948
Classification Train Epoch: 58 [6400/48000 (13%)]	Loss: 0.002479, KL fake Loss: 7.164283
Classification Train Epoch: 58 [12800/48000 (27%)]	Loss: 0.001264, KL fake Loss: 6.402414
Classification Train Epoch: 58 [19200/48000 (40%)]	Loss: 0.006423, KL fake Loss: 6.725876
Classification Train Epoch: 58 [25600/48000 (53%)]	Loss: 0.014990, KL fake Loss: 6.728312
Classification Train Epoch: 58 [32000/48000 (67%)]	Loss: 0.024053, KL fake Loss: 6.850829
Classification Train Epoch: 58 [38400/48000 (80%)]	Loss: 0.031475, KL fake Loss: 6.784407
Classification Train Epoch: 58 [44800/48000 (93%)]	Loss: 0.003485, KL fake Loss: 6.569287

Test set: Average loss: 0.3725, Accuracy: 7441/8000 (93%)

Classification Train Epoch: 59 [0/48000 (0%)]	Loss: 0.000429, KL fake Loss: 6.514098
Classification Train Epoch: 59 [6400/48000 (13%)]	Loss: 0.004237, KL fake Loss: 6.642762
Classification Train Epoch: 59 [12800/48000 (27%)]	Loss: 0.021600, KL fake Loss: 6.631592
Classification Train Epoch: 59 [19200/48000 (40%)]	Loss: 0.007348, KL fake Loss: 7.411953
Classification Train Epoch: 59 [25600/48000 (53%)]	Loss: 0.005217, KL fake Loss: 6.851519
Classification Train Epoch: 59 [32000/48000 (67%)]	Loss: 0.001472, KL fake Loss: 7.419318
Classification Train Epoch: 59 [38400/48000 (80%)]	Loss: 0.009360, KL fake Loss: 7.065476
Classification Train Epoch: 59 [44800/48000 (93%)]	Loss: 0.024410, KL fake Loss: 6.640221

Test set: Average loss: 0.3688, Accuracy: 7421/8000 (93%)

Classification Train Epoch: 60 [0/48000 (0%)]	Loss: 0.005295, KL fake Loss: 7.212623
Classification Train Epoch: 60 [6400/48000 (13%)]	Loss: 0.028979, KL fake Loss: 6.907766
Classification Train Epoch: 60 [12800/48000 (27%)]	Loss: 0.025504, KL fake Loss: 6.704778
Classification Train Epoch: 60 [19200/48000 (40%)]	Loss: 0.001668, KL fake Loss: 6.799910
Classification Train Epoch: 60 [25600/48000 (53%)]	Loss: 0.007862, KL fake Loss: 6.269099
Classification Train Epoch: 60 [32000/48000 (67%)]	Loss: 0.002122, KL fake Loss: 6.248682
Classification Train Epoch: 60 [38400/48000 (80%)]	Loss: 0.008921, KL fake Loss: 6.583864
Classification Train Epoch: 60 [44800/48000 (93%)]	Loss: 0.001344, KL fake Loss: 6.379886

Test set: Average loss: 0.3801, Accuracy: 7417/8000 (93%)

Classification Train Epoch: 61 [0/48000 (0%)]	Loss: 0.002439, KL fake Loss: 6.742397
Classification Train Epoch: 61 [6400/48000 (13%)]	Loss: 0.001928, KL fake Loss: 6.613269
Classification Train Epoch: 61 [12800/48000 (27%)]	Loss: 0.002360, KL fake Loss: 6.941320
Classification Train Epoch: 61 [19200/48000 (40%)]	Loss: 0.000670, KL fake Loss: 6.691779
Classification Train Epoch: 61 [25600/48000 (53%)]	Loss: 0.001326, KL fake Loss: 6.882503
Classification Train Epoch: 61 [32000/48000 (67%)]	Loss: 0.003207, KL fake Loss: 5.833902
Classification Train Epoch: 61 [38400/48000 (80%)]	Loss: 0.001771, KL fake Loss: 6.498802
Classification Train Epoch: 61 [44800/48000 (93%)]	Loss: 0.002043, KL fake Loss: 6.794821

Test set: Average loss: 0.3506, Accuracy: 7461/8000 (93%)

Classification Train Epoch: 62 [0/48000 (0%)]	Loss: 0.003527, KL fake Loss: 6.176635
Classification Train Epoch: 62 [6400/48000 (13%)]	Loss: 0.001175, KL fake Loss: 6.525572
Classification Train Epoch: 62 [12800/48000 (27%)]	Loss: 0.002358, KL fake Loss: 6.489673
Classification Train Epoch: 62 [19200/48000 (40%)]	Loss: 0.000957, KL fake Loss: 6.618627
Classification Train Epoch: 62 [25600/48000 (53%)]	Loss: 0.004966, KL fake Loss: 6.258430
Classification Train Epoch: 62 [32000/48000 (67%)]	Loss: 0.001583, KL fake Loss: 6.001384
Classification Train Epoch: 62 [38400/48000 (80%)]	Loss: 0.002150, KL fake Loss: 6.528520
Classification Train Epoch: 62 [44800/48000 (93%)]	Loss: 0.001046, KL fake Loss: 6.632530

Test set: Average loss: 0.3464, Accuracy: 7465/8000 (93%)

Classification Train Epoch: 63 [0/48000 (0%)]	Loss: 0.000937, KL fake Loss: 6.041763
Classification Train Epoch: 63 [6400/48000 (13%)]	Loss: 0.000597, KL fake Loss: 5.920724
Classification Train Epoch: 63 [12800/48000 (27%)]	Loss: 0.000965, KL fake Loss: 6.728110
Classification Train Epoch: 63 [19200/48000 (40%)]	Loss: 0.000849, KL fake Loss: 6.870769
Classification Train Epoch: 63 [25600/48000 (53%)]	Loss: 0.001482, KL fake Loss: 7.112567
Classification Train Epoch: 63 [32000/48000 (67%)]	Loss: 0.000611, KL fake Loss: 5.855814
Classification Train Epoch: 63 [38400/48000 (80%)]	Loss: 0.001545, KL fake Loss: 6.502477
Classification Train Epoch: 63 [44800/48000 (93%)]	Loss: 0.001408, KL fake Loss: 6.218396

Test set: Average loss: 0.3394, Accuracy: 7473/8000 (93%)

Classification Train Epoch: 64 [0/48000 (0%)]	Loss: 0.000954, KL fake Loss: 5.967509
Classification Train Epoch: 64 [6400/48000 (13%)]	Loss: 0.001513, KL fake Loss: 6.658457
Classification Train Epoch: 64 [12800/48000 (27%)]	Loss: 0.001523, KL fake Loss: 6.132135
 64%|██████▍   | 64/100 [2:54:29<1:38:09, 163.59s/it] 64%|██████▍   | 64/100 [3:46:23<2:07:20, 212.24s/it] 65%|██████▌   | 65/100 [2:57:13<1:35:25, 163.59s/it] 65%|██████▌   | 65/100 [3:49:56<2:03:48, 212.24s/it] 66%|██████▌   | 66/100 [2:59:56<1:32:42, 163.59s/it] 66%|██████▌   | 66/100 [3:53:28<2:00:16, 212.24s/it] 67%|██████▋   | 67/100 [3:02:40<1:29:58, 163.59s/it] 68%|██████▊   | 68/100 [3:05:23<1:27:14, 163.59s/it] 67%|██████▋   | 67/100 [3:57:00<1:56:43, 212.24s/it] 69%|██████▉   | 69/100 [3:08:07<1:24:31, 163.59s/it] 68%|██████▊   | 68/100 [4:00:32<1:53:11, 212.24s/it] 70%|███████   | 70/100 [3:10:51<1:21:47, 163.59s/it] 69%|██████▉   | 69/100 [4:04:05<1:49:39, 212.24s/it] 71%|███████   | 71/100 [3:13:34<1:19:04, 163.60s/it] 70%|███████   | 70/100 [4:07:37<1:46:07, 212.24s/it] 72%|███████▏  | 72/100 [3:16:18<1:16:20, 163.59s/it] 73%|███████▎  | 73/100 [3:19:01<1:13:36, 163.59s/it] 71%|███████   | 71/100 [4:11:09<1:42:34, 212.24s/it]Classification Train Epoch: 64 [19200/48000 (40%)]	Loss: 0.000871, KL fake Loss: 6.266273
Classification Train Epoch: 64 [25600/48000 (53%)]	Loss: 0.000662, KL fake Loss: 6.870131
Classification Train Epoch: 64 [32000/48000 (67%)]	Loss: 0.003541, KL fake Loss: 6.753684
Classification Train Epoch: 64 [38400/48000 (80%)]	Loss: 0.000793, KL fake Loss: 6.457353
Classification Train Epoch: 64 [44800/48000 (93%)]	Loss: 0.000768, KL fake Loss: 6.062042

Test set: Average loss: 0.3374, Accuracy: 7461/8000 (93%)

Classification Train Epoch: 65 [0/48000 (0%)]	Loss: 0.000812, KL fake Loss: 5.962814
Classification Train Epoch: 65 [6400/48000 (13%)]	Loss: 0.000875, KL fake Loss: 6.132791
Classification Train Epoch: 65 [12800/48000 (27%)]	Loss: 0.000598, KL fake Loss: 6.228955
Classification Train Epoch: 65 [19200/48000 (40%)]	Loss: 0.000436, KL fake Loss: 5.866350
Classification Train Epoch: 65 [25600/48000 (53%)]	Loss: 0.001293, KL fake Loss: 6.021965
Classification Train Epoch: 65 [32000/48000 (67%)]	Loss: 0.002088, KL fake Loss: 5.877583
Classification Train Epoch: 65 [38400/48000 (80%)]	Loss: 0.001013, KL fake Loss: 5.605244
Classification Train Epoch: 65 [44800/48000 (93%)]	Loss: 0.000827, KL fake Loss: 6.020699

Test set: Average loss: 0.3317, Accuracy: 7460/8000 (93%)

Classification Train Epoch: 66 [0/48000 (0%)]	Loss: 0.001014, KL fake Loss: 5.777578
Classification Train Epoch: 66 [6400/48000 (13%)]	Loss: 0.001239, KL fake Loss: 5.641844
Classification Train Epoch: 66 [12800/48000 (27%)]	Loss: 0.000378, KL fake Loss: 5.725199
Classification Train Epoch: 66 [19200/48000 (40%)]	Loss: 0.000745, KL fake Loss: 6.165972
Classification Train Epoch: 66 [25600/48000 (53%)]	Loss: 0.000794, KL fake Loss: 5.444460
Classification Train Epoch: 66 [32000/48000 (67%)]	Loss: 0.002758, KL fake Loss: 5.831415
Classification Train Epoch: 66 [38400/48000 (80%)]	Loss: 0.001041, KL fake Loss: 5.566308
Classification Train Epoch: 66 [44800/48000 (93%)]	Loss: 0.000960, KL fake Loss: 4.758224

Test set: Average loss: 0.3240, Accuracy: 7469/8000 (93%)

Classification Train Epoch: 67 [0/48000 (0%)]	Loss: 0.000713, KL fake Loss: 5.761871
Classification Train Epoch: 67 [6400/48000 (13%)]	Loss: 0.001432, KL fake Loss: 5.818551
Classification Train Epoch: 67 [12800/48000 (27%)]	Loss: 0.001241, KL fake Loss: 5.772321
Classification Train Epoch: 67 [19200/48000 (40%)]	Loss: 0.000602, KL fake Loss: 5.405210
Classification Train Epoch: 67 [25600/48000 (53%)]	Loss: 0.000711, KL fake Loss: 5.096346
Classification Train Epoch: 67 [32000/48000 (67%)]	Loss: 0.001030, KL fake Loss: 5.681992
Classification Train Epoch: 67 [38400/48000 (80%)]	Loss: 0.000738, KL fake Loss: 5.656335
Classification Train Epoch: 67 [44800/48000 (93%)]	Loss: 0.001292, KL fake Loss: 5.596374

Test set: Average loss: 0.3215, Accuracy: 7470/8000 (93%)

Classification Train Epoch: 68 [0/48000 (0%)]	Loss: 0.000876, KL fake Loss: 5.238038
Classification Train Epoch: 68 [6400/48000 (13%)]	Loss: 0.000939, KL fake Loss: 5.443096
Classification Train Epoch: 68 [12800/48000 (27%)]	Loss: 0.000525, KL fake Loss: 5.262491
Classification Train Epoch: 68 [19200/48000 (40%)]	Loss: 0.001271, KL fake Loss: 5.424935
Classification Train Epoch: 68 [25600/48000 (53%)]	Loss: 0.000985, KL fake Loss: 5.144269
Classification Train Epoch: 68 [32000/48000 (67%)]	Loss: 0.000635, KL fake Loss: 5.246949
Classification Train Epoch: 68 [38400/48000 (80%)]	Loss: 0.000751, KL fake Loss: 5.389483
Classification Train Epoch: 68 [44800/48000 (93%)]	Loss: 0.001175, KL fake Loss: 5.196421

Test set: Average loss: 0.3213, Accuracy: 7469/8000 (93%)

Classification Train Epoch: 69 [0/48000 (0%)]	Loss: 0.000792, KL fake Loss: 5.467740
Classification Train Epoch: 69 [6400/48000 (13%)]	Loss: 0.000556, KL fake Loss: 5.129344
Classification Train Epoch: 69 [12800/48000 (27%)]	Loss: 0.001100, KL fake Loss: 5.784991
Classification Train Epoch: 69 [19200/48000 (40%)]	Loss: 0.000885, KL fake Loss: 5.111592
Classification Train Epoch: 69 [25600/48000 (53%)]	Loss: 0.001633, KL fake Loss: 5.225894
Classification Train Epoch: 69 [32000/48000 (67%)]	Loss: 0.001132, KL fake Loss: 5.110967
Classification Train Epoch: 69 [38400/48000 (80%)]	Loss: 0.001039, KL fake Loss: 4.708818
Classification Train Epoch: 69 [44800/48000 (93%)]	Loss: 0.000757, KL fake Loss: 5.585587

Test set: Average loss: 0.3245, Accuracy: 7462/8000 (93%)

Classification Train Epoch: 70 [0/48000 (0%)]	Loss: 0.001212, KL fake Loss: 4.850838
Classification Train Epoch: 70 [6400/48000 (13%)]	Loss: 0.001217, KL fake Loss: 5.004226
Classification Train Epoch: 70 [12800/48000 (27%)]	Loss: 0.001276, KL fake Loss: 5.300065
Classification Train Epoch: 70 [19200/48000 (40%)]	Loss: 0.000689, KL fake Loss: 4.430059
Classification Train Epoch: 70 [25600/48000 (53%)]	Loss: 0.000717, KL fake Loss: 5.013948
Classification Train Epoch: 70 [32000/48000 (67%)]	Loss: 0.000446, KL fake Loss: 4.605213
Classification Train Epoch: 70 [38400/48000 (80%)]	Loss: 0.001237, KL fake Loss: 4.551625
Classification Train Epoch: 70 [44800/48000 (93%)]	Loss: 0.001291, KL fake Loss: 4.736207

Test set: Average loss: 0.3165, Accuracy: 7460/8000 (93%)

Classification Train Epoch: 71 [0/48000 (0%)]	Loss: 0.000511, KL fake Loss: 4.605418
Classification Train Epoch: 71 [6400/48000 (13%)]	Loss: 0.000761, KL fake Loss: 4.328038
Classification Train Epoch: 71 [12800/48000 (27%)]	Loss: 0.001004, KL fake Loss: 5.189097
Classification Train Epoch: 71 [19200/48000 (40%)]	Loss: 0.000621, KL fake Loss: 4.270171
Classification Train Epoch: 71 [25600/48000 (53%)]	Loss: 0.000814, KL fake Loss: 4.719834
Classification Train Epoch: 71 [32000/48000 (67%)]	Loss: 0.000609, KL fake Loss: 4.332245
Classification Train Epoch: 71 [38400/48000 (80%)]	Loss: 0.001273, KL fake Loss: 4.713903
Classification Train Epoch: 71 [44800/48000 (93%)]	Loss: 0.001251, KL fake Loss: 4.244289

Test set: Average loss: 0.3114, Accuracy: 7470/8000 (93%)

Classification Train Epoch: 72 [0/48000 (0%)]	Loss: 0.000862, KL fake Loss: 4.339425
Classification Train Epoch: 72 [6400/48000 (13%)]	Loss: 0.001392, KL fake Loss: 4.912988
Classification Train Epoch: 72 [12800/48000 (27%)]	Loss: 0.001026, KL fake Loss: 4.724941
Classification Train Epoch: 72 [19200/48000 (40%)]	Loss: 0.000628, KL fake Loss: 4.225976
Classification Train Epoch: 72 [25600/48000 (53%)]	Loss: 0.001425, KL fake Loss: 3.987614
Classification Train Epoch: 72 [32000/48000 (67%)]	Loss: 0.000619, KL fake Loss: 4.100078
Classification Train Epoch: 72 [38400/48000 (80%)]	Loss: 0.001059, KL fake Loss: 4.799439
Classification Train Epoch: 72 [44800/48000 (93%)]	Loss: 0.000642, KL fake Loss: 4.627766

Test set: Average loss: 0.3196, Accuracy: 7463/8000 (93%)

Classification Train Epoch: 73 [0/48000 (0%)]	Loss: 0.000776, KL fake Loss: 3.853996
Classification Train Epoch: 73 [6400/48000 (13%)]	Loss: 0.000526, KL fake Loss: 4.051397
Classification Train Epoch: 73 [12800/48000 (27%)]	Loss: 0.000429, KL fake Loss: 4.214465
Classification Train Epoch: 73 [19200/48000 (40%)]	Loss: 0.000549, KL fake Loss: 3.587549
Classification Train Epoch: 73 [25600/48000 (53%)]	Loss: 0.000845, KL fake Loss: 3.616838
Classification Train Epoch: 73 [32000/48000 (67%)]	Loss: 0.001184, KL fake Loss: 3.648567
Classification Train Epoch: 73 [38400/48000 (80%)]	Loss: 0.000744, KL fake Loss: 4.416180
Classification Train Epoch: 73 [44800/48000 (93%)]	Loss: 0.000493, KL fake Loss: 4.220544

Test set: Average loss: 0.3232, Accuracy: 7464/8000 (93%)

Classification Train Epoch: 74 [0/48000 (0%)]	Loss: 0.000522, KL fake Loss: 4.351988
Classification Train Epoch: 74 [6400/48000 (13%)]	Loss: 0.000470, KL fake Loss: 3.596976
Classification Train Epoch: 74 [12800/48000 (27%)]	Loss: 0.000749, KL fake Loss: 3.916485
Classification Train Epoch: 74 [19200/48000 (40%)]	Loss: 0.000833, KL fake Loss: 3.794523
Classification Train Epoch: 74 [25600/48000 (53%)]	Loss: 0.000605, KL fake Loss: 3.955979
Classification Train Epoch: 74 [32000/48000 (67%)]	Loss: 0.001042, KL fake Loss: 4.103145
Classification Train Epoch: 74 [38400/48000 (80%)]	Loss: 0.000653, KL fake Loss: 3.016324
Classification Train Epoch: 74 [44800/48000 (93%)]	Loss: 0.001261, KL fake Loss: 3.648649
 74%|███████▍  | 74/100 [3:21:45<1:10:53, 163.59s/it] 72%|███████▏  | 72/100 [4:14:41<1:39:02, 212.24s/it] 75%|███████▌  | 75/100 [3:24:29<1:08:09, 163.59s/it] 73%|███████▎  | 73/100 [4:18:14<1:35:30, 212.24s/it] 76%|███████▌  | 76/100 [3:27:12<1:05:26, 163.59s/it]Classification Train Epoch: 64 [6400/50000 (13%)]	Loss: 0.000492, KL fake Loss: 0.002063
Classification Train Epoch: 64 [12800/50000 (26%)]	Loss: 0.000098, KL fake Loss: 0.000363
Classification Train Epoch: 64 [19200/50000 (38%)]	Loss: 0.000158, KL fake Loss: 0.002863
Classification Train Epoch: 64 [25600/50000 (51%)]	Loss: 0.000315, KL fake Loss: 0.000172
Classification Train Epoch: 64 [32000/50000 (64%)]	Loss: 0.000614, KL fake Loss: 0.000204
Classification Train Epoch: 64 [38400/50000 (77%)]	Loss: 0.000175, KL fake Loss: 0.000332
Classification Train Epoch: 64 [44800/50000 (90%)]	Loss: 0.000588, KL fake Loss: 0.000043

Test set: Average loss: 2.0010, Accuracy: 2871/10000 (29%)

Classification Train Epoch: 65 [0/50000 (0%)]	Loss: 0.000248, KL fake Loss: 0.000488
Classification Train Epoch: 65 [6400/50000 (13%)]	Loss: 0.000423, KL fake Loss: 0.002385
Classification Train Epoch: 65 [12800/50000 (26%)]	Loss: 0.000931, KL fake Loss: 0.000352
Classification Train Epoch: 65 [19200/50000 (38%)]	Loss: 0.000131, KL fake Loss: 0.000697
Classification Train Epoch: 65 [25600/50000 (51%)]	Loss: 0.000409, KL fake Loss: 0.000944
Classification Train Epoch: 65 [32000/50000 (64%)]	Loss: 0.000299, KL fake Loss: 0.000132
Classification Train Epoch: 65 [38400/50000 (77%)]	Loss: 0.000510, KL fake Loss: 0.000253
Classification Train Epoch: 65 [44800/50000 (90%)]	Loss: 0.000092, KL fake Loss: 0.000230

Test set: Average loss: 2.0741, Accuracy: 2805/10000 (28%)

Classification Train Epoch: 66 [0/50000 (0%)]	Loss: 0.000192, KL fake Loss: 0.000123
Classification Train Epoch: 66 [6400/50000 (13%)]	Loss: 0.000446, KL fake Loss: 0.000530
Classification Train Epoch: 66 [12800/50000 (26%)]	Loss: 0.000288, KL fake Loss: 0.000196
Classification Train Epoch: 66 [19200/50000 (38%)]	Loss: 0.000943, KL fake Loss: 0.000112
Classification Train Epoch: 66 [25600/50000 (51%)]	Loss: 0.000943, KL fake Loss: 0.000092
Classification Train Epoch: 66 [32000/50000 (64%)]	Loss: 0.000696, KL fake Loss: 0.000092
Classification Train Epoch: 66 [38400/50000 (77%)]	Loss: 0.000589, KL fake Loss: 0.000160
Classification Train Epoch: 66 [44800/50000 (90%)]	Loss: 0.000834, KL fake Loss: 0.000710

Test set: Average loss: 2.1569, Accuracy: 3540/10000 (35%)

Classification Train Epoch: 67 [0/50000 (0%)]	Loss: 0.000102, KL fake Loss: 0.001974
Classification Train Epoch: 67 [6400/50000 (13%)]	Loss: 0.000285, KL fake Loss: 0.001132
Classification Train Epoch: 67 [12800/50000 (26%)]	Loss: 0.000740, KL fake Loss: 0.000045
Classification Train Epoch: 67 [19200/50000 (38%)]	Loss: 0.000159, KL fake Loss: 0.000049
Classification Train Epoch: 67 [25600/50000 (51%)]	Loss: 0.000332, KL fake Loss: 0.000815
Classification Train Epoch: 67 [32000/50000 (64%)]	Loss: 0.001087, KL fake Loss: 0.000851
Classification Train Epoch: 67 [38400/50000 (77%)]	Loss: 0.000523, KL fake Loss: 0.000377
Classification Train Epoch: 67 [44800/50000 (90%)]	Loss: 0.000417, KL fake Loss: 0.000220

Test set: Average loss: 2.0527, Accuracy: 2610/10000 (26%)

Classification Train Epoch: 68 [0/50000 (0%)]	Loss: 0.000524, KL fake Loss: 0.002771
Classification Train Epoch: 68 [6400/50000 (13%)]	Loss: 0.000133, KL fake Loss: 0.000151
Classification Train Epoch: 68 [12800/50000 (26%)]	Loss: 0.000254, KL fake Loss: 0.000492
Classification Train Epoch: 68 [19200/50000 (38%)]	Loss: 0.000723, KL fake Loss: 0.000139
Classification Train Epoch: 68 [25600/50000 (51%)]	Loss: 0.000133, KL fake Loss: 0.001527
Classification Train Epoch: 68 [32000/50000 (64%)]	Loss: 0.000363, KL fake Loss: 0.000175
Classification Train Epoch: 68 [38400/50000 (77%)]	Loss: 0.000367, KL fake Loss: 0.000579
Classification Train Epoch: 68 [44800/50000 (90%)]	Loss: 0.000236, KL fake Loss: 0.000165

Test set: Average loss: 2.0104, Accuracy: 2700/10000 (27%)

Classification Train Epoch: 69 [0/50000 (0%)]	Loss: 0.000132, KL fake Loss: 0.000887
Classification Train Epoch: 69 [6400/50000 (13%)]	Loss: 0.000329, KL fake Loss: 0.000979
Classification Train Epoch: 69 [12800/50000 (26%)]	Loss: 0.000217, KL fake Loss: 0.001642
Classification Train Epoch: 69 [19200/50000 (38%)]	Loss: 0.000152, KL fake Loss: 0.000823
Classification Train Epoch: 69 [25600/50000 (51%)]	Loss: 0.000386, KL fake Loss: 0.000778
Classification Train Epoch: 69 [32000/50000 (64%)]	Loss: 0.000017, KL fake Loss: 0.000275
Classification Train Epoch: 69 [38400/50000 (77%)]	Loss: 0.000023, KL fake Loss: 0.000070
Classification Train Epoch: 69 [44800/50000 (90%)]	Loss: 0.000056, KL fake Loss: 0.000914

Test set: Average loss: 2.0418, Accuracy: 3622/10000 (36%)

Classification Train Epoch: 70 [0/50000 (0%)]	Loss: 0.000201, KL fake Loss: 0.000212
Classification Train Epoch: 70 [6400/50000 (13%)]	Loss: 0.000153, KL fake Loss: 0.001284
Classification Train Epoch: 70 [12800/50000 (26%)]	Loss: 0.000028, KL fake Loss: 0.000062
Classification Train Epoch: 70 [19200/50000 (38%)]	Loss: 0.000183, KL fake Loss: 0.000067
Classification Train Epoch: 70 [25600/50000 (51%)]	Loss: 0.000132, KL fake Loss: 0.000155
Classification Train Epoch: 70 [32000/50000 (64%)]	Loss: 0.000118, KL fake Loss: 0.000033
Classification Train Epoch: 70 [38400/50000 (77%)]	Loss: 0.000251, KL fake Loss: 0.000341
Classification Train Epoch: 70 [44800/50000 (90%)]	Loss: 0.000082, KL fake Loss: 0.000068

Test set: Average loss: 2.1646, Accuracy: 3182/10000 (32%)

Classification Train Epoch: 71 [0/50000 (0%)]	Loss: 0.000654, KL fake Loss: 0.000172
Classification Train Epoch: 71 [6400/50000 (13%)]	Loss: 0.000047, KL fake Loss: 0.000236
Classification Train Epoch: 71 [12800/50000 (26%)]	Loss: 0.000152, KL fake Loss: 0.000169
Classification Train Epoch: 71 [19200/50000 (38%)]	Loss: 0.000172, KL fake Loss: 0.000076
Classification Train Epoch: 71 [25600/50000 (51%)]	Loss: 0.000226, KL fake Loss: 0.000096
Classification Train Epoch: 71 [32000/50000 (64%)]	Loss: 0.000113, KL fake Loss: 0.000362
Classification Train Epoch: 71 [38400/50000 (77%)]	Loss: 0.000358, KL fake Loss: 0.000366
Classification Train Epoch: 71 [44800/50000 (90%)]	Loss: 0.000041, KL fake Loss: 0.000042

Test set: Average loss: 2.0897, Accuracy: 2937/10000 (29%)

Classification Train Epoch: 72 [0/50000 (0%)]	Loss: 0.000325, KL fake Loss: 0.001054
Classification Train Epoch: 72 [6400/50000 (13%)]	Loss: 0.000268, KL fake Loss: 0.000644
Classification Train Epoch: 72 [12800/50000 (26%)]	Loss: 0.000226, KL fake Loss: 0.000666
Classification Train Epoch: 72 [19200/50000 (38%)]	Loss: 0.000142, KL fake Loss: 0.000061
Classification Train Epoch: 72 [25600/50000 (51%)]	Loss: 0.000214, KL fake Loss: 0.000088
Classification Train Epoch: 72 [32000/50000 (64%)]	Loss: 0.000276, KL fake Loss: 0.000392
Classification Train Epoch: 72 [38400/50000 (77%)]	Loss: 0.000048, KL fake Loss: 0.000962
Classification Train Epoch: 72 [44800/50000 (90%)]	Loss: 0.000103, KL fake Loss: 0.000175

Test set: Average loss: 2.0671, Accuracy: 2423/10000 (24%)

Classification Train Epoch: 73 [0/50000 (0%)]	Loss: 0.000214, KL fake Loss: 0.000790
Classification Train Epoch: 73 [6400/50000 (13%)]	Loss: 0.000066, KL fake Loss: 0.000056
Classification Train Epoch: 73 [12800/50000 (26%)]	Loss: 0.000029, KL fake Loss: 0.001943
Classification Train Epoch: 73 [19200/50000 (38%)]	Loss: 0.000161, KL fake Loss: 0.000391
Classification Train Epoch: 73 [25600/50000 (51%)]	Loss: 0.000457, KL fake Loss: 0.000394
Classification Train Epoch: 73 [32000/50000 (64%)]	Loss: 0.000061, KL fake Loss: 0.000249
Classification Train Epoch: 73 [38400/50000 (77%)]	Loss: 0.000151, KL fake Loss: 0.000611
Classification Train Epoch: 73 [44800/50000 (90%)]	Loss: 0.000488, KL fake Loss: 0.000031

Test set: Average loss: 2.0531, Accuracy: 2834/10000 (28%)

Classification Train Epoch: 74 [0/50000 (0%)]	Loss: 0.000013, KL fake Loss: 0.000546
Classification Train Epoch: 74 [6400/50000 (13%)]	Loss: 0.000083, KL fake Loss: 0.000739
Classification Train Epoch: 74 [12800/50000 (26%)]	Loss: 0.000211, KL fake Loss: 0.000016
Classification Train Epoch: 74 [19200/50000 (38%)]	Loss: 0.000179, KL fake Loss: 0.000041
Classification Train Epoch: 74 [25600/50000 (51%)]	Loss: 0.000045, KL fake Loss: 0.000039
 77%|███████▋  | 77/100 [3:29:56<1:02:42, 163.59s/it] 74%|███████▍  | 74/100 [4:21:46<1:31:58, 212.24s/it] 78%|███████▊  | 78/100 [3:32:39<59:58, 163.58s/it]   75%|███████▌  | 75/100 [4:25:18<1:28:25, 212.24s/it] 79%|███████▉  | 79/100 [3:35:23<57:15, 163.59s/it] 76%|███████▌  | 76/100 [4:28:50<1:24:53, 212.24s/it] 80%|████████  | 80/100 [3:38:07<54:32, 163.62s/it] 81%|████████  | 81/100 [3:40:50<51:48, 163.60s/it] 77%|███████▋  | 77/100 [4:32:22<1:21:21, 212.24s/it] 82%|████████▏ | 82/100 [3:43:34<49:04, 163.60s/it] 78%|███████▊  | 78/100 [4:35:55<1:17:49, 212.24s/it] 83%|████████▎ | 83/100 [3:46:17<46:21, 163.59s/it] 79%|███████▉  | 79/100 [4:39:27<1:14:17, 212.25s/it] 84%|████████▍ | 84/100 [3:49:01<43:37, 163.59s/it]
Test set: Average loss: 0.3306, Accuracy: 7445/8000 (93%)

Classification Train Epoch: 75 [0/48000 (0%)]	Loss: 0.000607, KL fake Loss: 3.571273
Classification Train Epoch: 75 [6400/48000 (13%)]	Loss: 0.000765, KL fake Loss: 3.131569
Classification Train Epoch: 75 [12800/48000 (27%)]	Loss: 0.000970, KL fake Loss: 3.209253
Classification Train Epoch: 75 [19200/48000 (40%)]	Loss: 0.000535, KL fake Loss: 3.249003
Classification Train Epoch: 75 [25600/48000 (53%)]	Loss: 0.000729, KL fake Loss: 3.302805
Classification Train Epoch: 75 [32000/48000 (67%)]	Loss: 0.000607, KL fake Loss: 3.207872
Classification Train Epoch: 75 [38400/48000 (80%)]	Loss: 0.000586, KL fake Loss: 3.417465
Classification Train Epoch: 75 [44800/48000 (93%)]	Loss: 0.000923, KL fake Loss: 3.013607

Test set: Average loss: 0.3294, Accuracy: 7455/8000 (93%)

Classification Train Epoch: 76 [0/48000 (0%)]	Loss: 0.000736, KL fake Loss: 3.118840
Classification Train Epoch: 76 [6400/48000 (13%)]	Loss: 0.000565, KL fake Loss: 3.192668
Classification Train Epoch: 76 [12800/48000 (27%)]	Loss: 0.001105, KL fake Loss: 3.324614
Classification Train Epoch: 76 [19200/48000 (40%)]	Loss: 0.000466, KL fake Loss: 2.547754
Classification Train Epoch: 76 [25600/48000 (53%)]	Loss: 0.001157, KL fake Loss: 3.024279
Classification Train Epoch: 76 [32000/48000 (67%)]	Loss: 0.000513, KL fake Loss: 2.532339
Classification Train Epoch: 76 [38400/48000 (80%)]	Loss: 0.001459, KL fake Loss: 2.604933
Classification Train Epoch: 76 [44800/48000 (93%)]	Loss: 0.000454, KL fake Loss: 2.524387

Test set: Average loss: 0.3427, Accuracy: 7426/8000 (93%)

Classification Train Epoch: 77 [0/48000 (0%)]	Loss: 0.000897, KL fake Loss: 2.476513
Classification Train Epoch: 77 [6400/48000 (13%)]	Loss: 0.000238, KL fake Loss: 3.207030
Classification Train Epoch: 77 [12800/48000 (27%)]	Loss: 0.001312, KL fake Loss: 2.576142
Classification Train Epoch: 77 [19200/48000 (40%)]	Loss: 0.000449, KL fake Loss: 2.201183
Classification Train Epoch: 77 [25600/48000 (53%)]	Loss: 0.000367, KL fake Loss: 2.314369
Classification Train Epoch: 77 [32000/48000 (67%)]	Loss: 0.001298, KL fake Loss: 2.338497
Classification Train Epoch: 77 [38400/48000 (80%)]	Loss: 0.000725, KL fake Loss: 2.219162
Classification Train Epoch: 77 [44800/48000 (93%)]	Loss: 0.000342, KL fake Loss: 2.482200

Test set: Average loss: 0.3286, Accuracy: 7440/8000 (93%)

Classification Train Epoch: 78 [0/48000 (0%)]	Loss: 0.000345, KL fake Loss: 3.229186
Classification Train Epoch: 78 [6400/48000 (13%)]	Loss: 0.001216, KL fake Loss: 2.222483
Classification Train Epoch: 78 [12800/48000 (27%)]	Loss: 0.000327, KL fake Loss: 2.295524
Classification Train Epoch: 78 [19200/48000 (40%)]	Loss: 0.000544, KL fake Loss: 2.262538
Classification Train Epoch: 78 [25600/48000 (53%)]	Loss: 0.000318, KL fake Loss: 2.176057
Classification Train Epoch: 78 [32000/48000 (67%)]	Loss: 0.000752, KL fake Loss: 2.676437
Classification Train Epoch: 78 [38400/48000 (80%)]	Loss: 0.001184, KL fake Loss: 2.560489
Classification Train Epoch: 78 [44800/48000 (93%)]	Loss: 0.000147, KL fake Loss: 3.708564

Test set: Average loss: 0.3314, Accuracy: 7424/8000 (93%)

Classification Train Epoch: 79 [0/48000 (0%)]	Loss: 0.000389, KL fake Loss: 2.577243
Classification Train Epoch: 79 [6400/48000 (13%)]	Loss: 0.000513, KL fake Loss: 4.092800
Classification Train Epoch: 79 [12800/48000 (27%)]	Loss: 0.000670, KL fake Loss: 2.418121
Classification Train Epoch: 79 [19200/48000 (40%)]	Loss: 0.000512, KL fake Loss: 2.528699
Classification Train Epoch: 79 [25600/48000 (53%)]	Loss: 0.000303, KL fake Loss: 2.358165
Classification Train Epoch: 79 [32000/48000 (67%)]	Loss: 0.000483, KL fake Loss: 2.008841
Classification Train Epoch: 79 [38400/48000 (80%)]	Loss: 0.000505, KL fake Loss: 1.865557
Classification Train Epoch: 79 [44800/48000 (93%)]	Loss: 0.000266, KL fake Loss: 2.160269

Test set: Average loss: 0.3358, Accuracy: 7432/8000 (93%)

Classification Train Epoch: 80 [0/48000 (0%)]	Loss: 0.000751, KL fake Loss: 2.688672
Classification Train Epoch: 80 [6400/48000 (13%)]	Loss: 0.000415, KL fake Loss: 2.221143
Classification Train Epoch: 80 [12800/48000 (27%)]	Loss: 0.000409, KL fake Loss: 2.161052
Classification Train Epoch: 80 [19200/48000 (40%)]	Loss: 0.000363, KL fake Loss: 2.452266
Classification Train Epoch: 80 [25600/48000 (53%)]	Loss: 0.000336, KL fake Loss: 1.764978
Classification Train Epoch: 80 [32000/48000 (67%)]	Loss: 0.000523, KL fake Loss: 2.166235
Classification Train Epoch: 80 [38400/48000 (80%)]	Loss: 0.001107, KL fake Loss: 2.019361
Classification Train Epoch: 80 [44800/48000 (93%)]	Loss: 0.000692, KL fake Loss: 2.047931

Test set: Average loss: 0.3353, Accuracy: 7436/8000 (93%)

Classification Train Epoch: 81 [0/48000 (0%)]	Loss: 0.000195, KL fake Loss: 2.002073
Classification Train Epoch: 81 [6400/48000 (13%)]	Loss: 0.000573, KL fake Loss: 1.931974
Classification Train Epoch: 81 [12800/48000 (27%)]	Loss: 0.000297, KL fake Loss: 2.145947
Classification Train Epoch: 81 [19200/48000 (40%)]	Loss: 0.000425, KL fake Loss: 1.698380
Classification Train Epoch: 81 [25600/48000 (53%)]	Loss: 0.000245, KL fake Loss: 1.878776
Classification Train Epoch: 81 [32000/48000 (67%)]	Loss: 0.000198, KL fake Loss: 2.954452
Classification Train Epoch: 81 [38400/48000 (80%)]	Loss: 0.000482, KL fake Loss: 2.033937
Classification Train Epoch: 81 [44800/48000 (93%)]	Loss: 0.000544, KL fake Loss: 2.149139

Test set: Average loss: 0.3454, Accuracy: 7434/8000 (93%)

Classification Train Epoch: 82 [0/48000 (0%)]	Loss: 0.000183, KL fake Loss: 1.901383
Classification Train Epoch: 82 [6400/48000 (13%)]	Loss: 0.001907, KL fake Loss: 1.886587
Classification Train Epoch: 82 [12800/48000 (27%)]	Loss: 0.000463, KL fake Loss: 2.097009
Classification Train Epoch: 82 [19200/48000 (40%)]	Loss: 0.000157, KL fake Loss: 1.576323
Classification Train Epoch: 82 [25600/48000 (53%)]	Loss: 0.000957, KL fake Loss: 1.863813
Classification Train Epoch: 82 [32000/48000 (67%)]	Loss: 0.000204, KL fake Loss: 1.646640
Classification Train Epoch: 82 [38400/48000 (80%)]	Loss: 0.000196, KL fake Loss: 2.129143
Classification Train Epoch: 82 [44800/48000 (93%)]	Loss: 0.002450, KL fake Loss: 2.081005

Test set: Average loss: 0.3291, Accuracy: 7431/8000 (93%)

Classification Train Epoch: 83 [0/48000 (0%)]	Loss: 0.000403, KL fake Loss: 1.770859
Classification Train Epoch: 83 [6400/48000 (13%)]	Loss: 0.000177, KL fake Loss: 1.719023
Classification Train Epoch: 83 [12800/48000 (27%)]	Loss: 0.000312, KL fake Loss: 2.174831
Classification Train Epoch: 83 [19200/48000 (40%)]	Loss: 0.001440, KL fake Loss: 1.734967
Classification Train Epoch: 83 [25600/48000 (53%)]	Loss: 0.000998, KL fake Loss: 1.716477
Classification Train Epoch: 83 [32000/48000 (67%)]	Loss: 0.000220, KL fake Loss: 1.929936
Classification Train Epoch: 83 [38400/48000 (80%)]	Loss: 0.000277, KL fake Loss: 1.555636
Classification Train Epoch: 83 [44800/48000 (93%)]	Loss: 0.000326, KL fake Loss: 1.614846

Test set: Average loss: 0.3270, Accuracy: 7436/8000 (93%)

Classification Train Epoch: 84 [0/48000 (0%)]	Loss: 0.000700, KL fake Loss: 1.988529
Classification Train Epoch: 84 [6400/48000 (13%)]	Loss: 0.000462, KL fake Loss: 1.943322
Classification Train Epoch: 84 [12800/48000 (27%)]	Loss: 0.000351, KL fake Loss: 1.604542
Classification Train Epoch: 84 [19200/48000 (40%)]	Loss: 0.000524, KL fake Loss: 1.394555
Classification Train Epoch: 84 [25600/48000 (53%)]	Loss: 0.001379, KL fake Loss: 1.530209
Classification Train Epoch: 84 [32000/48000 (67%)]	Loss: 0.000380, KL fake Loss: 1.323877
Classification Train Epoch: 84 [38400/48000 (80%)]	Loss: 0.000300, KL fake Loss: 1.501172
Classification Train Epoch: 84 [44800/48000 (93%)]	Loss: 0.000095, KL fake Loss: 1.175685

Test set: Average loss: 0.3247, Accuracy: 7438/8000 (93%)

Classification Train Epoch: 85 [0/48000 (0%)]	Loss: 0.000268, KL fake Loss: 1.658783
Classification Train Epoch: 85 [6400/48000 (13%)]	Loss: 0.000598, KL fake Loss: 1.463887
Classification Train Epoch: 85 [12800/48000 (27%)]	Loss: 0.000298, KL fake Loss: 1.619126
Classification Train Epoch: 85 [19200/48000 (40%)]	Loss: 0.000149, KL fake Loss: 1.351570
 80%|████████  | 80/100 [4:42:59<1:10:45, 212.28s/it] 85%|████████▌ | 85/100 [3:51:45<40:53, 163.59s/it] 86%|████████▌ | 86/100 [3:54:28<38:10, 163.59s/it] 81%|████████  | 81/100 [4:46:32<1:07:12, 212.25s/it] 87%|████████▋ | 87/100 [3:57:12<35:26, 163.58s/it] 82%|████████▏ | 82/100 [4:50:04<1:03:40, 212.24s/it] 88%|████████▊ | 88/100 [3:59:55<32:42, 163.58s/it] 83%|████████▎ | 83/100 [4:53:36<1:00:07, 212.23s/it] 89%|████████▉ | 89/100 [4:02:39<29:59, 163.58s/it] 90%|█████████ | 90/100 [4:05:22<27:15, 163.57s/it] 84%|████████▍ | 84/100 [4:57:08<56:35, 212.23s/it]  Classification Train Epoch: 74 [32000/50000 (64%)]	Loss: 0.000508, KL fake Loss: 0.000069
Classification Train Epoch: 74 [38400/50000 (77%)]	Loss: 0.009567, KL fake Loss: 0.000080
Classification Train Epoch: 74 [44800/50000 (90%)]	Loss: 0.000161, KL fake Loss: 0.000068

Test set: Average loss: 2.0909, Accuracy: 2344/10000 (23%)

Classification Train Epoch: 75 [0/50000 (0%)]	Loss: 0.000054, KL fake Loss: 0.000056
Classification Train Epoch: 75 [6400/50000 (13%)]	Loss: 0.000299, KL fake Loss: 0.000094
Classification Train Epoch: 75 [12800/50000 (26%)]	Loss: 0.000175, KL fake Loss: 0.000030
Classification Train Epoch: 75 [19200/50000 (38%)]	Loss: 0.000042, KL fake Loss: 0.000243
Classification Train Epoch: 75 [25600/50000 (51%)]	Loss: 0.000104, KL fake Loss: 0.000303
Classification Train Epoch: 75 [32000/50000 (64%)]	Loss: 0.000480, KL fake Loss: 0.000566
Classification Train Epoch: 75 [38400/50000 (77%)]	Loss: 0.000064, KL fake Loss: 0.000065
Classification Train Epoch: 75 [44800/50000 (90%)]	Loss: 0.000420, KL fake Loss: 0.000107

Test set: Average loss: 2.0530, Accuracy: 2380/10000 (24%)

Classification Train Epoch: 76 [0/50000 (0%)]	Loss: 0.000210, KL fake Loss: 0.000124
Classification Train Epoch: 76 [6400/50000 (13%)]	Loss: 0.000066, KL fake Loss: 0.000127
Classification Train Epoch: 76 [12800/50000 (26%)]	Loss: 0.000101, KL fake Loss: 0.000935
Classification Train Epoch: 76 [19200/50000 (38%)]	Loss: 0.000108, KL fake Loss: 0.000074
Classification Train Epoch: 76 [25600/50000 (51%)]	Loss: 0.000036, KL fake Loss: 0.000148
Classification Train Epoch: 76 [32000/50000 (64%)]	Loss: 0.000064, KL fake Loss: 0.000124
Classification Train Epoch: 76 [38400/50000 (77%)]	Loss: 0.000061, KL fake Loss: 0.000113
Classification Train Epoch: 76 [44800/50000 (90%)]	Loss: 0.000056, KL fake Loss: 0.001038

Test set: Average loss: 2.0071, Accuracy: 2907/10000 (29%)

Classification Train Epoch: 77 [0/50000 (0%)]	Loss: 0.000066, KL fake Loss: 0.000030
Classification Train Epoch: 77 [6400/50000 (13%)]	Loss: 0.000043, KL fake Loss: 0.001706
Classification Train Epoch: 77 [12800/50000 (26%)]	Loss: 0.000048, KL fake Loss: 0.000222
Classification Train Epoch: 77 [19200/50000 (38%)]	Loss: 0.000112, KL fake Loss: 0.000044
Classification Train Epoch: 77 [25600/50000 (51%)]	Loss: 0.000043, KL fake Loss: 0.001290
Classification Train Epoch: 77 [32000/50000 (64%)]	Loss: 0.000069, KL fake Loss: 0.000095
Classification Train Epoch: 77 [38400/50000 (77%)]	Loss: 0.005742, KL fake Loss: 0.000047
Classification Train Epoch: 77 [44800/50000 (90%)]	Loss: 0.000359, KL fake Loss: 0.000035

Test set: Average loss: 2.0157, Accuracy: 2907/10000 (29%)

Classification Train Epoch: 78 [0/50000 (0%)]	Loss: 0.000027, KL fake Loss: 0.000082
Classification Train Epoch: 78 [6400/50000 (13%)]	Loss: 0.000032, KL fake Loss: 0.000095
Classification Train Epoch: 78 [12800/50000 (26%)]	Loss: 0.000023, KL fake Loss: 0.000064
Classification Train Epoch: 78 [19200/50000 (38%)]	Loss: 0.000030, KL fake Loss: 0.000082
Classification Train Epoch: 78 [25600/50000 (51%)]	Loss: 0.000025, KL fake Loss: 0.000126
Classification Train Epoch: 78 [32000/50000 (64%)]	Loss: 0.000088, KL fake Loss: 0.000027
Classification Train Epoch: 78 [38400/50000 (77%)]	Loss: 0.000177, KL fake Loss: 0.000368
Classification Train Epoch: 78 [44800/50000 (90%)]	Loss: 0.000101, KL fake Loss: 0.000234

Test set: Average loss: 2.0865, Accuracy: 2647/10000 (26%)

Classification Train Epoch: 79 [0/50000 (0%)]	Loss: 0.000034, KL fake Loss: 0.000063
Classification Train Epoch: 79 [6400/50000 (13%)]	Loss: 0.000169, KL fake Loss: 0.000051
Classification Train Epoch: 79 [12800/50000 (26%)]	Loss: 0.000034, KL fake Loss: 0.000043
Classification Train Epoch: 79 [19200/50000 (38%)]	Loss: 0.000040, KL fake Loss: 0.000818
Classification Train Epoch: 79 [25600/50000 (51%)]	Loss: 0.000009, KL fake Loss: 0.000053
Classification Train Epoch: 79 [32000/50000 (64%)]	Loss: 0.000053, KL fake Loss: 0.001173
Classification Train Epoch: 79 [38400/50000 (77%)]	Loss: 0.000025, KL fake Loss: 0.000038
Classification Train Epoch: 79 [44800/50000 (90%)]	Loss: 0.000046, KL fake Loss: 0.000028

Test set: Average loss: 2.1120, Accuracy: 2800/10000 (28%)

Classification Train Epoch: 80 [0/50000 (0%)]	Loss: 0.000080, KL fake Loss: 0.000054
Classification Train Epoch: 80 [6400/50000 (13%)]	Loss: 0.000087, KL fake Loss: 0.000150
Classification Train Epoch: 80 [12800/50000 (26%)]	Loss: 0.000818, KL fake Loss: 0.000099
Classification Train Epoch: 80 [19200/50000 (38%)]	Loss: 0.000168, KL fake Loss: 0.000124
Classification Train Epoch: 80 [25600/50000 (51%)]	Loss: 0.000079, KL fake Loss: 0.000042
Classification Train Epoch: 80 [32000/50000 (64%)]	Loss: 0.001228, KL fake Loss: 0.000084
Classification Train Epoch: 80 [38400/50000 (77%)]	Loss: 0.000048, KL fake Loss: 0.000050
Classification Train Epoch: 80 [44800/50000 (90%)]	Loss: 0.000160, KL fake Loss: 0.000295

Test set: Average loss: 2.0606, Accuracy: 3282/10000 (33%)

Classification Train Epoch: 81 [0/50000 (0%)]	Loss: 0.000732, KL fake Loss: 0.000814
Classification Train Epoch: 81 [6400/50000 (13%)]	Loss: 0.000307, KL fake Loss: 0.000032
Classification Train Epoch: 81 [12800/50000 (26%)]	Loss: 0.000147, KL fake Loss: 0.000392
Classification Train Epoch: 81 [19200/50000 (38%)]	Loss: 0.000726, KL fake Loss: 0.000723
Classification Train Epoch: 81 [25600/50000 (51%)]	Loss: 0.000025, KL fake Loss: 0.000598
Classification Train Epoch: 81 [32000/50000 (64%)]	Loss: 0.000091, KL fake Loss: 0.000366
Classification Train Epoch: 81 [38400/50000 (77%)]	Loss: 0.000171, KL fake Loss: 0.000908
Classification Train Epoch: 81 [44800/50000 (90%)]	Loss: 0.000075, KL fake Loss: 0.000045

Test set: Average loss: 2.1801, Accuracy: 2789/10000 (28%)

Classification Train Epoch: 82 [0/50000 (0%)]	Loss: 0.000180, KL fake Loss: 0.000089
Classification Train Epoch: 82 [6400/50000 (13%)]	Loss: 0.000028, KL fake Loss: 0.000568
Classification Train Epoch: 82 [12800/50000 (26%)]	Loss: 0.000135, KL fake Loss: 0.000992
Classification Train Epoch: 82 [19200/50000 (38%)]	Loss: 0.000245, KL fake Loss: 0.000110
Classification Train Epoch: 82 [25600/50000 (51%)]	Loss: 0.000111, KL fake Loss: 0.001197
Classification Train Epoch: 82 [32000/50000 (64%)]	Loss: 0.000123, KL fake Loss: 0.000056
Classification Train Epoch: 82 [38400/50000 (77%)]	Loss: 0.000052, KL fake Loss: 0.000040
Classification Train Epoch: 82 [44800/50000 (90%)]	Loss: 0.000030, KL fake Loss: 0.000064

Test set: Average loss: 2.0770, Accuracy: 2477/10000 (25%)

Classification Train Epoch: 83 [0/50000 (0%)]	Loss: 0.000178, KL fake Loss: 0.000164
Classification Train Epoch: 83 [6400/50000 (13%)]	Loss: 0.000043, KL fake Loss: 0.000340
Classification Train Epoch: 83 [12800/50000 (26%)]	Loss: 0.000185, KL fake Loss: 0.000030
Classification Train Epoch: 83 [19200/50000 (38%)]	Loss: 0.000052, KL fake Loss: 0.000339
Classification Train Epoch: 83 [25600/50000 (51%)]	Loss: 0.000036, KL fake Loss: 0.000125
Classification Train Epoch: 83 [32000/50000 (64%)]	Loss: 0.000007, KL fake Loss: 0.000119
Classification Train Epoch: 83 [38400/50000 (77%)]	Loss: 0.000047, KL fake Loss: 0.000072
Classification Train Epoch: 83 [44800/50000 (90%)]	Loss: 0.000085, KL fake Loss: 0.000109

Test set: Average loss: 2.0766, Accuracy: 3004/10000 (30%)

Classification Train Epoch: 84 [0/50000 (0%)]	Loss: 0.000010, KL fake Loss: 0.000397
Classification Train Epoch: 84 [6400/50000 (13%)]	Loss: 0.000021, KL fake Loss: 0.000042
Classification Train Epoch: 84 [12800/50000 (26%)]	Loss: 0.000028, KL fake Loss: 0.000110
Classification Train Epoch: 84 [19200/50000 (38%)]	Loss: 0.000227, KL fake Loss: 0.000071
Classification Train Epoch: 84 [25600/50000 (51%)]	Loss: 0.000031, KL fake Loss: 0.000960
Classification Train Epoch: 84 [32000/50000 (64%)]	Loss: 0.000407, KL fake Loss: 0.000017
Classification Train Epoch: 84 [38400/50000 (77%)]	Loss: 0.000158, KL fake Loss: 0.000444
Classification Train Epoch: 84 [44800/50000 (90%)]	Loss: 0.000012, KL fake Loss: 0.000094

Test set: Average loss: 2.0642, Accuracy: 2486/10000 (25%)

Classification Train Epoch: 85 [0/50000 (0%)]	Loss: 0.000123, KL fake Loss: 0.000304
 91%|█████████ | 91/100 [4:08:06<24:32, 163.58s/it] 85%|████████▌ | 85/100 [5:00:40<53:03, 212.22s/it] 92%|█████████▏| 92/100 [4:10:50<21:48, 163.57s/it] 86%|████████▌ | 86/100 [5:04:13<49:31, 212.23s/it] 93%|█████████▎| 93/100 [4:13:33<19:05, 163.57s/it] 94%|█████████▍| 94/100 [4:16:17<16:21, 163.58s/it] 87%|████████▋ | 87/100 [5:07:45<45:58, 212.22s/it] 95%|█████████▌| 95/100 [4:19:00<13:37, 163.57s/it]Classification Train Epoch: 85 [25600/48000 (53%)]	Loss: 0.001230, KL fake Loss: 1.247082
Classification Train Epoch: 85 [32000/48000 (67%)]	Loss: 0.000328, KL fake Loss: 1.368056
Classification Train Epoch: 85 [38400/48000 (80%)]	Loss: 0.000878, KL fake Loss: 1.682586
Classification Train Epoch: 85 [44800/48000 (93%)]	Loss: 0.000520, KL fake Loss: 1.158377

Test set: Average loss: 0.3415, Accuracy: 7417/8000 (93%)

Classification Train Epoch: 86 [0/48000 (0%)]	Loss: 0.001072, KL fake Loss: 1.494113
Classification Train Epoch: 86 [6400/48000 (13%)]	Loss: 0.000251, KL fake Loss: 1.247663
Classification Train Epoch: 86 [12800/48000 (27%)]	Loss: 0.000891, KL fake Loss: 2.526550
Classification Train Epoch: 86 [19200/48000 (40%)]	Loss: 0.000120, KL fake Loss: 1.139631
Classification Train Epoch: 86 [25600/48000 (53%)]	Loss: 0.000462, KL fake Loss: 1.622089
Classification Train Epoch: 86 [32000/48000 (67%)]	Loss: 0.000252, KL fake Loss: 1.342076
Classification Train Epoch: 86 [38400/48000 (80%)]	Loss: 0.000096, KL fake Loss: 1.340456
Classification Train Epoch: 86 [44800/48000 (93%)]	Loss: 0.000264, KL fake Loss: 1.368751

Test set: Average loss: 0.3226, Accuracy: 7428/8000 (93%)

Classification Train Epoch: 87 [0/48000 (0%)]	Loss: 0.000083, KL fake Loss: 1.197976
Classification Train Epoch: 87 [6400/48000 (13%)]	Loss: 0.000392, KL fake Loss: 1.355164
Classification Train Epoch: 87 [12800/48000 (27%)]	Loss: 0.000301, KL fake Loss: 1.386875
Classification Train Epoch: 87 [19200/48000 (40%)]	Loss: 0.004984, KL fake Loss: 1.432784
Classification Train Epoch: 87 [25600/48000 (53%)]	Loss: 0.000617, KL fake Loss: 0.963762
Classification Train Epoch: 87 [32000/48000 (67%)]	Loss: 0.000280, KL fake Loss: 1.248725
Classification Train Epoch: 87 [38400/48000 (80%)]	Loss: 0.000135, KL fake Loss: 1.327978
Classification Train Epoch: 87 [44800/48000 (93%)]	Loss: 0.000405, KL fake Loss: 1.049088

Test set: Average loss: 0.4470, Accuracy: 7190/8000 (90%)

Classification Train Epoch: 88 [0/48000 (0%)]	Loss: 0.000539, KL fake Loss: 1.163504
Classification Train Epoch: 88 [6400/48000 (13%)]	Loss: 0.000200, KL fake Loss: 1.076593
Classification Train Epoch: 88 [12800/48000 (27%)]	Loss: 0.000049, KL fake Loss: 1.332194
Classification Train Epoch: 88 [19200/48000 (40%)]	Loss: 0.000134, KL fake Loss: 1.185778
Classification Train Epoch: 88 [25600/48000 (53%)]	Loss: 0.000164, KL fake Loss: 1.013708
Classification Train Epoch: 88 [32000/48000 (67%)]	Loss: 0.000339, KL fake Loss: 0.982506
Classification Train Epoch: 88 [38400/48000 (80%)]	Loss: 0.000639, KL fake Loss: 1.159315
Classification Train Epoch: 88 [44800/48000 (93%)]	Loss: 0.000410, KL fake Loss: 1.142978

Test set: Average loss: 0.3121, Accuracy: 7430/8000 (93%)

Classification Train Epoch: 89 [0/48000 (0%)]	Loss: 0.000319, KL fake Loss: 1.088964
Classification Train Epoch: 89 [6400/48000 (13%)]	Loss: 0.000593, KL fake Loss: 1.254481
Classification Train Epoch: 89 [12800/48000 (27%)]	Loss: 0.000440, KL fake Loss: 1.198045
Classification Train Epoch: 89 [19200/48000 (40%)]	Loss: 0.000192, KL fake Loss: 1.088979
Classification Train Epoch: 89 [25600/48000 (53%)]	Loss: 0.000266, KL fake Loss: 0.843153
Classification Train Epoch: 89 [32000/48000 (67%)]	Loss: 0.000279, KL fake Loss: 0.977836
Classification Train Epoch: 89 [38400/48000 (80%)]	Loss: 0.000347, KL fake Loss: 1.017663
Classification Train Epoch: 89 [44800/48000 (93%)]	Loss: 0.000207, KL fake Loss: 0.947341

Test set: Average loss: 0.3133, Accuracy: 7416/8000 (93%)

Classification Train Epoch: 90 [0/48000 (0%)]	Loss: 0.000355, KL fake Loss: 0.956718
Classification Train Epoch: 90 [6400/48000 (13%)]	Loss: 0.000097, KL fake Loss: 0.893098
Classification Train Epoch: 90 [12800/48000 (27%)]	Loss: 0.000136, KL fake Loss: 0.834545
Classification Train Epoch: 90 [19200/48000 (40%)]	Loss: 0.007167, KL fake Loss: 0.774787
Classification Train Epoch: 90 [25600/48000 (53%)]	Loss: 0.000175, KL fake Loss: 1.006022
Classification Train Epoch: 90 [32000/48000 (67%)]	Loss: 0.000175, KL fake Loss: 1.092829
Classification Train Epoch: 90 [38400/48000 (80%)]	Loss: 0.000159, KL fake Loss: 0.691607
Classification Train Epoch: 90 [44800/48000 (93%)]	Loss: 0.000580, KL fake Loss: 0.925033

Test set: Average loss: 0.3255, Accuracy: 7401/8000 (93%)

Classification Train Epoch: 91 [0/48000 (0%)]	Loss: 0.000058, KL fake Loss: 0.931926
Classification Train Epoch: 91 [6400/48000 (13%)]	Loss: 0.001098, KL fake Loss: 1.029046
Classification Train Epoch: 91 [12800/48000 (27%)]	Loss: 0.000106, KL fake Loss: 0.956566
Classification Train Epoch: 91 [19200/48000 (40%)]	Loss: 0.000547, KL fake Loss: 1.386278
Classification Train Epoch: 91 [25600/48000 (53%)]	Loss: 0.000067, KL fake Loss: 1.003182
Classification Train Epoch: 91 [32000/48000 (67%)]	Loss: 0.000099, KL fake Loss: 0.863107
Classification Train Epoch: 91 [38400/48000 (80%)]	Loss: 0.000249, KL fake Loss: 0.797858
Classification Train Epoch: 91 [44800/48000 (93%)]	Loss: 0.000070, KL fake Loss: 0.888806

Test set: Average loss: 0.3020, Accuracy: 7421/8000 (93%)

Classification Train Epoch: 92 [0/48000 (0%)]	Loss: 0.000075, KL fake Loss: 0.793714
Classification Train Epoch: 92 [6400/48000 (13%)]	Loss: 0.000159, KL fake Loss: 0.639482
Classification Train Epoch: 92 [12800/48000 (27%)]	Loss: 0.000462, KL fake Loss: 0.805732
Classification Train Epoch: 92 [19200/48000 (40%)]	Loss: 0.000062, KL fake Loss: 2.185702
Classification Train Epoch: 92 [25600/48000 (53%)]	Loss: 0.000069, KL fake Loss: 0.935337
Classification Train Epoch: 92 [32000/48000 (67%)]	Loss: 0.002835, KL fake Loss: 4.902064
Classification Train Epoch: 92 [38400/48000 (80%)]	Loss: 0.000278, KL fake Loss: 0.982428
Classification Train Epoch: 92 [44800/48000 (93%)]	Loss: 0.001468, KL fake Loss: 0.878613

Test set: Average loss: 0.3117, Accuracy: 7412/8000 (93%)

Classification Train Epoch: 93 [0/48000 (0%)]	Loss: 0.000065, KL fake Loss: 0.829141
Classification Train Epoch: 93 [6400/48000 (13%)]	Loss: 0.000057, KL fake Loss: 1.278396
Classification Train Epoch: 93 [12800/48000 (27%)]	Loss: 0.001681, KL fake Loss: 0.898632
Classification Train Epoch: 93 [19200/48000 (40%)]	Loss: 0.000573, KL fake Loss: 0.744142
Classification Train Epoch: 93 [25600/48000 (53%)]	Loss: 0.000193, KL fake Loss: 1.321298
Classification Train Epoch: 93 [32000/48000 (67%)]	Loss: 0.000102, KL fake Loss: 1.242775
Classification Train Epoch: 93 [38400/48000 (80%)]	Loss: 0.001281, KL fake Loss: 2.253292
Classification Train Epoch: 93 [44800/48000 (93%)]	Loss: 0.000510, KL fake Loss: 0.767726

Test set: Average loss: 0.3213, Accuracy: 7404/8000 (93%)

Classification Train Epoch: 94 [0/48000 (0%)]	Loss: 0.000167, KL fake Loss: 0.912786
Classification Train Epoch: 94 [6400/48000 (13%)]	Loss: 0.001214, KL fake Loss: 0.814224
Classification Train Epoch: 94 [12800/48000 (27%)]	Loss: 0.000105, KL fake Loss: 0.740041
Classification Train Epoch: 94 [19200/48000 (40%)]	Loss: 0.000316, KL fake Loss: 1.317209
Classification Train Epoch: 94 [25600/48000 (53%)]	Loss: 0.000228, KL fake Loss: 0.763137
Classification Train Epoch: 94 [32000/48000 (67%)]	Loss: 0.000137, KL fake Loss: 0.787329
Classification Train Epoch: 94 [38400/48000 (80%)]	Loss: 0.001363, KL fake Loss: 0.827698
Classification Train Epoch: 94 [44800/48000 (93%)]	Loss: 0.000078, KL fake Loss: 0.733949

Test set: Average loss: 0.3117, Accuracy: 7410/8000 (93%)

Classification Train Epoch: 95 [0/48000 (0%)]	Loss: 0.000063, KL fake Loss: 0.875518
Classification Train Epoch: 95 [6400/48000 (13%)]	Loss: 0.000027, KL fake Loss: 0.949228
Classification Train Epoch: 95 [12800/48000 (27%)]	Loss: 0.000123, KL fake Loss: 1.086522
Classification Train Epoch: 95 [19200/48000 (40%)]	Loss: 0.000604, KL fake Loss: 0.735788
Classification Train Epoch: 95 [25600/48000 (53%)]	Loss: 0.000292, KL fake Loss: 0.719176
Classification Train Epoch: 95 [32000/48000 (67%)]	Loss: 0.000224, KL fake Loss: 0.747548
Classification Train Epoch: 95 [38400/48000 (80%)]	Loss: 0.000081, KL fake Loss: 0.665168
Classification Train Epoch: 95 [44800/48000 (93%)]	Loss: 0.000256, KL fake Loss: 0.876098

Test set: Average loss: 0.3159, Accuracy: 7400/8000 (92%)

 88%|████████▊ | 88/100 [5:11:17<42:26, 212.22s/it] 96%|█████████▌| 96/100 [4:21:44<10:54, 163.57s/it] 89%|████████▉ | 89/100 [5:14:49<38:54, 212.23s/it] 97%|█████████▋| 97/100 [4:24:27<08:10, 163.58s/it] 90%|█████████ | 90/100 [5:18:21<35:22, 212.22s/it] 98%|█████████▊| 98/100 [4:27:11<05:27, 163.57s/it] 99%|█████████▉| 99/100 [4:29:55<02:43, 163.57s/it] 91%|█████████ | 91/100 [5:21:54<31:49, 212.22s/it]100%|██████████| 100/100 [4:32:38<00:00, 163.62s/it]100%|██████████| 100/100 [4:32:38<00:00, 163.59s/it]
Classification Train Epoch: 96 [0/48000 (0%)]	Loss: 0.000407, KL fake Loss: 0.955509
Classification Train Epoch: 96 [6400/48000 (13%)]	Loss: 0.000373, KL fake Loss: 0.798865
Classification Train Epoch: 96 [12800/48000 (27%)]	Loss: 0.000746, KL fake Loss: 1.292930
Classification Train Epoch: 96 [19200/48000 (40%)]	Loss: 0.000496, KL fake Loss: 0.873727
Classification Train Epoch: 96 [25600/48000 (53%)]	Loss: 0.000112, KL fake Loss: 0.718419
Classification Train Epoch: 96 [32000/48000 (67%)]	Loss: 0.000275, KL fake Loss: 0.683694
Classification Train Epoch: 96 [38400/48000 (80%)]	Loss: 0.000874, KL fake Loss: 0.797206
Classification Train Epoch: 96 [44800/48000 (93%)]	Loss: 0.000245, KL fake Loss: 0.851346

Test set: Average loss: 0.2986, Accuracy: 7425/8000 (93%)

Classification Train Epoch: 97 [0/48000 (0%)]	Loss: 0.000167, KL fake Loss: 0.695030
Classification Train Epoch: 97 [6400/48000 (13%)]	Loss: 0.000038, KL fake Loss: 0.575250
Classification Train Epoch: 97 [12800/48000 (27%)]	Loss: 0.001222, KL fake Loss: 0.602738
Classification Train Epoch: 97 [19200/48000 (40%)]	Loss: 0.000127, KL fake Loss: 0.819716
Classification Train Epoch: 97 [25600/48000 (53%)]	Loss: 0.000220, KL fake Loss: 0.606234
Classification Train Epoch: 97 [32000/48000 (67%)]	Loss: 0.000040, KL fake Loss: 0.715077
Classification Train Epoch: 97 [38400/48000 (80%)]	Loss: 0.000480, KL fake Loss: 0.710207
Classification Train Epoch: 97 [44800/48000 (93%)]	Loss: 0.000102, KL fake Loss: 0.655182

Test set: Average loss: 0.3058, Accuracy: 7413/8000 (93%)

Classification Train Epoch: 98 [0/48000 (0%)]	Loss: 0.000431, KL fake Loss: 0.816616
Classification Train Epoch: 98 [6400/48000 (13%)]	Loss: 0.000061, KL fake Loss: 0.636204
Classification Train Epoch: 98 [12800/48000 (27%)]	Loss: 0.000063, KL fake Loss: 0.885270
Classification Train Epoch: 98 [19200/48000 (40%)]	Loss: 0.000061, KL fake Loss: 0.915734
Classification Train Epoch: 98 [25600/48000 (53%)]	Loss: 0.000422, KL fake Loss: 0.701598
Classification Train Epoch: 98 [32000/48000 (67%)]	Loss: 0.000060, KL fake Loss: 0.755756
Classification Train Epoch: 98 [38400/48000 (80%)]	Loss: 0.000045, KL fake Loss: 0.629986
Classification Train Epoch: 98 [44800/48000 (93%)]	Loss: 0.000087, KL fake Loss: 0.926894

Test set: Average loss: 0.3083, Accuracy: 7397/8000 (92%)

Classification Train Epoch: 99 [0/48000 (0%)]	Loss: 0.000118, KL fake Loss: 0.637920
Classification Train Epoch: 99 [6400/48000 (13%)]	Loss: 0.000129, KL fake Loss: 0.712308
Classification Train Epoch: 99 [12800/48000 (27%)]	Loss: 0.000271, KL fake Loss: 0.563759
Classification Train Epoch: 99 [19200/48000 (40%)]	Loss: 0.000104, KL fake Loss: 0.903584
Classification Train Epoch: 99 [25600/48000 (53%)]	Loss: 0.000134, KL fake Loss: 0.782865
Classification Train Epoch: 99 [32000/48000 (67%)]	Loss: 0.000658, KL fake Loss: 0.755889
Classification Train Epoch: 99 [38400/48000 (80%)]	Loss: 0.000060, KL fake Loss: 0.609313
Classification Train Epoch: 99 [44800/48000 (93%)]	Loss: 0.000025, KL fake Loss: 0.536624

Test set: Average loss: 0.3282, Accuracy: 7420/8000 (93%)

Classification Train Epoch: 100 [0/48000 (0%)]	Loss: 0.000414, KL fake Loss: 0.735299
Classification Train Epoch: 100 [6400/48000 (13%)]	Loss: 0.000141, KL fake Loss: 0.768936
Classification Train Epoch: 100 [12800/48000 (27%)]	Loss: 0.000022, KL fake Loss: 0.742357
Classification Train Epoch: 100 [19200/48000 (40%)]	Loss: 0.000158, KL fake Loss: 0.592933
Classification Train Epoch: 100 [25600/48000 (53%)]	Loss: 0.000076, KL fake Loss: 0.534788
Classification Train Epoch: 100 [32000/48000 (67%)]	Loss: 0.000342, KL fake Loss: 1.389641
Classification Train Epoch: 100 [38400/48000 (80%)]	Loss: 0.000108, KL fake Loss: 0.720036
Classification Train Epoch: 100 [44800/48000 (93%)]	Loss: 0.000154, KL fake Loss: 1.340575

Test set: Average loss: 0.3045, Accuracy: 7425/8000 (93%)

Namespace(batch_size=128, no_cuda=False, seed=1, dataset='FashionMNIST', dataroot='./data', imageSize=32, outf='./results/joint_confidence_loss/FM-0.001/', out_dataset='FashionMNIST', num_classes=8, num_channels=1, pre_trained_net='results/joint_confidence_loss/FM-0.001/model_epoch_100.pth')
Random Seed:  1
Load model
DenseNet3(
  (conv1): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock(
    (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock(
    (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(342, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (fc): Linear(in_features=342, out_features=8, bias=True)
  (softmax): Softmax(dim=1)
)ic| len(dset): 60000
ic| len(dset): 10000
ic| len(dset): 60000
ic| len(dset): 10000
 92%|█████████▏| 92/100 [5:25:26<28:17, 212.22s/it]
load target data:  FashionMNIST
load non target data:  FashionMNIST
generate log from in-distribution data

 Final Accuracy: 7425/8000 (92.81%)

generate log  from out-of-distribution data
calculate metrics
  Performance of Baseline detector
TNR at TPR 95%:            25.753%
TNR at TPR 99%:             7.906%
AUROC:                     68.437%
Detection acc:             67.744%
AUPR In:                   62.636%
AUPR Out:                  71.765%
 93%|█████████▎| 93/100 [5:28:58<24:45, 212.22s/it] 94%|█████████▍| 94/100 [5:32:30<21:13, 212.21s/it]Classification Train Epoch: 85 [6400/50000 (13%)]	Loss: 0.000067, KL fake Loss: 0.000053
Classification Train Epoch: 85 [12800/50000 (26%)]	Loss: 0.000029, KL fake Loss: 0.001554
Classification Train Epoch: 85 [19200/50000 (38%)]	Loss: 0.000064, KL fake Loss: 0.000022
Classification Train Epoch: 85 [25600/50000 (51%)]	Loss: 0.000240, KL fake Loss: 0.000410
Classification Train Epoch: 85 [32000/50000 (64%)]	Loss: 0.000071, KL fake Loss: 0.000044
Classification Train Epoch: 85 [38400/50000 (77%)]	Loss: 0.000196, KL fake Loss: 0.000132
Classification Train Epoch: 85 [44800/50000 (90%)]	Loss: 0.000019, KL fake Loss: 0.000078

Test set: Average loss: 2.1056, Accuracy: 3542/10000 (35%)

Classification Train Epoch: 86 [0/50000 (0%)]	Loss: 0.000086, KL fake Loss: 0.000161
Classification Train Epoch: 86 [6400/50000 (13%)]	Loss: 0.000059, KL fake Loss: 0.000032
Classification Train Epoch: 86 [12800/50000 (26%)]	Loss: 0.000043, KL fake Loss: 0.000146
Classification Train Epoch: 86 [19200/50000 (38%)]	Loss: 0.000009, KL fake Loss: 0.000289
Classification Train Epoch: 86 [25600/50000 (51%)]	Loss: 0.000088, KL fake Loss: 0.000009
Classification Train Epoch: 86 [32000/50000 (64%)]	Loss: 0.000026, KL fake Loss: 0.000076
Classification Train Epoch: 86 [38400/50000 (77%)]	Loss: 0.000041, KL fake Loss: 0.000030
Classification Train Epoch: 86 [44800/50000 (90%)]	Loss: 0.000019, KL fake Loss: 0.000056

Test set: Average loss: 1.9983, Accuracy: 3054/10000 (31%)

Classification Train Epoch: 87 [0/50000 (0%)]	Loss: 0.000047, KL fake Loss: 0.000394
Classification Train Epoch: 87 [6400/50000 (13%)]	Loss: 0.000026, KL fake Loss: 0.000277
Classification Train Epoch: 87 [12800/50000 (26%)]	Loss: 0.000033, KL fake Loss: 0.000114
Classification Train Epoch: 87 [19200/50000 (38%)]	Loss: 0.000036, KL fake Loss: 0.000199
Classification Train Epoch: 87 [25600/50000 (51%)]	Loss: 0.000070, KL fake Loss: 0.000036
Classification Train Epoch: 87 [32000/50000 (64%)]	Loss: 0.000067, KL fake Loss: 0.000041
Classification Train Epoch: 87 [38400/50000 (77%)]	Loss: 0.000489, KL fake Loss: 0.000435
Classification Train Epoch: 87 [44800/50000 (90%)]	Loss: 0.000041, KL fake Loss: 0.000038

Test set: Average loss: 2.0035, Accuracy: 2704/10000 (27%)

Classification Train Epoch: 88 [0/50000 (0%)]	Loss: 0.000006, KL fake Loss: 0.000120
Classification Train Epoch: 88 [6400/50000 (13%)]	Loss: 0.000020, KL fake Loss: 0.000011
Classification Train Epoch: 88 [12800/50000 (26%)]	Loss: 0.000035, KL fake Loss: 0.000454
Classification Train Epoch: 88 [19200/50000 (38%)]	Loss: 0.000012, KL fake Loss: 0.000157
Classification Train Epoch: 88 [25600/50000 (51%)]	Loss: 0.000003, KL fake Loss: 0.000022
Classification Train Epoch: 88 [32000/50000 (64%)]	Loss: 0.000302, KL fake Loss: 0.000022
Classification Train Epoch: 88 [38400/50000 (77%)]	Loss: 0.000340, KL fake Loss: 0.000045
Classification Train Epoch: 88 [44800/50000 (90%)]	Loss: 0.000020, KL fake Loss: 0.000042

Test set: Average loss: 2.1220, Accuracy: 2246/10000 (22%)

Classification Train Epoch: 89 [0/50000 (0%)]	Loss: 0.000031, KL fake Loss: 0.000077
Classification Train Epoch: 89 [6400/50000 (13%)]	Loss: 0.000114, KL fake Loss: 0.000461
Classification Train Epoch: 89 [12800/50000 (26%)]	Loss: 0.000008, KL fake Loss: 0.000054
Classification Train Epoch: 89 [19200/50000 (38%)]	Loss: 0.000045, KL fake Loss: 0.000042
Classification Train Epoch: 89 [25600/50000 (51%)]	Loss: 0.000081, KL fake Loss: 0.000027
Classification Train Epoch: 89 [32000/50000 (64%)]	Loss: 0.000005, KL fake Loss: 0.000092
Classification Train Epoch: 89 [38400/50000 (77%)]	Loss: 0.000117, KL fake Loss: 0.000010
Classification Train Epoch: 89 [44800/50000 (90%)]	Loss: 0.000026, KL fake Loss: 0.000675

Test set: Average loss: 2.1387, Accuracy: 2247/10000 (22%)

Classification Train Epoch: 90 [0/50000 (0%)]	Loss: 0.000030, KL fake Loss: 0.000051
Classification Train Epoch: 90 [6400/50000 (13%)]	Loss: 0.000348, KL fake Loss: 0.000229
Classification Train Epoch: 90 [12800/50000 (26%)]	Loss: 0.004618, KL fake Loss: 0.000051
Classification Train Epoch: 90 [19200/50000 (38%)]	Loss: 0.000145, KL fake Loss: 0.000608
Classification Train Epoch: 90 [25600/50000 (51%)]	Loss: 0.000013, KL fake Loss: 0.000028
Classification Train Epoch: 90 [32000/50000 (64%)]	Loss: 0.000015, KL fake Loss: 0.000176
Classification Train Epoch: 90 [38400/50000 (77%)]	Loss: 0.000158, KL fake Loss: 0.000034
Classification Train Epoch: 90 [44800/50000 (90%)]	Loss: 0.000063, KL fake Loss: 0.000022

Test set: Average loss: 2.1091, Accuracy: 2217/10000 (22%)

Classification Train Epoch: 91 [0/50000 (0%)]	Loss: 0.000240, KL fake Loss: 0.000257
Classification Train Epoch: 91 [6400/50000 (13%)]	Loss: 0.000022, KL fake Loss: 0.000036
Classification Train Epoch: 91 [12800/50000 (26%)]	Loss: 0.000031, KL fake Loss: 0.000336
Classification Train Epoch: 91 [19200/50000 (38%)]	Loss: 0.000040, KL fake Loss: 0.000025
Classification Train Epoch: 91 [25600/50000 (51%)]	Loss: 0.000067, KL fake Loss: 0.000277
Classification Train Epoch: 91 [32000/50000 (64%)]	Loss: 0.000021, KL fake Loss: 0.000016
Classification Train Epoch: 91 [38400/50000 (77%)]	Loss: 0.000009, KL fake Loss: 0.000071
Classification Train Epoch: 91 [44800/50000 (90%)]	Loss: 0.000092, KL fake Loss: 0.000081

Test set: Average loss: 2.1368, Accuracy: 2271/10000 (23%)

Classification Train Epoch: 92 [0/50000 (0%)]	Loss: 0.000384, KL fake Loss: 0.000265
Classification Train Epoch: 92 [6400/50000 (13%)]	Loss: 0.000063, KL fake Loss: 0.000023
Classification Train Epoch: 92 [12800/50000 (26%)]	Loss: 0.000022, KL fake Loss: 0.000019
Classification Train Epoch: 92 [19200/50000 (38%)]	Loss: 0.000075, KL fake Loss: 0.000008
Classification Train Epoch: 92 [25600/50000 (51%)]	Loss: 0.000015, KL fake Loss: 0.000032
Classification Train Epoch: 92 [32000/50000 (64%)]	Loss: 0.000545, KL fake Loss: 0.000165
Classification Train Epoch: 92 [38400/50000 (77%)]	Loss: 0.000005, KL fake Loss: 0.000141
Classification Train Epoch: 92 [44800/50000 (90%)]	Loss: 0.000018, KL fake Loss: 0.000072

Test set: Average loss: 2.0413, Accuracy: 2599/10000 (26%)

Classification Train Epoch: 93 [0/50000 (0%)]	Loss: 0.000002, KL fake Loss: 0.000065
Classification Train Epoch: 93 [6400/50000 (13%)]	Loss: 0.000123, KL fake Loss: 0.000008
Classification Train Epoch: 93 [12800/50000 (26%)]	Loss: 0.000025, KL fake Loss: 0.000030
Classification Train Epoch: 93 [19200/50000 (38%)]	Loss: 0.000045, KL fake Loss: 0.000073
Classification Train Epoch: 93 [25600/50000 (51%)]	Loss: 0.000034, KL fake Loss: 0.000402
Classification Train Epoch: 93 [32000/50000 (64%)]	Loss: 0.000066, KL fake Loss: 0.000193
Classification Train Epoch: 93 [38400/50000 (77%)]	Loss: 0.000079, KL fake Loss: 0.000103
Classification Train Epoch: 93 [44800/50000 (90%)]	Loss: 0.000015, KL fake Loss: 0.000015

Test set: Average loss: 2.1128, Accuracy: 2370/10000 (24%)

Classification Train Epoch: 94 [0/50000 (0%)]	Loss: 0.000008, KL fake Loss: 0.000030
Classification Train Epoch: 94 [6400/50000 (13%)]	Loss: 0.000077, KL fake Loss: 0.000219
Classification Train Epoch: 94 [12800/50000 (26%)]	Loss: 0.000011, KL fake Loss: 0.000037
Classification Train Epoch: 94 [19200/50000 (38%)]	Loss: 0.000009, KL fake Loss: 0.000137
Classification Train Epoch: 94 [25600/50000 (51%)]	Loss: 0.000015, KL fake Loss: 0.000350
Classification Train Epoch: 94 [32000/50000 (64%)]	Loss: 0.000123, KL fake Loss: 0.000049
Classification Train Epoch: 94 [38400/50000 (77%)]	Loss: 0.000189, KL fake Loss: 0.000043
Classification Train Epoch: 94 [44800/50000 (90%)]	Loss: 0.000546, KL fake Loss: 0.000356

Test set: Average loss: 2.1708, Accuracy: 2501/10000 (25%)

Classification Train Epoch: 95 [0/50000 (0%)]	Loss: 0.000189, KL fake Loss: 0.000600
Classification Train Epoch: 95 [6400/50000 (13%)]	Loss: 0.000039, KL fake Loss: 0.000157
Classification Train Epoch: 95 [12800/50000 (26%)]	Loss: 0.000021, KL fake Loss: 0.000163
Classification Train Epoch: 95 [19200/50000 (38%)]	Loss: 0.002838, KL fake Loss: 0.000014
Classification Train Epoch: 95 [25600/50000 (51%)]	Loss: 0.000029, KL fake Loss: 0.000041
 95%|█████████▌| 95/100 [5:36:03<17:41, 212.21s/it] 96%|█████████▌| 96/100 [5:39:35<14:08, 212.21s/it] 97%|█████████▋| 97/100 [5:43:07<10:36, 212.21s/it] 98%|█████████▊| 98/100 [5:46:39<07:04, 212.21s/it] 99%|█████████▉| 99/100 [5:50:11<03:32, 212.21s/it]100%|██████████| 100/100 [5:53:44<00:00, 212.24s/it]100%|██████████| 100/100 [5:53:44<00:00, 212.24s/it]
Classification Train Epoch: 95 [32000/50000 (64%)]	Loss: 0.000702, KL fake Loss: 0.000234
Classification Train Epoch: 95 [38400/50000 (77%)]	Loss: 0.000024, KL fake Loss: 0.000055
Classification Train Epoch: 95 [44800/50000 (90%)]	Loss: 0.000018, KL fake Loss: 0.000076

Test set: Average loss: 2.1000, Accuracy: 2670/10000 (27%)

Classification Train Epoch: 96 [0/50000 (0%)]	Loss: 0.000242, KL fake Loss: 0.000018
Classification Train Epoch: 96 [6400/50000 (13%)]	Loss: 0.000024, KL fake Loss: 0.000009
Classification Train Epoch: 96 [12800/50000 (26%)]	Loss: 0.000010, KL fake Loss: 0.000477
Classification Train Epoch: 96 [19200/50000 (38%)]	Loss: 0.000020, KL fake Loss: 0.000023
Classification Train Epoch: 96 [25600/50000 (51%)]	Loss: 0.000069, KL fake Loss: 0.000270
Classification Train Epoch: 96 [32000/50000 (64%)]	Loss: 0.000014, KL fake Loss: 0.000310
Classification Train Epoch: 96 [38400/50000 (77%)]	Loss: 0.000015, KL fake Loss: 0.000006
Classification Train Epoch: 96 [44800/50000 (90%)]	Loss: 0.000004, KL fake Loss: 0.000021

Test set: Average loss: 2.1338, Accuracy: 2559/10000 (26%)

Classification Train Epoch: 97 [0/50000 (0%)]	Loss: 0.000021, KL fake Loss: 0.000115
Classification Train Epoch: 97 [6400/50000 (13%)]	Loss: 0.000106, KL fake Loss: 0.000244
Classification Train Epoch: 97 [12800/50000 (26%)]	Loss: 0.000042, KL fake Loss: 0.000048
Classification Train Epoch: 97 [19200/50000 (38%)]	Loss: 0.000004, KL fake Loss: 0.000200
Classification Train Epoch: 97 [25600/50000 (51%)]	Loss: 0.000014, KL fake Loss: 0.000114
Classification Train Epoch: 97 [32000/50000 (64%)]	Loss: 0.000046, KL fake Loss: 0.000050
Classification Train Epoch: 97 [38400/50000 (77%)]	Loss: 0.000012, KL fake Loss: 0.000063
Classification Train Epoch: 97 [44800/50000 (90%)]	Loss: 0.000011, KL fake Loss: 0.000030

Test set: Average loss: 2.0398, Accuracy: 2622/10000 (26%)

Classification Train Epoch: 98 [0/50000 (0%)]	Loss: 0.000005, KL fake Loss: 0.063331
Classification Train Epoch: 98 [6400/50000 (13%)]	Loss: 0.000388, KL fake Loss: 0.000013
Classification Train Epoch: 98 [12800/50000 (26%)]	Loss: 0.000009, KL fake Loss: 0.000479
Classification Train Epoch: 98 [19200/50000 (38%)]	Loss: 0.000019, KL fake Loss: 0.000022
Classification Train Epoch: 98 [25600/50000 (51%)]	Loss: 0.000049, KL fake Loss: 0.000212
Classification Train Epoch: 98 [32000/50000 (64%)]	Loss: 0.000014, KL fake Loss: 0.000027
Classification Train Epoch: 98 [38400/50000 (77%)]	Loss: 0.000017, KL fake Loss: 0.000030
Classification Train Epoch: 98 [44800/50000 (90%)]	Loss: 0.000030, KL fake Loss: 0.000068

Test set: Average loss: 2.1082, Accuracy: 2351/10000 (24%)

Classification Train Epoch: 99 [0/50000 (0%)]	Loss: 0.000021, KL fake Loss: 0.000586
Classification Train Epoch: 99 [6400/50000 (13%)]	Loss: 0.000095, KL fake Loss: 0.000041
Classification Train Epoch: 99 [12800/50000 (26%)]	Loss: 0.000017, KL fake Loss: 0.000169
Classification Train Epoch: 99 [19200/50000 (38%)]	Loss: 0.000014, KL fake Loss: 0.000040
Classification Train Epoch: 99 [25600/50000 (51%)]	Loss: 0.000051, KL fake Loss: 0.000102
Classification Train Epoch: 99 [32000/50000 (64%)]	Loss: 0.000006, KL fake Loss: 0.002222
Classification Train Epoch: 99 [38400/50000 (77%)]	Loss: 0.000157, KL fake Loss: 0.000073
Classification Train Epoch: 99 [44800/50000 (90%)]	Loss: 0.000020, KL fake Loss: 0.000048

Test set: Average loss: 2.2070, Accuracy: 2990/10000 (30%)

Classification Train Epoch: 100 [0/50000 (0%)]	Loss: 0.000081, KL fake Loss: 0.000597
Classification Train Epoch: 100 [6400/50000 (13%)]	Loss: 0.001293, KL fake Loss: 0.000121
Classification Train Epoch: 100 [12800/50000 (26%)]	Loss: 0.000166, KL fake Loss: 0.000410
Classification Train Epoch: 100 [19200/50000 (38%)]	Loss: 0.000127, KL fake Loss: 0.000123
Classification Train Epoch: 100 [25600/50000 (51%)]	Loss: 0.000274, KL fake Loss: 0.000101
Classification Train Epoch: 100 [32000/50000 (64%)]	Loss: 0.000657, KL fake Loss: 0.000008
Classification Train Epoch: 100 [38400/50000 (77%)]	Loss: 0.000076, KL fake Loss: 0.000081
Classification Train Epoch: 100 [44800/50000 (90%)]	Loss: 0.000066, KL fake Loss: 0.000012

Test set: Average loss: 2.2218, Accuracy: 2182/10000 (22%)

Namespace(batch_size=128, no_cuda=False, seed=1, dataset='CIFAR10-SVHN', dataroot='./data', imageSize=32, outf='./results/joint_confidence_loss/CS-0.001/', out_dataset='CIFAR10-SVHN', num_classes=10, num_channels=3, pre_trained_net='results/joint_confidence_loss/CS-0.001/model_epoch_100.pth')
Random Seed:  1
Load model
DenseNet3(
  (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock(
    (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock(
    (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(342, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (fc): Linear(in_features=342, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)ic| len(dset): 73257
ic| len(dset): 73257

load target data:  CIFAR10-SVHN
Files already downloaded and verified
Files already downloaded and verified
Using downloaded and verified file: ./Datasets/SVHN/train_32x32.mat
Using downloaded and verified file: ./Datasets/SVHN/test_32x32.mat
load non target data:  CIFAR10-SVHN
Files already downloaded and verified
Files already downloaded and verified
Using downloaded and verified file: ./Datasets/SVHN/train_32x32.mat
Using downloaded and verified file: ./Datasets/SVHN/test_32x32.mat
generate log from in-distribution data

 Final Accuracy: 2182/10000 (21.82%)

generate log  from out-of-distribution data
calculate metrics
  Performance of Baseline detector
TNR at TPR 95%:             1.029%
TNR at TPR 99%:             0.068%
AUROC:                     61.337%
Detection acc:             61.863%
AUPR In:                   68.827%
AUPR Out:                  52.853%

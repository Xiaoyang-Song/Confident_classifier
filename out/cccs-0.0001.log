ic| len(dset): 60000
ic| len(dset): 10000
Namespace(batch_size=64, epochs=100, lr=0.001, no_cuda=False, seed=1, log_interval=100, dataset='FashionMNIST', dataroot='./data', imageSize=32, outf='./results/joint_confidence_loss/FM-0.0001/', wd=0.0, droprate=0.1, decreasing_lr='60', num_classes=8, beta=0.0001, num_channels=1)
Random Seed:  1
load InD data for Experiment:  FashionMNIST
Load model
DenseNet3(
  (conv1): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock(
    (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock(
    (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(342, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (fc): Linear(in_features=342, out_features=8, bias=True)
  (softmax): Softmax(dim=1)
)
load GAN
Setup optimizer
0.001
  0%|          | 0/100 [00:00<?, ?it/s]/home/xysong/.conda/envs/OoD/lib/python3.9/site-packages/torch/nn/functional.py:2904: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
  9%|▉         | 9/100 [32:00<5:23:36, 213.37s/it]  1%|          | 1/100 [02:44<4:30:43, 164.07s/it] 10%|█         | 10/100 [35:33<5:20:02, 213.36s/it]  2%|▏         | 2/100 [05:27<4:27:39, 163.88s/it]Classification Train Epoch: 1 [0/50000 (0%)]	Loss: 2.342744, KL fake Loss: 0.030272
Classification Train Epoch: 1 [6400/50000 (13%)]	Loss: 1.658363, KL fake Loss: 0.026435
Classification Train Epoch: 1 [12800/50000 (26%)]	Loss: 1.391027, KL fake Loss: 0.010289
Classification Train Epoch: 1 [19200/50000 (38%)]	Loss: 1.262624, KL fake Loss: 0.010073
Classification Train Epoch: 1 [25600/50000 (51%)]	Loss: 0.976649, KL fake Loss: 0.013532
Classification Train Epoch: 1 [32000/50000 (64%)]	Loss: 1.067955, KL fake Loss: 0.010392
Classification Train Epoch: 1 [38400/50000 (77%)]	Loss: 0.937168, KL fake Loss: 0.007235
Classification Train Epoch: 1 [44800/50000 (90%)]	Loss: 1.078756, KL fake Loss: 0.014482

Test set: Average loss: 3.6733, Accuracy: 3927/10000 (39%)

Classification Train Epoch: 2 [0/50000 (0%)]	Loss: 0.805602, KL fake Loss: 0.010055
Classification Train Epoch: 2 [6400/50000 (13%)]	Loss: 0.780424, KL fake Loss: 0.014885
Classification Train Epoch: 2 [12800/50000 (26%)]	Loss: 0.747287, KL fake Loss: 0.015935
Classification Train Epoch: 2 [19200/50000 (38%)]	Loss: 0.776130, KL fake Loss: 0.009209
Classification Train Epoch: 2 [25600/50000 (51%)]	Loss: 0.934251, KL fake Loss: 0.006184
Classification Train Epoch: 2 [32000/50000 (64%)]	Loss: 0.756762, KL fake Loss: 0.009538
Classification Train Epoch: 2 [38400/50000 (77%)]	Loss: 0.710673, KL fake Loss: 0.011806
Classification Train Epoch: 2 [44800/50000 (90%)]	Loss: 0.607508, KL fake Loss: 0.013355

Test set: Average loss: 2.5184, Accuracy: 4310/10000 (43%)

Classification Train Epoch: 3 [0/50000 (0%)]	Loss: 0.707013, KL fake Loss: 0.003168
Classification Train Epoch: 3 [6400/50000 (13%)]	Loss: 0.659063, KL fake Loss: 0.010139
Classification Train Epoch: 3 [12800/50000 (26%)]	Loss: 0.566108, KL fake Loss: 0.003970
Classification Train Epoch: 3 [19200/50000 (38%)]	Loss: 0.518153, KL fake Loss: 0.002846
Classification Train Epoch: 3 [25600/50000 (51%)]	Loss: 0.624588, KL fake Loss: 0.006802
Classification Train Epoch: 3 [32000/50000 (64%)]	Loss: 0.737588, KL fake Loss: 0.002666
Classification Train Epoch: 3 [38400/50000 (77%)]	Loss: 0.433088, KL fake Loss: 0.004527
Classification Train Epoch: 3 [44800/50000 (90%)]	Loss: 0.441441, KL fake Loss: 0.003227

Test set: Average loss: 2.4624, Accuracy: 4671/10000 (47%)

Classification Train Epoch: 4 [0/50000 (0%)]	Loss: 0.416944, KL fake Loss: 0.004605
Classification Train Epoch: 4 [6400/50000 (13%)]	Loss: 0.484695, KL fake Loss: 0.004646
Classification Train Epoch: 4 [12800/50000 (26%)]	Loss: 0.467329, KL fake Loss: 0.002427
Classification Train Epoch: 4 [19200/50000 (38%)]	Loss: 0.544536, KL fake Loss: 0.004907
Classification Train Epoch: 4 [25600/50000 (51%)]	Loss: 0.555062, KL fake Loss: 0.003023
Classification Train Epoch: 4 [32000/50000 (64%)]	Loss: 0.366848, KL fake Loss: 0.001750
Classification Train Epoch: 4 [38400/50000 (77%)]	Loss: 0.425817, KL fake Loss: 0.013059
Classification Train Epoch: 4 [44800/50000 (90%)]	Loss: 0.197359, KL fake Loss: 0.002376

Test set: Average loss: 1.4384, Accuracy: 6234/10000 (62%)

Classification Train Epoch: 5 [0/50000 (0%)]	Loss: 0.273323, KL fake Loss: 0.004499
Classification Train Epoch: 5 [6400/50000 (13%)]	Loss: 0.267584, KL fake Loss: 0.002182
Classification Train Epoch: 5 [12800/50000 (26%)]	Loss: 0.395610, KL fake Loss: 0.003002
Classification Train Epoch: 5 [19200/50000 (38%)]	Loss: 0.423073, KL fake Loss: 0.002988
Classification Train Epoch: 5 [25600/50000 (51%)]	Loss: 0.364211, KL fake Loss: 0.003409
Classification Train Epoch: 5 [32000/50000 (64%)]	Loss: 0.362711, KL fake Loss: 0.003931
Classification Train Epoch: 5 [38400/50000 (77%)]	Loss: 0.359893, KL fake Loss: 0.003139
Classification Train Epoch: 5 [44800/50000 (90%)]	Loss: 0.360874, KL fake Loss: 0.001843

Test set: Average loss: 2.9900, Accuracy: 4860/10000 (49%)

Classification Train Epoch: 6 [0/50000 (0%)]	Loss: 0.208908, KL fake Loss: 0.001162
Classification Train Epoch: 6 [6400/50000 (13%)]	Loss: 0.201299, KL fake Loss: 0.000839
Classification Train Epoch: 6 [12800/50000 (26%)]	Loss: 0.509423, KL fake Loss: 0.002475
Classification Train Epoch: 6 [19200/50000 (38%)]	Loss: 0.266374, KL fake Loss: 0.001590
Classification Train Epoch: 6 [25600/50000 (51%)]	Loss: 0.380618, KL fake Loss: 0.001521
Classification Train Epoch: 6 [32000/50000 (64%)]	Loss: 0.443742, KL fake Loss: 0.008643
Classification Train Epoch: 6 [38400/50000 (77%)]	Loss: 0.250192, KL fake Loss: 0.004290
Classification Train Epoch: 6 [44800/50000 (90%)]	Loss: 0.383077, KL fake Loss: 0.003409

Test set: Average loss: 2.5240, Accuracy: 5376/10000 (54%)

Classification Train Epoch: 7 [0/50000 (0%)]	Loss: 0.200571, KL fake Loss: 0.002065
Classification Train Epoch: 7 [6400/50000 (13%)]	Loss: 0.248680, KL fake Loss: 0.002637
Classification Train Epoch: 7 [12800/50000 (26%)]	Loss: 0.109648, KL fake Loss: 0.001340
Classification Train Epoch: 7 [19200/50000 (38%)]	Loss: 0.210167, KL fake Loss: 0.032167
Classification Train Epoch: 7 [25600/50000 (51%)]	Loss: 0.419279, KL fake Loss: 0.018579
Classification Train Epoch: 7 [32000/50000 (64%)]	Loss: 0.209454, KL fake Loss: 0.022160
Classification Train Epoch: 7 [38400/50000 (77%)]	Loss: 0.291927, KL fake Loss: 0.014265
Classification Train Epoch: 7 [44800/50000 (90%)]	Loss: 0.220433, KL fake Loss: 0.013281

Test set: Average loss: 3.2507, Accuracy: 5315/10000 (53%)

Classification Train Epoch: 8 [0/50000 (0%)]	Loss: 0.195783, KL fake Loss: 0.025787
Classification Train Epoch: 8 [6400/50000 (13%)]	Loss: 0.096671, KL fake Loss: 0.005304
Classification Train Epoch: 8 [12800/50000 (26%)]	Loss: 0.092051, KL fake Loss: 0.010250
Classification Train Epoch: 8 [19200/50000 (38%)]	Loss: 0.253654, KL fake Loss: 0.014483
Classification Train Epoch: 8 [25600/50000 (51%)]	Loss: 0.268017, KL fake Loss: 0.021344
Classification Train Epoch: 8 [32000/50000 (64%)]	Loss: 0.351314, KL fake Loss: 0.016908
Classification Train Epoch: 8 [38400/50000 (77%)]	Loss: 0.279846, KL fake Loss: 0.010177
Classification Train Epoch: 8 [44800/50000 (90%)]	Loss: 0.274358, KL fake Loss: 0.012230

Test set: Average loss: 7.3295, Accuracy: 3735/10000 (37%)

Classification Train Epoch: 9 [0/50000 (0%)]	Loss: 0.353379, KL fake Loss: 0.013229
Classification Train Epoch: 9 [6400/50000 (13%)]	Loss: 0.097596, KL fake Loss: 0.013510
Classification Train Epoch: 9 [12800/50000 (26%)]	Loss: 0.118285, KL fake Loss: 0.007804
Classification Train Epoch: 9 [19200/50000 (38%)]	Loss: 0.244560, KL fake Loss: 0.020977
Classification Train Epoch: 9 [25600/50000 (51%)]	Loss: 0.302333, KL fake Loss: 0.016592
Classification Train Epoch: 9 [32000/50000 (64%)]	Loss: 0.178911, KL fake Loss: 0.010586
Classification Train Epoch: 9 [38400/50000 (77%)]	Loss: 0.189726, KL fake Loss: 0.011934
Classification Train Epoch: 9 [44800/50000 (90%)]	Loss: 0.181727, KL fake Loss: 0.014272

Test set: Average loss: 6.5753, Accuracy: 3814/10000 (38%)

Classification Train Epoch: 10 [0/50000 (0%)]	Loss: 0.088532, KL fake Loss: 0.011004
Classification Train Epoch: 10 [6400/50000 (13%)]	Loss: 0.123847, KL fake Loss: 0.010441
Classification Train Epoch: 10 [12800/50000 (26%)]	Loss: 0.159915, KL fake Loss: 0.007240
Classification Train Epoch: 10 [19200/50000 (38%)]	Loss: 0.164946, KL fake Loss: 0.004137
Classification Train Epoch: 10 [25600/50000 (51%)]	Loss: 0.123221, KL fake Loss: 0.009039
Classification Train Epoch: 10 [32000/50000 (64%)]	Loss: 0.090910, KL fake Loss: 0.015535
Classification Train Epoch: 10 [38400/50000 (77%)]	Loss: 0.193534, KL fake Loss: 0.016772
Classification Train Epoch: 10 [44800/50000 (90%)]	Loss: 0.201079, KL fake Loss: 0.012734

Test set: Average loss: 4.7909, Accuracy: 4273/10000 (43%)

Classification Train Epoch: 11 [0/50000 (0%)]	Loss: 0.245804, KL fake Loss: 0.011026
Classification Train Epoch: 11 [6400/50000 (13%)]	Loss: 0.178757, KL fake Loss: 0.011484
Classification Train Epoch: 11 [12800/50000 (26%)]	Loss: 0.204999, KL fake Loss: 0.006811
Classification Train Epoch: 11 [19200/50000 (38%)]	Loss: 0.075916, KL fake Loss: 0.002459
Classification Train Epoch: 11 [25600/50000 (51%)]	Loss: 0.197631, KL fake Loss: 0.011684
 11%|█         | 11/100 [39:07<5:16:28, 213.35s/it]  3%|▎         | 3/100 [08:11<4:24:51, 163.83s/it] 12%|█▏        | 12/100 [42:40<5:12:54, 213.35s/it]  4%|▍         | 4/100 [10:55<4:22:05, 163.81s/it]  5%|▌         | 5/100 [13:39<4:19:21, 163.80s/it] 13%|█▎        | 13/100 [46:13<5:09:20, 213.34s/it]  6%|▌         | 6/100 [16:22<4:16:37, 163.80s/it] 14%|█▍        | 14/100 [49:47<5:05:46, 213.34s/it]  7%|▋         | 7/100 [19:06<4:13:53, 163.80s/it] 15%|█▌        | 15/100 [53:20<5:02:12, 213.33s/it]  8%|▊         | 8/100 [21:50<4:11:09, 163.79s/it]  9%|▉         | 9/100 [24:34<4:08:25, 163.80s/it] 16%|█▌        | 16/100 [56:53<4:58:38, 213.32s/it] 10%|█         | 10/100 [27:18<4:05:41, 163.79s/it] 17%|█▋        | 17/100 [1:00:27<4:55:05, 213.32s/it]Classification Train Epoch: 1 [0/48000 (0%)]	Loss: 2.131171, KL fake Loss: 0.036355
Classification Train Epoch: 1 [6400/48000 (13%)]	Loss: 0.551974, KL fake Loss: 1.326861
Classification Train Epoch: 1 [12800/48000 (27%)]	Loss: 0.400628, KL fake Loss: 2.110529
Classification Train Epoch: 1 [19200/48000 (40%)]	Loss: 0.490941, KL fake Loss: 2.665861
Classification Train Epoch: 1 [25600/48000 (53%)]	Loss: 0.257487, KL fake Loss: 3.033288
Classification Train Epoch: 1 [32000/48000 (67%)]	Loss: 0.238601, KL fake Loss: 3.435099
Classification Train Epoch: 1 [38400/48000 (80%)]	Loss: 0.304904, KL fake Loss: 3.171142
Classification Train Epoch: 1 [44800/48000 (93%)]	Loss: 0.358292, KL fake Loss: 3.231414

Test set: Average loss: 0.3686, Accuracy: 6956/8000 (87%)

Classification Train Epoch: 2 [0/48000 (0%)]	Loss: 0.384040, KL fake Loss: 3.498661
Classification Train Epoch: 2 [6400/48000 (13%)]	Loss: 0.237164, KL fake Loss: 4.385677
Classification Train Epoch: 2 [12800/48000 (27%)]	Loss: 0.193040, KL fake Loss: 3.848152
Classification Train Epoch: 2 [19200/48000 (40%)]	Loss: 0.393396, KL fake Loss: 4.003160
Classification Train Epoch: 2 [25600/48000 (53%)]	Loss: 0.172596, KL fake Loss: 4.108061
Classification Train Epoch: 2 [32000/48000 (67%)]	Loss: 0.198031, KL fake Loss: 4.174922
Classification Train Epoch: 2 [38400/48000 (80%)]	Loss: 0.083882, KL fake Loss: 4.460194
Classification Train Epoch: 2 [44800/48000 (93%)]	Loss: 0.277307, KL fake Loss: 4.307420

Test set: Average loss: 0.3204, Accuracy: 7098/8000 (89%)

Classification Train Epoch: 3 [0/48000 (0%)]	Loss: 0.291831, KL fake Loss: 4.737352
Classification Train Epoch: 3 [6400/48000 (13%)]	Loss: 0.285034, KL fake Loss: 4.625959
Classification Train Epoch: 3 [12800/48000 (27%)]	Loss: 0.202680, KL fake Loss: 5.218409
Classification Train Epoch: 3 [19200/48000 (40%)]	Loss: 0.344562, KL fake Loss: 4.759920
Classification Train Epoch: 3 [25600/48000 (53%)]	Loss: 0.265313, KL fake Loss: 5.027951
Classification Train Epoch: 3 [32000/48000 (67%)]	Loss: 0.241545, KL fake Loss: 4.972546
Classification Train Epoch: 3 [38400/48000 (80%)]	Loss: 0.312894, KL fake Loss: 4.660205
Classification Train Epoch: 3 [44800/48000 (93%)]	Loss: 0.139953, KL fake Loss: 4.842425

Test set: Average loss: 0.2634, Accuracy: 7293/8000 (91%)

Classification Train Epoch: 4 [0/48000 (0%)]	Loss: 0.286652, KL fake Loss: 5.055404
Classification Train Epoch: 4 [6400/48000 (13%)]	Loss: 0.161981, KL fake Loss: 4.983957
Classification Train Epoch: 4 [12800/48000 (27%)]	Loss: 0.196731, KL fake Loss: 5.007549
Classification Train Epoch: 4 [19200/48000 (40%)]	Loss: 0.136394, KL fake Loss: 5.434258
Classification Train Epoch: 4 [25600/48000 (53%)]	Loss: 0.252689, KL fake Loss: 5.487899
Classification Train Epoch: 4 [32000/48000 (67%)]	Loss: 0.285443, KL fake Loss: 5.233150
Classification Train Epoch: 4 [38400/48000 (80%)]	Loss: 0.107192, KL fake Loss: 5.339097
Classification Train Epoch: 4 [44800/48000 (93%)]	Loss: 0.210426, KL fake Loss: 5.243303

Test set: Average loss: 0.2356, Accuracy: 7348/8000 (92%)

Classification Train Epoch: 5 [0/48000 (0%)]	Loss: 0.169975, KL fake Loss: 5.503917
Classification Train Epoch: 5 [6400/48000 (13%)]	Loss: 0.173645, KL fake Loss: 5.409343
Classification Train Epoch: 5 [12800/48000 (27%)]	Loss: 0.219201, KL fake Loss: 5.849247
Classification Train Epoch: 5 [19200/48000 (40%)]	Loss: 0.250420, KL fake Loss: 5.515075
Classification Train Epoch: 5 [25600/48000 (53%)]	Loss: 0.165580, KL fake Loss: 6.082999
Classification Train Epoch: 5 [32000/48000 (67%)]	Loss: 0.303985, KL fake Loss: 5.871128
Classification Train Epoch: 5 [38400/48000 (80%)]	Loss: 0.091245, KL fake Loss: 5.876637
Classification Train Epoch: 5 [44800/48000 (93%)]	Loss: 0.246622, KL fake Loss: 5.994871

Test set: Average loss: 0.2557, Accuracy: 7276/8000 (91%)

Classification Train Epoch: 6 [0/48000 (0%)]	Loss: 0.208266, KL fake Loss: 5.984101
Classification Train Epoch: 6 [6400/48000 (13%)]	Loss: 0.149918, KL fake Loss: 6.045743
Classification Train Epoch: 6 [12800/48000 (27%)]	Loss: 0.242888, KL fake Loss: 6.155079
Classification Train Epoch: 6 [19200/48000 (40%)]	Loss: 0.081184, KL fake Loss: 6.078866
Classification Train Epoch: 6 [25600/48000 (53%)]	Loss: 0.133061, KL fake Loss: 6.334990
Classification Train Epoch: 6 [32000/48000 (67%)]	Loss: 0.262329, KL fake Loss: 6.842563
Classification Train Epoch: 6 [38400/48000 (80%)]	Loss: 0.274597, KL fake Loss: 5.994778
Classification Train Epoch: 6 [44800/48000 (93%)]	Loss: 0.115385, KL fake Loss: 6.635917

Test set: Average loss: 0.2206, Accuracy: 7376/8000 (92%)

Classification Train Epoch: 7 [0/48000 (0%)]	Loss: 0.208936, KL fake Loss: 6.020595
Classification Train Epoch: 7 [6400/48000 (13%)]	Loss: 0.113936, KL fake Loss: 6.331391
Classification Train Epoch: 7 [12800/48000 (27%)]	Loss: 0.106817, KL fake Loss: 6.518326
Classification Train Epoch: 7 [19200/48000 (40%)]	Loss: 0.289146, KL fake Loss: 5.779381
Classification Train Epoch: 7 [25600/48000 (53%)]	Loss: 0.087776, KL fake Loss: 6.456240
Classification Train Epoch: 7 [32000/48000 (67%)]	Loss: 0.054931, KL fake Loss: 6.145601
Classification Train Epoch: 7 [38400/48000 (80%)]	Loss: 0.299514, KL fake Loss: 6.642560
Classification Train Epoch: 7 [44800/48000 (93%)]	Loss: 0.123152, KL fake Loss: 6.375216

Test set: Average loss: 0.2474, Accuracy: 7342/8000 (92%)

Classification Train Epoch: 8 [0/48000 (0%)]	Loss: 0.189564, KL fake Loss: 6.641317
Classification Train Epoch: 8 [6400/48000 (13%)]	Loss: 0.207968, KL fake Loss: 6.273972
Classification Train Epoch: 8 [12800/48000 (27%)]	Loss: 0.144486, KL fake Loss: 6.657438
Classification Train Epoch: 8 [19200/48000 (40%)]	Loss: 0.118479, KL fake Loss: 6.724813
Classification Train Epoch: 8 [25600/48000 (53%)]	Loss: 0.121654, KL fake Loss: 6.795976
Classification Train Epoch: 8 [32000/48000 (67%)]	Loss: 0.193045, KL fake Loss: 7.227954
Classification Train Epoch: 8 [38400/48000 (80%)]	Loss: 0.117186, KL fake Loss: 6.803099
Classification Train Epoch: 8 [44800/48000 (93%)]	Loss: 0.091367, KL fake Loss: 7.032513

Test set: Average loss: 0.2243, Accuracy: 7385/8000 (92%)

Classification Train Epoch: 9 [0/48000 (0%)]	Loss: 0.063599, KL fake Loss: 7.059602
Classification Train Epoch: 9 [6400/48000 (13%)]	Loss: 0.296058, KL fake Loss: 7.267660
Classification Train Epoch: 9 [12800/48000 (27%)]	Loss: 0.139980, KL fake Loss: 6.381136
Classification Train Epoch: 9 [19200/48000 (40%)]	Loss: 0.116776, KL fake Loss: 7.802773
Classification Train Epoch: 9 [25600/48000 (53%)]	Loss: 0.146663, KL fake Loss: 7.095735
Classification Train Epoch: 9 [32000/48000 (67%)]	Loss: 0.164315, KL fake Loss: 7.212722
Classification Train Epoch: 9 [38400/48000 (80%)]	Loss: 0.103015, KL fake Loss: 7.061898
Classification Train Epoch: 9 [44800/48000 (93%)]	Loss: 0.078409, KL fake Loss: 7.466418

Test set: Average loss: 0.2351, Accuracy: 7408/8000 (93%)

Classification Train Epoch: 10 [0/48000 (0%)]	Loss: 0.179351, KL fake Loss: 7.208632
Classification Train Epoch: 10 [6400/48000 (13%)]	Loss: 0.058330, KL fake Loss: 7.158080
Classification Train Epoch: 10 [12800/48000 (27%)]	Loss: 0.140768, KL fake Loss: 7.348297
Classification Train Epoch: 10 [19200/48000 (40%)]	Loss: 0.072434, KL fake Loss: 7.957420
Classification Train Epoch: 10 [25600/48000 (53%)]	Loss: 0.197993, KL fake Loss: 6.859100
Classification Train Epoch: 10 [32000/48000 (67%)]	Loss: 0.247593, KL fake Loss: 7.299333
Classification Train Epoch: 10 [38400/48000 (80%)]	Loss: 0.098147, KL fake Loss: 7.136283
Classification Train Epoch: 10 [44800/48000 (93%)]	Loss: 0.166757, KL fake Loss: 7.585628

Test set: Average loss: 0.2514, Accuracy: 7382/8000 (92%)

Classification Train Epoch: 11 [0/48000 (0%)]	Loss: 0.223156, KL fake Loss: 7.899372
Classification Train Epoch: 11 [6400/48000 (13%)]	Loss: 0.063696, KL fake Loss: 7.642683
Classification Train Epoch: 11 [12800/48000 (27%)]	Loss: 0.053988, KL fake Loss: 7.709186
Classification Train Epoch: 11 [19200/48000 (40%)]	Loss: 0.087857, KL fake Loss: 8.120660
Classification Train Epoch: 11 [25600/48000 (53%)]	Loss: 0.030359, KL fake Loss: 7.112175
 11%|█         | 11/100 [30:01<4:02:57, 163.79s/it] 18%|█▊        | 18/100 [1:04:00<4:51:31, 213.32s/it] 12%|█▏        | 12/100 [32:45<4:00:14, 163.80s/it] 13%|█▎        | 13/100 [35:29<3:57:29, 163.79s/it] 19%|█▉        | 19/100 [1:07:33<4:47:58, 213.31s/it] 14%|█▍        | 14/100 [38:13<3:54:46, 163.80s/it] 20%|██        | 20/100 [1:11:07<4:44:26, 213.33s/it] 15%|█▌        | 15/100 [40:57<3:52:02, 163.80s/it] 21%|██        | 21/100 [1:14:40<4:40:53, 213.33s/it]Classification Train Epoch: 11 [32000/50000 (64%)]	Loss: 0.240102, KL fake Loss: 0.006969
Classification Train Epoch: 11 [38400/50000 (77%)]	Loss: 0.176274, KL fake Loss: 0.020207
Classification Train Epoch: 11 [44800/50000 (90%)]	Loss: 0.218628, KL fake Loss: 0.023825

Test set: Average loss: 1.9834, Accuracy: 6604/10000 (66%)

Classification Train Epoch: 12 [0/50000 (0%)]	Loss: 0.080176, KL fake Loss: 0.008441
Classification Train Epoch: 12 [6400/50000 (13%)]	Loss: 0.140095, KL fake Loss: 0.010179
Classification Train Epoch: 12 [12800/50000 (26%)]	Loss: 0.061334, KL fake Loss: 0.008442
Classification Train Epoch: 12 [19200/50000 (38%)]	Loss: 0.124106, KL fake Loss: 0.019429
Classification Train Epoch: 12 [25600/50000 (51%)]	Loss: 0.259079, KL fake Loss: 0.005088
Classification Train Epoch: 12 [32000/50000 (64%)]	Loss: 0.358113, KL fake Loss: 0.007181
Classification Train Epoch: 12 [38400/50000 (77%)]	Loss: 0.263637, KL fake Loss: 0.025133
Classification Train Epoch: 12 [44800/50000 (90%)]	Loss: 0.090316, KL fake Loss: 0.007094

Test set: Average loss: 2.5408, Accuracy: 6249/10000 (62%)

Classification Train Epoch: 13 [0/50000 (0%)]	Loss: 0.156905, KL fake Loss: 0.025022
Classification Train Epoch: 13 [6400/50000 (13%)]	Loss: 0.173899, KL fake Loss: 0.006876
Classification Train Epoch: 13 [12800/50000 (26%)]	Loss: 0.122264, KL fake Loss: 0.003948
Classification Train Epoch: 13 [19200/50000 (38%)]	Loss: 0.069716, KL fake Loss: 0.003636
Classification Train Epoch: 13 [25600/50000 (51%)]	Loss: 0.138162, KL fake Loss: 0.004935
Classification Train Epoch: 13 [32000/50000 (64%)]	Loss: 0.086495, KL fake Loss: 0.019087
Classification Train Epoch: 13 [38400/50000 (77%)]	Loss: 0.132153, KL fake Loss: 0.010332
Classification Train Epoch: 13 [44800/50000 (90%)]	Loss: 0.040253, KL fake Loss: 0.006918

Test set: Average loss: 3.0645, Accuracy: 5869/10000 (59%)

Classification Train Epoch: 14 [0/50000 (0%)]	Loss: 0.030955, KL fake Loss: 0.002998
Classification Train Epoch: 14 [6400/50000 (13%)]	Loss: 0.102637, KL fake Loss: 0.011860
Classification Train Epoch: 14 [12800/50000 (26%)]	Loss: 0.094424, KL fake Loss: 0.010257
Classification Train Epoch: 14 [19200/50000 (38%)]	Loss: 0.100759, KL fake Loss: 0.014734
Classification Train Epoch: 14 [25600/50000 (51%)]	Loss: 0.086911, KL fake Loss: 0.014395
Classification Train Epoch: 14 [32000/50000 (64%)]	Loss: 0.147548, KL fake Loss: 0.006104
Classification Train Epoch: 14 [38400/50000 (77%)]	Loss: 0.094174, KL fake Loss: 0.013675
Classification Train Epoch: 14 [44800/50000 (90%)]	Loss: 0.037922, KL fake Loss: 0.001658

Test set: Average loss: 4.0897, Accuracy: 5542/10000 (55%)

Classification Train Epoch: 15 [0/50000 (0%)]	Loss: 0.178532, KL fake Loss: 0.003625
Classification Train Epoch: 15 [6400/50000 (13%)]	Loss: 0.082350, KL fake Loss: 0.002410
Classification Train Epoch: 15 [12800/50000 (26%)]	Loss: 0.154953, KL fake Loss: 0.002107
Classification Train Epoch: 15 [19200/50000 (38%)]	Loss: 0.143618, KL fake Loss: 0.001447
Classification Train Epoch: 15 [25600/50000 (51%)]	Loss: 0.035579, KL fake Loss: 0.002932
Classification Train Epoch: 15 [32000/50000 (64%)]	Loss: 0.055597, KL fake Loss: 0.005988
Classification Train Epoch: 15 [38400/50000 (77%)]	Loss: 0.029047, KL fake Loss: 0.002988
Classification Train Epoch: 15 [44800/50000 (90%)]	Loss: 0.113566, KL fake Loss: 0.004211

Test set: Average loss: 2.1371, Accuracy: 6698/10000 (67%)

Classification Train Epoch: 16 [0/50000 (0%)]	Loss: 0.097150, KL fake Loss: 0.003758
Classification Train Epoch: 16 [6400/50000 (13%)]	Loss: 0.033199, KL fake Loss: 0.000767
Classification Train Epoch: 16 [12800/50000 (26%)]	Loss: 0.042846, KL fake Loss: 0.006488
Classification Train Epoch: 16 [19200/50000 (38%)]	Loss: 0.056038, KL fake Loss: 0.010176
Classification Train Epoch: 16 [25600/50000 (51%)]	Loss: 0.049272, KL fake Loss: 0.006384
Classification Train Epoch: 16 [32000/50000 (64%)]	Loss: 0.123036, KL fake Loss: 0.005822
Classification Train Epoch: 16 [38400/50000 (77%)]	Loss: 0.178369, KL fake Loss: 0.005262
Classification Train Epoch: 16 [44800/50000 (90%)]	Loss: 0.092941, KL fake Loss: 0.000929

Test set: Average loss: 3.0075, Accuracy: 6217/10000 (62%)

Classification Train Epoch: 17 [0/50000 (0%)]	Loss: 0.112859, KL fake Loss: 0.012207
Classification Train Epoch: 17 [6400/50000 (13%)]	Loss: 0.026161, KL fake Loss: 0.003762
Classification Train Epoch: 17 [12800/50000 (26%)]	Loss: 0.017578, KL fake Loss: 0.003792
Classification Train Epoch: 17 [19200/50000 (38%)]	Loss: 0.052288, KL fake Loss: 0.001836
Classification Train Epoch: 17 [25600/50000 (51%)]	Loss: 0.059468, KL fake Loss: 0.003248
Classification Train Epoch: 17 [32000/50000 (64%)]	Loss: 0.077280, KL fake Loss: 0.006613
Classification Train Epoch: 17 [38400/50000 (77%)]	Loss: 0.145465, KL fake Loss: 0.007097
Classification Train Epoch: 17 [44800/50000 (90%)]	Loss: 0.120960, KL fake Loss: 0.013222

Test set: Average loss: 4.2057, Accuracy: 5820/10000 (58%)

Classification Train Epoch: 18 [0/50000 (0%)]	Loss: 0.083551, KL fake Loss: 0.001061
Classification Train Epoch: 18 [6400/50000 (13%)]	Loss: 0.060510, KL fake Loss: 0.001203
Classification Train Epoch: 18 [12800/50000 (26%)]	Loss: 0.076319, KL fake Loss: 0.001090
Classification Train Epoch: 18 [19200/50000 (38%)]	Loss: 0.097317, KL fake Loss: 0.009521
Classification Train Epoch: 18 [25600/50000 (51%)]	Loss: 0.142131, KL fake Loss: 0.002512
Classification Train Epoch: 18 [32000/50000 (64%)]	Loss: 0.135007, KL fake Loss: 0.001462
Classification Train Epoch: 18 [38400/50000 (77%)]	Loss: 0.061278, KL fake Loss: 0.001053
Classification Train Epoch: 18 [44800/50000 (90%)]	Loss: 0.056913, KL fake Loss: 0.008919

Test set: Average loss: 2.8102, Accuracy: 6184/10000 (62%)

Classification Train Epoch: 19 [0/50000 (0%)]	Loss: 0.140621, KL fake Loss: 0.003442
Classification Train Epoch: 19 [6400/50000 (13%)]	Loss: 0.114080, KL fake Loss: 0.005503
Classification Train Epoch: 19 [12800/50000 (26%)]	Loss: 0.105065, KL fake Loss: 0.000703
Classification Train Epoch: 19 [19200/50000 (38%)]	Loss: 0.073757, KL fake Loss: 0.002518
Classification Train Epoch: 19 [25600/50000 (51%)]	Loss: 0.060825, KL fake Loss: 0.002238
Classification Train Epoch: 19 [32000/50000 (64%)]	Loss: 0.080655, KL fake Loss: 0.008341
Classification Train Epoch: 19 [38400/50000 (77%)]	Loss: 0.021744, KL fake Loss: 0.005104
Classification Train Epoch: 19 [44800/50000 (90%)]	Loss: 0.083931, KL fake Loss: 0.008112

Test set: Average loss: 3.1175, Accuracy: 6267/10000 (63%)

Classification Train Epoch: 20 [0/50000 (0%)]	Loss: 0.067084, KL fake Loss: 0.005101
Classification Train Epoch: 20 [6400/50000 (13%)]	Loss: 0.084426, KL fake Loss: 0.001210
Classification Train Epoch: 20 [12800/50000 (26%)]	Loss: 0.095835, KL fake Loss: 0.001398
Classification Train Epoch: 20 [19200/50000 (38%)]	Loss: 0.045981, KL fake Loss: 0.001474
Classification Train Epoch: 20 [25600/50000 (51%)]	Loss: 0.025918, KL fake Loss: 0.005080
Classification Train Epoch: 20 [32000/50000 (64%)]	Loss: 0.028737, KL fake Loss: 0.001293
Classification Train Epoch: 20 [38400/50000 (77%)]	Loss: 0.045032, KL fake Loss: 0.010974
Classification Train Epoch: 20 [44800/50000 (90%)]	Loss: 0.037445, KL fake Loss: 0.010130

Test set: Average loss: 2.9586, Accuracy: 6322/10000 (63%)

Classification Train Epoch: 21 [0/50000 (0%)]	Loss: 0.025791, KL fake Loss: 0.004220
Classification Train Epoch: 21 [6400/50000 (13%)]	Loss: 0.005605, KL fake Loss: 0.002540
Classification Train Epoch: 21 [12800/50000 (26%)]	Loss: 0.033036, KL fake Loss: 0.002759
Classification Train Epoch: 21 [19200/50000 (38%)]	Loss: 0.081500, KL fake Loss: 0.006229
Classification Train Epoch: 21 [25600/50000 (51%)]	Loss: 0.085447, KL fake Loss: 0.032384
Classification Train Epoch: 21 [32000/50000 (64%)]	Loss: 0.133662, KL fake Loss: 0.005792
Classification Train Epoch: 21 [38400/50000 (77%)]	Loss: 0.029931, KL fake Loss: 0.017866
Classification Train Epoch: 21 [44800/50000 (90%)]	Loss: 0.069831, KL fake Loss: 0.011786

Test set: Average loss: 3.2636, Accuracy: 5977/10000 (60%)

Classification Train Epoch: 22 [0/50000 (0%)]	Loss: 0.027513, KL fake Loss: 0.023099
 16%|█▌        | 16/100 [43:40<3:49:19, 163.80s/it] 22%|██▏       | 22/100 [1:18:13<4:37:19, 213.33s/it] 17%|█▋        | 17/100 [46:24<3:46:35, 163.81s/it] 18%|█▊        | 18/100 [49:08<3:43:51, 163.80s/it] 23%|██▎       | 23/100 [1:21:47<4:33:45, 213.32s/it] 19%|█▉        | 19/100 [51:52<3:41:07, 163.80s/it] 24%|██▍       | 24/100 [1:25:20<4:30:12, 213.32s/it] 20%|██        | 20/100 [54:36<3:38:27, 163.84s/it] 25%|██▌       | 25/100 [1:28:53<4:26:38, 213.31s/it] 21%|██        | 21/100 [57:20<3:35:41, 163.82s/it]Classification Train Epoch: 11 [32000/48000 (67%)]	Loss: 0.083775, KL fake Loss: 8.125602
Classification Train Epoch: 11 [38400/48000 (80%)]	Loss: 0.211142, KL fake Loss: 7.553111
Classification Train Epoch: 11 [44800/48000 (93%)]	Loss: 0.036087, KL fake Loss: 7.849627

Test set: Average loss: 0.2791, Accuracy: 7332/8000 (92%)

Classification Train Epoch: 12 [0/48000 (0%)]	Loss: 0.057550, KL fake Loss: 8.053466
Classification Train Epoch: 12 [6400/48000 (13%)]	Loss: 0.052275, KL fake Loss: 7.675706
Classification Train Epoch: 12 [12800/48000 (27%)]	Loss: 0.099755, KL fake Loss: 7.851950
Classification Train Epoch: 12 [19200/48000 (40%)]	Loss: 0.068402, KL fake Loss: 7.712547
Classification Train Epoch: 12 [25600/48000 (53%)]	Loss: 0.123770, KL fake Loss: 8.068788
Classification Train Epoch: 12 [32000/48000 (67%)]	Loss: 0.178724, KL fake Loss: 8.315742
Classification Train Epoch: 12 [38400/48000 (80%)]	Loss: 0.054309, KL fake Loss: 8.556723
Classification Train Epoch: 12 [44800/48000 (93%)]	Loss: 0.155762, KL fake Loss: 7.965056

Test set: Average loss: 0.2579, Accuracy: 7369/8000 (92%)

Classification Train Epoch: 13 [0/48000 (0%)]	Loss: 0.042638, KL fake Loss: 7.644365
Classification Train Epoch: 13 [6400/48000 (13%)]	Loss: 0.011087, KL fake Loss: 8.247484
Classification Train Epoch: 13 [12800/48000 (27%)]	Loss: 0.061045, KL fake Loss: 7.914247
Classification Train Epoch: 13 [19200/48000 (40%)]	Loss: 0.049793, KL fake Loss: 8.333235
Classification Train Epoch: 13 [25600/48000 (53%)]	Loss: 0.085674, KL fake Loss: 7.631080
Classification Train Epoch: 13 [32000/48000 (67%)]	Loss: 0.110494, KL fake Loss: 8.366906
Classification Train Epoch: 13 [38400/48000 (80%)]	Loss: 0.090322, KL fake Loss: 8.582216
Classification Train Epoch: 13 [44800/48000 (93%)]	Loss: 0.178994, KL fake Loss: 8.468029

Test set: Average loss: 0.2619, Accuracy: 7367/8000 (92%)

Classification Train Epoch: 14 [0/48000 (0%)]	Loss: 0.050337, KL fake Loss: 8.123609
Classification Train Epoch: 14 [6400/48000 (13%)]	Loss: 0.022904, KL fake Loss: 7.755733
Classification Train Epoch: 14 [12800/48000 (27%)]	Loss: 0.025391, KL fake Loss: 8.271342
Classification Train Epoch: 14 [19200/48000 (40%)]	Loss: 0.021946, KL fake Loss: 8.808170
Classification Train Epoch: 14 [25600/48000 (53%)]	Loss: 0.077415, KL fake Loss: 9.066244
Classification Train Epoch: 14 [32000/48000 (67%)]	Loss: 0.032866, KL fake Loss: 8.770751
Classification Train Epoch: 14 [38400/48000 (80%)]	Loss: 0.037600, KL fake Loss: 8.600209
Classification Train Epoch: 14 [44800/48000 (93%)]	Loss: 0.068727, KL fake Loss: 8.715141

Test set: Average loss: 0.2782, Accuracy: 7383/8000 (92%)

Classification Train Epoch: 15 [0/48000 (0%)]	Loss: 0.032954, KL fake Loss: 8.028535
Classification Train Epoch: 15 [6400/48000 (13%)]	Loss: 0.112892, KL fake Loss: 8.731482
Classification Train Epoch: 15 [12800/48000 (27%)]	Loss: 0.056493, KL fake Loss: 8.367344
Classification Train Epoch: 15 [19200/48000 (40%)]	Loss: 0.070905, KL fake Loss: 8.535587
Classification Train Epoch: 15 [25600/48000 (53%)]	Loss: 0.042859, KL fake Loss: 8.875497
Classification Train Epoch: 15 [32000/48000 (67%)]	Loss: 0.051226, KL fake Loss: 9.216860
Classification Train Epoch: 15 [38400/48000 (80%)]	Loss: 0.033819, KL fake Loss: 9.900061
Classification Train Epoch: 15 [44800/48000 (93%)]	Loss: 0.035011, KL fake Loss: 9.569420

Test set: Average loss: 0.2956, Accuracy: 7317/8000 (91%)

Classification Train Epoch: 16 [0/48000 (0%)]	Loss: 0.037333, KL fake Loss: 8.794184
Classification Train Epoch: 16 [6400/48000 (13%)]	Loss: 0.032498, KL fake Loss: 7.865694
Classification Train Epoch: 16 [12800/48000 (27%)]	Loss: 0.051713, KL fake Loss: 9.128920
Classification Train Epoch: 16 [19200/48000 (40%)]	Loss: 0.012205, KL fake Loss: 9.380484
Classification Train Epoch: 16 [25600/48000 (53%)]	Loss: 0.055269, KL fake Loss: 9.177618
Classification Train Epoch: 16 [32000/48000 (67%)]	Loss: 0.150031, KL fake Loss: 9.295783
Classification Train Epoch: 16 [38400/48000 (80%)]	Loss: 0.038160, KL fake Loss: 8.771506
Classification Train Epoch: 16 [44800/48000 (93%)]	Loss: 0.153097, KL fake Loss: 9.331272

Test set: Average loss: 0.3224, Accuracy: 7336/8000 (92%)

Classification Train Epoch: 17 [0/48000 (0%)]	Loss: 0.080750, KL fake Loss: 9.196981
Classification Train Epoch: 17 [6400/48000 (13%)]	Loss: 0.017064, KL fake Loss: 9.139519
Classification Train Epoch: 17 [12800/48000 (27%)]	Loss: 0.013847, KL fake Loss: 8.824998
Classification Train Epoch: 17 [19200/48000 (40%)]	Loss: 0.028251, KL fake Loss: 10.139774
Classification Train Epoch: 17 [25600/48000 (53%)]	Loss: 0.013369, KL fake Loss: 9.565748
Classification Train Epoch: 17 [32000/48000 (67%)]	Loss: 0.153128, KL fake Loss: 9.494841
Classification Train Epoch: 17 [38400/48000 (80%)]	Loss: 0.058017, KL fake Loss: 9.659719
Classification Train Epoch: 17 [44800/48000 (93%)]	Loss: 0.018066, KL fake Loss: 9.374827

Test set: Average loss: 0.2854, Accuracy: 7412/8000 (93%)

Classification Train Epoch: 18 [0/48000 (0%)]	Loss: 0.047023, KL fake Loss: 9.623526
Classification Train Epoch: 18 [6400/48000 (13%)]	Loss: 0.027102, KL fake Loss: 9.878820
Classification Train Epoch: 18 [12800/48000 (27%)]	Loss: 0.085646, KL fake Loss: 9.213461
Classification Train Epoch: 18 [19200/48000 (40%)]	Loss: 0.078065, KL fake Loss: 9.650116
Classification Train Epoch: 18 [25600/48000 (53%)]	Loss: 0.018891, KL fake Loss: 9.618355
Classification Train Epoch: 18 [32000/48000 (67%)]	Loss: 0.047716, KL fake Loss: 9.454802
Classification Train Epoch: 18 [38400/48000 (80%)]	Loss: 0.048894, KL fake Loss: 10.268070
Classification Train Epoch: 18 [44800/48000 (93%)]	Loss: 0.028399, KL fake Loss: 9.130684

Test set: Average loss: 0.3073, Accuracy: 7385/8000 (92%)

Classification Train Epoch: 19 [0/48000 (0%)]	Loss: 0.022201, KL fake Loss: 9.317150
Classification Train Epoch: 19 [6400/48000 (13%)]	Loss: 0.017525, KL fake Loss: 9.207083
Classification Train Epoch: 19 [12800/48000 (27%)]	Loss: 0.028702, KL fake Loss: 9.503667
Classification Train Epoch: 19 [19200/48000 (40%)]	Loss: 0.038830, KL fake Loss: 9.819066
Classification Train Epoch: 19 [25600/48000 (53%)]	Loss: 0.040737, KL fake Loss: 9.959533
Classification Train Epoch: 19 [32000/48000 (67%)]	Loss: 0.026273, KL fake Loss: 10.374904
Classification Train Epoch: 19 [38400/48000 (80%)]	Loss: 0.080982, KL fake Loss: 10.429415
Classification Train Epoch: 19 [44800/48000 (93%)]	Loss: 0.090684, KL fake Loss: 10.004303

Test set: Average loss: 0.3209, Accuracy: 7405/8000 (93%)

Classification Train Epoch: 20 [0/48000 (0%)]	Loss: 0.028943, KL fake Loss: 9.782509
Classification Train Epoch: 20 [6400/48000 (13%)]	Loss: 0.014610, KL fake Loss: 9.972821
Classification Train Epoch: 20 [12800/48000 (27%)]	Loss: 0.052910, KL fake Loss: 10.610577
Classification Train Epoch: 20 [19200/48000 (40%)]	Loss: 0.033948, KL fake Loss: 10.763235
Classification Train Epoch: 20 [25600/48000 (53%)]	Loss: 0.009167, KL fake Loss: 8.880507
Classification Train Epoch: 20 [32000/48000 (67%)]	Loss: 0.043700, KL fake Loss: 9.918474
Classification Train Epoch: 20 [38400/48000 (80%)]	Loss: 0.019968, KL fake Loss: 10.712025
Classification Train Epoch: 20 [44800/48000 (93%)]	Loss: 0.015297, KL fake Loss: 10.043112

Test set: Average loss: 0.3397, Accuracy: 7379/8000 (92%)

Classification Train Epoch: 21 [0/48000 (0%)]	Loss: 0.034573, KL fake Loss: 9.441665
Classification Train Epoch: 21 [6400/48000 (13%)]	Loss: 0.024083, KL fake Loss: 10.139118
Classification Train Epoch: 21 [12800/48000 (27%)]	Loss: 0.056954, KL fake Loss: 10.640596
Classification Train Epoch: 21 [19200/48000 (40%)]	Loss: 0.035700, KL fake Loss: 10.480239
Classification Train Epoch: 21 [25600/48000 (53%)]	Loss: 0.014871, KL fake Loss: 10.919698
Classification Train Epoch: 21 [32000/48000 (67%)]	Loss: 0.025932, KL fake Loss: 10.011806
Classification Train Epoch: 21 [38400/48000 (80%)]	Loss: 0.061227, KL fake Loss: 10.896368
Classification Train Epoch: 21 [44800/48000 (93%)]	Loss: 0.015244, KL fake Loss: 10.699098

Test set: Average loss: 0.3119, Accuracy: 7411/8000 (93%)

Classification Train Epoch: 22 [0/48000 (0%)]	Loss: 0.033731, KL fake Loss: 10.626513
 22%|██▏       | 22/100 [1:00:03<3:32:57, 163.81s/it] 26%|██▌       | 26/100 [1:32:26<4:23:04, 213.31s/it] 23%|██▎       | 23/100 [1:02:47<3:30:12, 163.80s/it] 27%|██▋       | 27/100 [1:36:00<4:19:31, 213.31s/it] 24%|██▍       | 24/100 [1:05:31<3:27:28, 163.79s/it] 28%|██▊       | 28/100 [1:39:33<4:15:58, 213.31s/it] 25%|██▌       | 25/100 [1:08:15<3:24:43, 163.79s/it] 26%|██▌       | 26/100 [1:10:58<3:22:00, 163.78s/it] 29%|██▉       | 29/100 [1:43:06<4:12:24, 213.31s/it] 27%|██▋       | 27/100 [1:13:42<3:19:16, 163.78s/it] 30%|███       | 30/100 [1:46:40<4:08:51, 213.31s/it] 28%|██▊       | 28/100 [1:16:26<3:16:32, 163.78s/it] 31%|███       | 31/100 [1:50:13<4:05:18, 213.31s/it] 29%|██▉       | 29/100 [1:19:10<3:13:48, 163.78s/it]Classification Train Epoch: 22 [6400/50000 (13%)]	Loss: 0.057992, KL fake Loss: 0.004671
Classification Train Epoch: 22 [12800/50000 (26%)]	Loss: 0.051834, KL fake Loss: 0.014410
Classification Train Epoch: 22 [19200/50000 (38%)]	Loss: 0.014110, KL fake Loss: 0.006012
Classification Train Epoch: 22 [25600/50000 (51%)]	Loss: 0.024168, KL fake Loss: 0.005840
Classification Train Epoch: 22 [32000/50000 (64%)]	Loss: 0.147498, KL fake Loss: 0.005119
Classification Train Epoch: 22 [38400/50000 (77%)]	Loss: 0.023213, KL fake Loss: 0.003240
Classification Train Epoch: 22 [44800/50000 (90%)]	Loss: 0.035770, KL fake Loss: 0.009164

Test set: Average loss: 3.8962, Accuracy: 6096/10000 (61%)

Classification Train Epoch: 23 [0/50000 (0%)]	Loss: 0.106182, KL fake Loss: 0.010884
Classification Train Epoch: 23 [6400/50000 (13%)]	Loss: 0.174967, KL fake Loss: 0.003534
Classification Train Epoch: 23 [12800/50000 (26%)]	Loss: 0.064587, KL fake Loss: 0.002040
Classification Train Epoch: 23 [19200/50000 (38%)]	Loss: 0.026219, KL fake Loss: 0.007629
Classification Train Epoch: 23 [25600/50000 (51%)]	Loss: 0.070942, KL fake Loss: 0.004962
Classification Train Epoch: 23 [32000/50000 (64%)]	Loss: 0.057801, KL fake Loss: 0.003136
Classification Train Epoch: 23 [38400/50000 (77%)]	Loss: 0.030129, KL fake Loss: 0.014800
Classification Train Epoch: 23 [44800/50000 (90%)]	Loss: 0.042900, KL fake Loss: 0.001748

Test set: Average loss: 2.5882, Accuracy: 6660/10000 (67%)

Classification Train Epoch: 24 [0/50000 (0%)]	Loss: 0.047711, KL fake Loss: 0.007473
Classification Train Epoch: 24 [6400/50000 (13%)]	Loss: 0.018506, KL fake Loss: 0.011538
Classification Train Epoch: 24 [12800/50000 (26%)]	Loss: 0.032812, KL fake Loss: 0.005640
Classification Train Epoch: 24 [19200/50000 (38%)]	Loss: 0.034110, KL fake Loss: 0.010522
Classification Train Epoch: 24 [25600/50000 (51%)]	Loss: 0.026247, KL fake Loss: 0.008692
Classification Train Epoch: 24 [32000/50000 (64%)]	Loss: 0.022206, KL fake Loss: 0.009954
Classification Train Epoch: 24 [38400/50000 (77%)]	Loss: 0.059918, KL fake Loss: 0.006034
Classification Train Epoch: 24 [44800/50000 (90%)]	Loss: 0.038085, KL fake Loss: 0.002327

Test set: Average loss: 2.6176, Accuracy: 6438/10000 (64%)

Classification Train Epoch: 25 [0/50000 (0%)]	Loss: 0.030293, KL fake Loss: 0.004734
Classification Train Epoch: 25 [6400/50000 (13%)]	Loss: 0.040944, KL fake Loss: 0.006962
Classification Train Epoch: 25 [12800/50000 (26%)]	Loss: 0.049082, KL fake Loss: 0.007716
Classification Train Epoch: 25 [19200/50000 (38%)]	Loss: 0.036655, KL fake Loss: 0.004416
Classification Train Epoch: 25 [25600/50000 (51%)]	Loss: 0.117613, KL fake Loss: 0.003674
Classification Train Epoch: 25 [32000/50000 (64%)]	Loss: 0.013501, KL fake Loss: 0.007516
Classification Train Epoch: 25 [38400/50000 (77%)]	Loss: 0.097741, KL fake Loss: 0.005991
Classification Train Epoch: 25 [44800/50000 (90%)]	Loss: 0.037896, KL fake Loss: 0.008340

Test set: Average loss: 3.8589, Accuracy: 5909/10000 (59%)

Classification Train Epoch: 26 [0/50000 (0%)]	Loss: 0.045918, KL fake Loss: 0.013554
Classification Train Epoch: 26 [6400/50000 (13%)]	Loss: 0.077787, KL fake Loss: 0.008371
Classification Train Epoch: 26 [12800/50000 (26%)]	Loss: 0.035619, KL fake Loss: 0.001765
Classification Train Epoch: 26 [19200/50000 (38%)]	Loss: 0.006151, KL fake Loss: 0.007305
Classification Train Epoch: 26 [25600/50000 (51%)]	Loss: 0.020130, KL fake Loss: 0.008391
Classification Train Epoch: 26 [32000/50000 (64%)]	Loss: 0.021151, KL fake Loss: 0.001093
Classification Train Epoch: 26 [38400/50000 (77%)]	Loss: 0.012813, KL fake Loss: 0.010310
Classification Train Epoch: 26 [44800/50000 (90%)]	Loss: 0.059692, KL fake Loss: 0.004321

Test set: Average loss: 2.6297, Accuracy: 6845/10000 (68%)

Classification Train Epoch: 27 [0/50000 (0%)]	Loss: 0.013239, KL fake Loss: 0.001359
Classification Train Epoch: 27 [6400/50000 (13%)]	Loss: 0.056554, KL fake Loss: 0.005358
Classification Train Epoch: 27 [12800/50000 (26%)]	Loss: 0.033306, KL fake Loss: 0.005588
Classification Train Epoch: 27 [19200/50000 (38%)]	Loss: 0.014031, KL fake Loss: 0.002564
Classification Train Epoch: 27 [25600/50000 (51%)]	Loss: 0.020907, KL fake Loss: 0.006663
Classification Train Epoch: 27 [32000/50000 (64%)]	Loss: 0.042481, KL fake Loss: 0.028132
Classification Train Epoch: 27 [38400/50000 (77%)]	Loss: 0.073904, KL fake Loss: 0.003037
Classification Train Epoch: 27 [44800/50000 (90%)]	Loss: 0.018391, KL fake Loss: 0.004853

Test set: Average loss: 2.8691, Accuracy: 6641/10000 (66%)

Classification Train Epoch: 28 [0/50000 (0%)]	Loss: 0.113370, KL fake Loss: 0.004665
Classification Train Epoch: 28 [6400/50000 (13%)]	Loss: 0.124723, KL fake Loss: 0.002723
Classification Train Epoch: 28 [12800/50000 (26%)]	Loss: 0.012519, KL fake Loss: 0.004467
Classification Train Epoch: 28 [19200/50000 (38%)]	Loss: 0.035360, KL fake Loss: 0.009694
Classification Train Epoch: 28 [25600/50000 (51%)]	Loss: 0.059191, KL fake Loss: 0.002479
Classification Train Epoch: 28 [32000/50000 (64%)]	Loss: 0.048251, KL fake Loss: 0.003322
Classification Train Epoch: 28 [38400/50000 (77%)]	Loss: 0.054384, KL fake Loss: 0.000931
Classification Train Epoch: 28 [44800/50000 (90%)]	Loss: 0.079686, KL fake Loss: 0.001744

Test set: Average loss: 5.4496, Accuracy: 5467/10000 (55%)

Classification Train Epoch: 29 [0/50000 (0%)]	Loss: 0.013466, KL fake Loss: 0.001626
Classification Train Epoch: 29 [6400/50000 (13%)]	Loss: 0.052614, KL fake Loss: 0.000659
Classification Train Epoch: 29 [12800/50000 (26%)]	Loss: 0.007839, KL fake Loss: 0.000706
Classification Train Epoch: 29 [19200/50000 (38%)]	Loss: 0.075187, KL fake Loss: 0.004004
Classification Train Epoch: 29 [25600/50000 (51%)]	Loss: 0.095764, KL fake Loss: 0.003473
Classification Train Epoch: 29 [32000/50000 (64%)]	Loss: 0.018786, KL fake Loss: 0.001863
Classification Train Epoch: 29 [38400/50000 (77%)]	Loss: 0.025328, KL fake Loss: 0.002887
Classification Train Epoch: 29 [44800/50000 (90%)]	Loss: 0.007203, KL fake Loss: 0.024820

Test set: Average loss: 3.4674, Accuracy: 6380/10000 (64%)

Classification Train Epoch: 30 [0/50000 (0%)]	Loss: 0.010720, KL fake Loss: 0.005740
Classification Train Epoch: 30 [6400/50000 (13%)]	Loss: 0.059174, KL fake Loss: 0.002264
Classification Train Epoch: 30 [12800/50000 (26%)]	Loss: 0.009342, KL fake Loss: 0.010033
Classification Train Epoch: 30 [19200/50000 (38%)]	Loss: 0.027671, KL fake Loss: 0.002792
Classification Train Epoch: 30 [25600/50000 (51%)]	Loss: 0.033270, KL fake Loss: 0.015209
Classification Train Epoch: 30 [32000/50000 (64%)]	Loss: 0.061257, KL fake Loss: 0.014275
Classification Train Epoch: 30 [38400/50000 (77%)]	Loss: 0.078378, KL fake Loss: 0.004412
Classification Train Epoch: 30 [44800/50000 (90%)]	Loss: 0.011113, KL fake Loss: 0.010985

Test set: Average loss: 4.1140, Accuracy: 5934/10000 (59%)

Classification Train Epoch: 31 [0/50000 (0%)]	Loss: 0.004372, KL fake Loss: 0.009146
Classification Train Epoch: 31 [6400/50000 (13%)]	Loss: 0.055625, KL fake Loss: 0.001872
Classification Train Epoch: 31 [12800/50000 (26%)]	Loss: 0.001887, KL fake Loss: 0.003736
Classification Train Epoch: 31 [19200/50000 (38%)]	Loss: 0.008373, KL fake Loss: 0.004612
Classification Train Epoch: 31 [25600/50000 (51%)]	Loss: 0.016516, KL fake Loss: 0.006696
Classification Train Epoch: 31 [32000/50000 (64%)]	Loss: 0.107182, KL fake Loss: 0.022298
Classification Train Epoch: 31 [38400/50000 (77%)]	Loss: 0.057111, KL fake Loss: 0.008141
Classification Train Epoch: 31 [44800/50000 (90%)]	Loss: 0.022065, KL fake Loss: 0.012759

Test set: Average loss: 3.9414, Accuracy: 6309/10000 (63%)

Classification Train Epoch: 32 [0/50000 (0%)]	Loss: 0.054845, KL fake Loss: 0.008791
Classification Train Epoch: 32 [6400/50000 (13%)]	Loss: 0.004489, KL fake Loss: 0.011563
Classification Train Epoch: 32 [12800/50000 (26%)]	Loss: 0.006378, KL fake Loss: 0.004464
Classification Train Epoch: 32 [19200/50000 (38%)]	Loss: 0.023873, KL fake Loss: 0.024622
Classification Train Epoch: 32 [25600/50000 (51%)]	Loss: 0.109880, KL fake Loss: 0.004509
 30%|███       | 30/100 [1:21:54<3:11:04, 163.78s/it] 32%|███▏      | 32/100 [1:53:46<4:01:44, 213.31s/it] 31%|███       | 31/100 [1:24:37<3:08:21, 163.78s/it] 33%|███▎      | 33/100 [1:57:20<3:58:11, 213.30s/it]Classification Train Epoch: 22 [6400/48000 (13%)]	Loss: 0.044326, KL fake Loss: 10.381445
Classification Train Epoch: 22 [12800/48000 (27%)]	Loss: 0.008705, KL fake Loss: 10.238502
Classification Train Epoch: 22 [19200/48000 (40%)]	Loss: 0.030768, KL fake Loss: 10.211697
Classification Train Epoch: 22 [25600/48000 (53%)]	Loss: 0.014977, KL fake Loss: 10.899277
Classification Train Epoch: 22 [32000/48000 (67%)]	Loss: 0.016191, KL fake Loss: 10.841728
Classification Train Epoch: 22 [38400/48000 (80%)]	Loss: 0.024350, KL fake Loss: 10.377618
Classification Train Epoch: 22 [44800/48000 (93%)]	Loss: 0.045944, KL fake Loss: 11.185759

Test set: Average loss: 0.3154, Accuracy: 7404/8000 (93%)

Classification Train Epoch: 23 [0/48000 (0%)]	Loss: 0.013893, KL fake Loss: 10.983914
Classification Train Epoch: 23 [6400/48000 (13%)]	Loss: 0.016375, KL fake Loss: 11.167833
Classification Train Epoch: 23 [12800/48000 (27%)]	Loss: 0.002817, KL fake Loss: 10.695253
Classification Train Epoch: 23 [19200/48000 (40%)]	Loss: 0.043600, KL fake Loss: 10.600667
Classification Train Epoch: 23 [25600/48000 (53%)]	Loss: 0.065786, KL fake Loss: 10.987608
Classification Train Epoch: 23 [32000/48000 (67%)]	Loss: 0.158783, KL fake Loss: 11.440857
Classification Train Epoch: 23 [38400/48000 (80%)]	Loss: 0.122884, KL fake Loss: 11.143839
Classification Train Epoch: 23 [44800/48000 (93%)]	Loss: 0.029981, KL fake Loss: 9.778237

Test set: Average loss: 0.3557, Accuracy: 7390/8000 (92%)

Classification Train Epoch: 24 [0/48000 (0%)]	Loss: 0.110430, KL fake Loss: 9.851778
Classification Train Epoch: 24 [6400/48000 (13%)]	Loss: 0.015404, KL fake Loss: 11.109294
Classification Train Epoch: 24 [12800/48000 (27%)]	Loss: 0.008031, KL fake Loss: 10.266955
Classification Train Epoch: 24 [19200/48000 (40%)]	Loss: 0.031413, KL fake Loss: 10.789078
Classification Train Epoch: 24 [25600/48000 (53%)]	Loss: 0.018889, KL fake Loss: 10.258266
Classification Train Epoch: 24 [32000/48000 (67%)]	Loss: 0.032741, KL fake Loss: 11.534320
Classification Train Epoch: 24 [38400/48000 (80%)]	Loss: 0.010369, KL fake Loss: 10.830034
Classification Train Epoch: 24 [44800/48000 (93%)]	Loss: 0.031167, KL fake Loss: 11.119005

Test set: Average loss: 0.3304, Accuracy: 7387/8000 (92%)

Classification Train Epoch: 25 [0/48000 (0%)]	Loss: 0.010102, KL fake Loss: 11.416782
Classification Train Epoch: 25 [6400/48000 (13%)]	Loss: 0.059720, KL fake Loss: 11.015486
Classification Train Epoch: 25 [12800/48000 (27%)]	Loss: 0.033610, KL fake Loss: 11.436972
Classification Train Epoch: 25 [19200/48000 (40%)]	Loss: 0.020208, KL fake Loss: 10.963100
Classification Train Epoch: 25 [25600/48000 (53%)]	Loss: 0.002695, KL fake Loss: 11.025703
Classification Train Epoch: 25 [32000/48000 (67%)]	Loss: 0.010407, KL fake Loss: 10.235279
Classification Train Epoch: 25 [38400/48000 (80%)]	Loss: 0.006072, KL fake Loss: 10.600498
Classification Train Epoch: 25 [44800/48000 (93%)]	Loss: 0.024043, KL fake Loss: 10.706057

Test set: Average loss: 0.3658, Accuracy: 7381/8000 (92%)

Classification Train Epoch: 26 [0/48000 (0%)]	Loss: 0.029484, KL fake Loss: 10.630945
Classification Train Epoch: 26 [6400/48000 (13%)]	Loss: 0.037244, KL fake Loss: 11.378038
Classification Train Epoch: 26 [12800/48000 (27%)]	Loss: 0.015265, KL fake Loss: 11.243202
Classification Train Epoch: 26 [19200/48000 (40%)]	Loss: 0.027723, KL fake Loss: 11.328730
Classification Train Epoch: 26 [25600/48000 (53%)]	Loss: 0.013296, KL fake Loss: 10.981510
Classification Train Epoch: 26 [32000/48000 (67%)]	Loss: 0.012535, KL fake Loss: 11.254519
Classification Train Epoch: 26 [38400/48000 (80%)]	Loss: 0.066732, KL fake Loss: 10.294037
Classification Train Epoch: 26 [44800/48000 (93%)]	Loss: 0.003011, KL fake Loss: 11.237875

Test set: Average loss: 0.3697, Accuracy: 7372/8000 (92%)

Classification Train Epoch: 27 [0/48000 (0%)]	Loss: 0.010698, KL fake Loss: 10.618427
Classification Train Epoch: 27 [6400/48000 (13%)]	Loss: 0.013867, KL fake Loss: 11.650284
Classification Train Epoch: 27 [12800/48000 (27%)]	Loss: 0.002428, KL fake Loss: 11.388239
Classification Train Epoch: 27 [19200/48000 (40%)]	Loss: 0.013258, KL fake Loss: 10.900539
Classification Train Epoch: 27 [25600/48000 (53%)]	Loss: 0.067077, KL fake Loss: 10.897217
Classification Train Epoch: 27 [32000/48000 (67%)]	Loss: 0.178370, KL fake Loss: 11.555853
Classification Train Epoch: 27 [38400/48000 (80%)]	Loss: 0.003083, KL fake Loss: 11.373960
Classification Train Epoch: 27 [44800/48000 (93%)]	Loss: 0.033075, KL fake Loss: 11.107099

Test set: Average loss: 0.3782, Accuracy: 7345/8000 (92%)

Classification Train Epoch: 28 [0/48000 (0%)]	Loss: 0.058585, KL fake Loss: 10.302397
Classification Train Epoch: 28 [6400/48000 (13%)]	Loss: 0.011395, KL fake Loss: 11.133683
Classification Train Epoch: 28 [12800/48000 (27%)]	Loss: 0.006671, KL fake Loss: 11.640113
Classification Train Epoch: 28 [19200/48000 (40%)]	Loss: 0.063690, KL fake Loss: 11.706236
Classification Train Epoch: 28 [25600/48000 (53%)]	Loss: 0.002897, KL fake Loss: 11.672382
Classification Train Epoch: 28 [32000/48000 (67%)]	Loss: 0.012871, KL fake Loss: 11.894205
Classification Train Epoch: 28 [38400/48000 (80%)]	Loss: 0.047666, KL fake Loss: 11.040827
Classification Train Epoch: 28 [44800/48000 (93%)]	Loss: 0.003837, KL fake Loss: 11.415854

Test set: Average loss: 0.3518, Accuracy: 7404/8000 (93%)

Classification Train Epoch: 29 [0/48000 (0%)]	Loss: 0.009540, KL fake Loss: 11.297686
Classification Train Epoch: 29 [6400/48000 (13%)]	Loss: 0.031443, KL fake Loss: 11.695004
Classification Train Epoch: 29 [12800/48000 (27%)]	Loss: 0.021403, KL fake Loss: 11.387815
Classification Train Epoch: 29 [19200/48000 (40%)]	Loss: 0.010803, KL fake Loss: 11.492300
Classification Train Epoch: 29 [25600/48000 (53%)]	Loss: 0.000975, KL fake Loss: 11.903265
Classification Train Epoch: 29 [32000/48000 (67%)]	Loss: 0.015009, KL fake Loss: 11.302647
Classification Train Epoch: 29 [38400/48000 (80%)]	Loss: 0.003800, KL fake Loss: 11.431058
Classification Train Epoch: 29 [44800/48000 (93%)]	Loss: 0.108201, KL fake Loss: 11.594334

Test set: Average loss: 0.3723, Accuracy: 7386/8000 (92%)

Classification Train Epoch: 30 [0/48000 (0%)]	Loss: 0.009038, KL fake Loss: 11.115528
Classification Train Epoch: 30 [6400/48000 (13%)]	Loss: 0.008519, KL fake Loss: 11.689995
Classification Train Epoch: 30 [12800/48000 (27%)]	Loss: 0.063909, KL fake Loss: 12.022404
Classification Train Epoch: 30 [19200/48000 (40%)]	Loss: 0.012944, KL fake Loss: 12.061760
Classification Train Epoch: 30 [25600/48000 (53%)]	Loss: 0.001029, KL fake Loss: 11.245290
Classification Train Epoch: 30 [32000/48000 (67%)]	Loss: 0.047511, KL fake Loss: 10.549191
Classification Train Epoch: 30 [38400/48000 (80%)]	Loss: 0.002251, KL fake Loss: 12.012560
Classification Train Epoch: 30 [44800/48000 (93%)]	Loss: 0.095704, KL fake Loss: 11.689731

Test set: Average loss: 0.4062, Accuracy: 7384/8000 (92%)

Classification Train Epoch: 31 [0/48000 (0%)]	Loss: 0.002390, KL fake Loss: 11.354855
Classification Train Epoch: 31 [6400/48000 (13%)]	Loss: 0.010911, KL fake Loss: 11.247332
Classification Train Epoch: 31 [12800/48000 (27%)]	Loss: 0.003424, KL fake Loss: 11.449846
Classification Train Epoch: 31 [19200/48000 (40%)]	Loss: 0.034905, KL fake Loss: 11.084059
Classification Train Epoch: 31 [25600/48000 (53%)]	Loss: 0.008435, KL fake Loss: 10.341967
Classification Train Epoch: 31 [32000/48000 (67%)]	Loss: 0.029185, KL fake Loss: 12.061000
Classification Train Epoch: 31 [38400/48000 (80%)]	Loss: 0.012689, KL fake Loss: 10.897909
Classification Train Epoch: 31 [44800/48000 (93%)]	Loss: 0.034899, KL fake Loss: 12.539659

Test set: Average loss: 0.3804, Accuracy: 7424/8000 (93%)

Classification Train Epoch: 32 [0/48000 (0%)]	Loss: 0.017185, KL fake Loss: 12.755102
Classification Train Epoch: 32 [6400/48000 (13%)]	Loss: 0.002089, KL fake Loss: 11.800215
Classification Train Epoch: 32 [12800/48000 (27%)]	Loss: 0.020249, KL fake Loss: 11.296381
Classification Train Epoch: 32 [19200/48000 (40%)]	Loss: 0.010365, KL fake Loss: 11.234874
Classification Train Epoch: 32 [25600/48000 (53%)]	Loss: 0.089947, KL fake Loss: 11.690838
 32%|███▏      | 32/100 [1:27:21<3:05:37, 163.79s/it] 34%|███▍      | 34/100 [2:00:53<3:54:37, 213.30s/it] 33%|███▎      | 33/100 [1:30:05<3:02:53, 163.79s/it] 35%|███▌      | 35/100 [2:04:26<3:51:03, 213.29s/it] 34%|███▍      | 34/100 [1:32:49<3:00:09, 163.78s/it] 35%|███▌      | 35/100 [1:35:32<2:57:25, 163.78s/it] 36%|███▌      | 36/100 [2:07:59<3:47:30, 213.29s/it] 36%|███▌      | 36/100 [1:38:16<2:54:42, 163.79s/it] 37%|███▋      | 37/100 [2:11:33<3:43:57, 213.30s/it] 37%|███▋      | 37/100 [1:41:00<2:51:58, 163.78s/it] 38%|███▊      | 38/100 [2:15:06<3:40:24, 213.30s/it] 38%|███▊      | 38/100 [1:43:44<2:49:14, 163.78s/it] 39%|███▉      | 39/100 [1:46:28<2:46:31, 163.79s/it] 39%|███▉      | 39/100 [2:18:39<3:36:50, 213.29s/it] 40%|████      | 40/100 [1:49:12<2:43:49, 163.83s/it] 40%|████      | 40/100 [2:22:13<3:33:19, 213.33s/it] 41%|████      | 41/100 [1:51:55<2:41:05, 163.82s/it] 41%|████      | 41/100 [2:25:46<3:29:45, 213.32s/it] 42%|████▏     | 42/100 [1:54:39<2:38:20, 163.81s/it]Classification Train Epoch: 32 [32000/48000 (67%)]	Loss: 0.024724, KL fake Loss: 12.171741
Classification Train Epoch: 32 [38400/48000 (80%)]	Loss: 0.002159, KL fake Loss: 11.785379
Classification Train Epoch: 32 [44800/48000 (93%)]	Loss: 0.047926, KL fake Loss: 11.797703

Test set: Average loss: 0.3898, Accuracy: 7402/8000 (93%)

Classification Train Epoch: 33 [0/48000 (0%)]	Loss: 0.005936, KL fake Loss: 11.004234
Classification Train Epoch: 33 [6400/48000 (13%)]	Loss: 0.045553, KL fake Loss: 12.357948
Classification Train Epoch: 33 [12800/48000 (27%)]	Loss: 0.015825, KL fake Loss: 11.163412
Classification Train Epoch: 33 [19200/48000 (40%)]	Loss: 0.025258, KL fake Loss: 12.189331
Classification Train Epoch: 33 [25600/48000 (53%)]	Loss: 0.002697, KL fake Loss: 11.828976
Classification Train Epoch: 33 [32000/48000 (67%)]	Loss: 0.001491, KL fake Loss: 11.458565
Classification Train Epoch: 33 [38400/48000 (80%)]	Loss: 0.005668, KL fake Loss: 12.033975
Classification Train Epoch: 33 [44800/48000 (93%)]	Loss: 0.003404, KL fake Loss: 12.606153

Test set: Average loss: 0.4043, Accuracy: 7392/8000 (92%)

Classification Train Epoch: 34 [0/48000 (0%)]	Loss: 0.007466, KL fake Loss: 12.054520
Classification Train Epoch: 34 [6400/48000 (13%)]	Loss: 0.007690, KL fake Loss: 11.711086
Classification Train Epoch: 34 [12800/48000 (27%)]	Loss: 0.010889, KL fake Loss: 11.645299
Classification Train Epoch: 34 [19200/48000 (40%)]	Loss: 0.025231, KL fake Loss: 12.078649
Classification Train Epoch: 34 [25600/48000 (53%)]	Loss: 0.128515, KL fake Loss: 11.778151
Classification Train Epoch: 34 [32000/48000 (67%)]	Loss: 0.007271, KL fake Loss: 11.766563
Classification Train Epoch: 34 [38400/48000 (80%)]	Loss: 0.008931, KL fake Loss: 11.417064
Classification Train Epoch: 34 [44800/48000 (93%)]	Loss: 0.042375, KL fake Loss: 11.504698

Test set: Average loss: 0.4123, Accuracy: 7430/8000 (93%)

Classification Train Epoch: 35 [0/48000 (0%)]	Loss: 0.007851, KL fake Loss: 11.873250
Classification Train Epoch: 35 [6400/48000 (13%)]	Loss: 0.008010, KL fake Loss: 11.979095
Classification Train Epoch: 35 [12800/48000 (27%)]	Loss: 0.004772, KL fake Loss: 11.749344
Classification Train Epoch: 35 [19200/48000 (40%)]	Loss: 0.028094, KL fake Loss: 11.855771
Classification Train Epoch: 35 [25600/48000 (53%)]	Loss: 0.001130, KL fake Loss: 11.778530
Classification Train Epoch: 35 [32000/48000 (67%)]	Loss: 0.002193, KL fake Loss: 12.234331
Classification Train Epoch: 35 [38400/48000 (80%)]	Loss: 0.026256, KL fake Loss: 11.428151
Classification Train Epoch: 35 [44800/48000 (93%)]	Loss: 0.029666, KL fake Loss: 12.936628

Test set: Average loss: 0.3836, Accuracy: 7406/8000 (93%)

Classification Train Epoch: 36 [0/48000 (0%)]	Loss: 0.010691, KL fake Loss: 12.261965
Classification Train Epoch: 36 [6400/48000 (13%)]	Loss: 0.003218, KL fake Loss: 11.209659
Classification Train Epoch: 36 [12800/48000 (27%)]	Loss: 0.006649, KL fake Loss: 11.226544
Classification Train Epoch: 36 [19200/48000 (40%)]	Loss: 0.030774, KL fake Loss: 11.639159
Classification Train Epoch: 36 [25600/48000 (53%)]	Loss: 0.027058, KL fake Loss: 11.652124
Classification Train Epoch: 36 [32000/48000 (67%)]	Loss: 0.011154, KL fake Loss: 12.652004
Classification Train Epoch: 36 [38400/48000 (80%)]	Loss: 0.003251, KL fake Loss: 11.699541
Classification Train Epoch: 36 [44800/48000 (93%)]	Loss: 0.026599, KL fake Loss: 12.261347

Test set: Average loss: 0.4171, Accuracy: 7379/8000 (92%)

Classification Train Epoch: 37 [0/48000 (0%)]	Loss: 0.006301, KL fake Loss: 11.526033
Classification Train Epoch: 37 [6400/48000 (13%)]	Loss: 0.035982, KL fake Loss: 11.939546
Classification Train Epoch: 37 [12800/48000 (27%)]	Loss: 0.002482, KL fake Loss: 12.036207
Classification Train Epoch: 37 [19200/48000 (40%)]	Loss: 0.002219, KL fake Loss: 11.996641
Classification Train Epoch: 37 [25600/48000 (53%)]	Loss: 0.002498, KL fake Loss: 11.624692
Classification Train Epoch: 37 [32000/48000 (67%)]	Loss: 0.003749, KL fake Loss: 11.462288
Classification Train Epoch: 37 [38400/48000 (80%)]	Loss: 0.002184, KL fake Loss: 11.550509
Classification Train Epoch: 37 [44800/48000 (93%)]	Loss: 0.001195, KL fake Loss: 12.043278

Test set: Average loss: 0.4222, Accuracy: 7363/8000 (92%)

Classification Train Epoch: 38 [0/48000 (0%)]	Loss: 0.002268, KL fake Loss: 12.452568
Classification Train Epoch: 38 [6400/48000 (13%)]	Loss: 0.012783, KL fake Loss: 11.833635
Classification Train Epoch: 38 [12800/48000 (27%)]	Loss: 0.006670, KL fake Loss: 12.395006
Classification Train Epoch: 38 [19200/48000 (40%)]	Loss: 0.004169, KL fake Loss: 11.055453
Classification Train Epoch: 38 [25600/48000 (53%)]	Loss: 0.022882, KL fake Loss: 11.559029
Classification Train Epoch: 38 [32000/48000 (67%)]	Loss: 0.026898, KL fake Loss: 11.812653
Classification Train Epoch: 38 [38400/48000 (80%)]	Loss: 0.040669, KL fake Loss: 12.198848
Classification Train Epoch: 38 [44800/48000 (93%)]	Loss: 0.001015, KL fake Loss: 12.380037

Test set: Average loss: 0.4200, Accuracy: 7420/8000 (93%)

Classification Train Epoch: 39 [0/48000 (0%)]	Loss: 0.010670, KL fake Loss: 12.238934
Classification Train Epoch: 39 [6400/48000 (13%)]	Loss: 0.013153, KL fake Loss: 11.512949
Classification Train Epoch: 39 [12800/48000 (27%)]	Loss: 0.004570, KL fake Loss: 11.958773
Classification Train Epoch: 39 [19200/48000 (40%)]	Loss: 0.012597, KL fake Loss: 11.730010
Classification Train Epoch: 39 [25600/48000 (53%)]	Loss: 0.009220, KL fake Loss: 11.837370
Classification Train Epoch: 39 [32000/48000 (67%)]	Loss: 0.016174, KL fake Loss: 11.895655
Classification Train Epoch: 39 [38400/48000 (80%)]	Loss: 0.004323, KL fake Loss: 11.802797
Classification Train Epoch: 39 [44800/48000 (93%)]	Loss: 0.009370, KL fake Loss: 12.143251

Test set: Average loss: 0.3893, Accuracy: 7399/8000 (92%)

Classification Train Epoch: 40 [0/48000 (0%)]	Loss: 0.001826, KL fake Loss: 11.795321
Classification Train Epoch: 40 [6400/48000 (13%)]	Loss: 0.017552, KL fake Loss: 11.682774
Classification Train Epoch: 40 [12800/48000 (27%)]	Loss: 0.014034, KL fake Loss: 11.928024
Classification Train Epoch: 40 [19200/48000 (40%)]	Loss: 0.010227, KL fake Loss: 11.664913
Classification Train Epoch: 40 [25600/48000 (53%)]	Loss: 0.004136, KL fake Loss: 11.777405
Classification Train Epoch: 40 [32000/48000 (67%)]	Loss: 0.001483, KL fake Loss: 11.660494
Classification Train Epoch: 40 [38400/48000 (80%)]	Loss: 0.029116, KL fake Loss: 11.678331
Classification Train Epoch: 40 [44800/48000 (93%)]	Loss: 0.035092, KL fake Loss: 12.788469

Test set: Average loss: 0.3721, Accuracy: 7431/8000 (93%)

Classification Train Epoch: 41 [0/48000 (0%)]	Loss: 0.005451, KL fake Loss: 11.440693
Classification Train Epoch: 41 [6400/48000 (13%)]	Loss: 0.055172, KL fake Loss: 12.746464
Classification Train Epoch: 41 [12800/48000 (27%)]	Loss: 0.001224, KL fake Loss: 12.578862
Classification Train Epoch: 41 [19200/48000 (40%)]	Loss: 0.003373, KL fake Loss: 12.533485
Classification Train Epoch: 41 [25600/48000 (53%)]	Loss: 0.015788, KL fake Loss: 12.845305
Classification Train Epoch: 41 [32000/48000 (67%)]	Loss: 0.019002, KL fake Loss: 12.001753
Classification Train Epoch: 41 [38400/48000 (80%)]	Loss: 0.075017, KL fake Loss: 12.604038
Classification Train Epoch: 41 [44800/48000 (93%)]	Loss: 0.002173, KL fake Loss: 12.846905

Test set: Average loss: 0.4017, Accuracy: 7391/8000 (92%)

Classification Train Epoch: 42 [0/48000 (0%)]	Loss: 0.003908, KL fake Loss: 11.254921
Classification Train Epoch: 42 [6400/48000 (13%)]	Loss: 0.003223, KL fake Loss: 12.672548
Classification Train Epoch: 42 [12800/48000 (27%)]	Loss: 0.003358, KL fake Loss: 11.978322
Classification Train Epoch: 42 [19200/48000 (40%)]	Loss: 0.015050, KL fake Loss: 11.632756
Classification Train Epoch: 42 [25600/48000 (53%)]	Loss: 0.018508, KL fake Loss: 12.574352
Classification Train Epoch: 42 [32000/48000 (67%)]	Loss: 0.065182, KL fake Loss: 13.498020
Classification Train Epoch: 42 [38400/48000 (80%)]	Loss: 0.002280, KL fake Loss: 13.262460
Classification Train Epoch: 42 [44800/48000 (93%)]	Loss: 0.086352, KL fake Loss: 12.891812

Test set: Average loss: 0.4171, Accuracy: 7390/8000 (92%)

 43%|████▎     | 43/100 [1:57:23<2:35:36, 163.79s/it] 42%|████▏     | 42/100 [2:29:19<3:26:12, 213.32s/it]Classification Train Epoch: 32 [32000/50000 (64%)]	Loss: 0.016694, KL fake Loss: 0.026203
Classification Train Epoch: 32 [38400/50000 (77%)]	Loss: 0.058196, KL fake Loss: 0.020942
Classification Train Epoch: 32 [44800/50000 (90%)]	Loss: 0.014147, KL fake Loss: 0.021770

Test set: Average loss: 3.5389, Accuracy: 6475/10000 (65%)

Classification Train Epoch: 33 [0/50000 (0%)]	Loss: 0.039649, KL fake Loss: 0.012429
Classification Train Epoch: 33 [6400/50000 (13%)]	Loss: 0.096073, KL fake Loss: 0.003619
Classification Train Epoch: 33 [12800/50000 (26%)]	Loss: 0.025400, KL fake Loss: 0.002055
Classification Train Epoch: 33 [19200/50000 (38%)]	Loss: 0.003091, KL fake Loss: 0.009876
Classification Train Epoch: 33 [25600/50000 (51%)]	Loss: 0.003645, KL fake Loss: 0.001552
Classification Train Epoch: 33 [32000/50000 (64%)]	Loss: 0.111242, KL fake Loss: 0.016150
Classification Train Epoch: 33 [38400/50000 (77%)]	Loss: 0.010338, KL fake Loss: 0.014106
Classification Train Epoch: 33 [44800/50000 (90%)]	Loss: 0.029586, KL fake Loss: 0.004895

Test set: Average loss: 1.9797, Accuracy: 7219/10000 (72%)

Classification Train Epoch: 34 [0/50000 (0%)]	Loss: 0.003798, KL fake Loss: 0.007290
Classification Train Epoch: 34 [6400/50000 (13%)]	Loss: 0.012804, KL fake Loss: 0.000819
Classification Train Epoch: 34 [12800/50000 (26%)]	Loss: 0.037983, KL fake Loss: 0.003472
Classification Train Epoch: 34 [19200/50000 (38%)]	Loss: 0.006407, KL fake Loss: 0.003012
Classification Train Epoch: 34 [25600/50000 (51%)]	Loss: 0.010094, KL fake Loss: 0.010978
Classification Train Epoch: 34 [32000/50000 (64%)]	Loss: 0.023793, KL fake Loss: 0.015591
Classification Train Epoch: 34 [38400/50000 (77%)]	Loss: 0.025518, KL fake Loss: 0.024290
Classification Train Epoch: 34 [44800/50000 (90%)]	Loss: 0.016297, KL fake Loss: 0.006425

Test set: Average loss: 2.3486, Accuracy: 7021/10000 (70%)

Classification Train Epoch: 35 [0/50000 (0%)]	Loss: 0.043708, KL fake Loss: 0.006357
Classification Train Epoch: 35 [6400/50000 (13%)]	Loss: 0.012940, KL fake Loss: 0.024166
Classification Train Epoch: 35 [12800/50000 (26%)]	Loss: 0.032652, KL fake Loss: 0.008512
Classification Train Epoch: 35 [19200/50000 (38%)]	Loss: 0.041676, KL fake Loss: 0.018064
Classification Train Epoch: 35 [25600/50000 (51%)]	Loss: 0.070833, KL fake Loss: 0.007968
Classification Train Epoch: 35 [32000/50000 (64%)]	Loss: 0.030681, KL fake Loss: 0.007801
Classification Train Epoch: 35 [38400/50000 (77%)]	Loss: 0.094516, KL fake Loss: 0.009257
Classification Train Epoch: 35 [44800/50000 (90%)]	Loss: 0.024216, KL fake Loss: 0.035652

Test set: Average loss: 2.2542, Accuracy: 6979/10000 (70%)

Classification Train Epoch: 36 [0/50000 (0%)]	Loss: 0.007169, KL fake Loss: 0.045414
Classification Train Epoch: 36 [6400/50000 (13%)]	Loss: 0.014857, KL fake Loss: 0.005488
Classification Train Epoch: 36 [12800/50000 (26%)]	Loss: 0.019982, KL fake Loss: 0.002651
Classification Train Epoch: 36 [19200/50000 (38%)]	Loss: 0.016442, KL fake Loss: 0.006997
Classification Train Epoch: 36 [25600/50000 (51%)]	Loss: 0.075604, KL fake Loss: 0.005194
Classification Train Epoch: 36 [32000/50000 (64%)]	Loss: 0.016212, KL fake Loss: 0.004709
Classification Train Epoch: 36 [38400/50000 (77%)]	Loss: 0.006009, KL fake Loss: 0.043318
Classification Train Epoch: 36 [44800/50000 (90%)]	Loss: 0.137279, KL fake Loss: 0.011060

Test set: Average loss: 3.3812, Accuracy: 6217/10000 (62%)

Classification Train Epoch: 37 [0/50000 (0%)]	Loss: 0.002209, KL fake Loss: 0.019425
Classification Train Epoch: 37 [6400/50000 (13%)]	Loss: 0.001105, KL fake Loss: 0.009527
Classification Train Epoch: 37 [12800/50000 (26%)]	Loss: 0.038748, KL fake Loss: 0.002634
Classification Train Epoch: 37 [19200/50000 (38%)]	Loss: 0.005986, KL fake Loss: 0.017334
Classification Train Epoch: 37 [25600/50000 (51%)]	Loss: 0.031187, KL fake Loss: 0.006681
Classification Train Epoch: 37 [32000/50000 (64%)]	Loss: 0.034055, KL fake Loss: 0.019038
Classification Train Epoch: 37 [38400/50000 (77%)]	Loss: 0.069462, KL fake Loss: 0.033168
Classification Train Epoch: 37 [44800/50000 (90%)]	Loss: 0.038754, KL fake Loss: 0.013672

Test set: Average loss: 3.0657, Accuracy: 6589/10000 (66%)

Classification Train Epoch: 38 [0/50000 (0%)]	Loss: 0.013332, KL fake Loss: 0.009948
Classification Train Epoch: 38 [6400/50000 (13%)]	Loss: 0.022821, KL fake Loss: 0.002020
Classification Train Epoch: 38 [12800/50000 (26%)]	Loss: 0.006200, KL fake Loss: 0.009733
Classification Train Epoch: 38 [19200/50000 (38%)]	Loss: 0.070892, KL fake Loss: 0.011753
Classification Train Epoch: 38 [25600/50000 (51%)]	Loss: 0.041379, KL fake Loss: 0.007331
Classification Train Epoch: 38 [32000/50000 (64%)]	Loss: 0.056408, KL fake Loss: 0.004419
Classification Train Epoch: 38 [38400/50000 (77%)]	Loss: 0.019035, KL fake Loss: 0.006016
Classification Train Epoch: 38 [44800/50000 (90%)]	Loss: 0.016296, KL fake Loss: 0.002555

Test set: Average loss: 4.8284, Accuracy: 5762/10000 (58%)

Classification Train Epoch: 39 [0/50000 (0%)]	Loss: 0.019076, KL fake Loss: 0.003350
Classification Train Epoch: 39 [6400/50000 (13%)]	Loss: 0.006897, KL fake Loss: 0.003770
Classification Train Epoch: 39 [12800/50000 (26%)]	Loss: 0.013328, KL fake Loss: 0.004390
Classification Train Epoch: 39 [19200/50000 (38%)]	Loss: 0.001133, KL fake Loss: 0.001375
Classification Train Epoch: 39 [25600/50000 (51%)]	Loss: 0.036600, KL fake Loss: 0.003106
Classification Train Epoch: 39 [32000/50000 (64%)]	Loss: 0.009870, KL fake Loss: 0.011866
Classification Train Epoch: 39 [38400/50000 (77%)]	Loss: 0.015689, KL fake Loss: 0.014197
Classification Train Epoch: 39 [44800/50000 (90%)]	Loss: 0.040111, KL fake Loss: 0.000844

Test set: Average loss: 5.8412, Accuracy: 5403/10000 (54%)

Classification Train Epoch: 40 [0/50000 (0%)]	Loss: 0.008517, KL fake Loss: 0.007118
Classification Train Epoch: 40 [6400/50000 (13%)]	Loss: 0.023013, KL fake Loss: 0.010564
Classification Train Epoch: 40 [12800/50000 (26%)]	Loss: 0.004053, KL fake Loss: 0.012793
Classification Train Epoch: 40 [19200/50000 (38%)]	Loss: 0.007339, KL fake Loss: 0.005795
Classification Train Epoch: 40 [25600/50000 (51%)]	Loss: 0.015663, KL fake Loss: 0.014227
Classification Train Epoch: 40 [32000/50000 (64%)]	Loss: 0.003700, KL fake Loss: 0.004901
Classification Train Epoch: 40 [38400/50000 (77%)]	Loss: 0.003766, KL fake Loss: 0.001853
Classification Train Epoch: 40 [44800/50000 (90%)]	Loss: 0.053222, KL fake Loss: 0.003608

Test set: Average loss: 3.2114, Accuracy: 6457/10000 (65%)

Classification Train Epoch: 41 [0/50000 (0%)]	Loss: 0.023418, KL fake Loss: 0.012289
Classification Train Epoch: 41 [6400/50000 (13%)]	Loss: 0.004713, KL fake Loss: 0.019434
Classification Train Epoch: 41 [12800/50000 (26%)]	Loss: 0.092837, KL fake Loss: 0.002346
Classification Train Epoch: 41 [19200/50000 (38%)]	Loss: 0.017390, KL fake Loss: 0.000898
Classification Train Epoch: 41 [25600/50000 (51%)]	Loss: 0.009745, KL fake Loss: 0.001872
Classification Train Epoch: 41 [32000/50000 (64%)]	Loss: 0.002950, KL fake Loss: 0.000640
Classification Train Epoch: 41 [38400/50000 (77%)]	Loss: 0.003003, KL fake Loss: 0.002563
Classification Train Epoch: 41 [44800/50000 (90%)]	Loss: 0.014051, KL fake Loss: 0.001621

Test set: Average loss: 1.7819, Accuracy: 7541/10000 (75%)

Classification Train Epoch: 42 [0/50000 (0%)]	Loss: 0.086647, KL fake Loss: 0.061807
Classification Train Epoch: 42 [6400/50000 (13%)]	Loss: 0.052349, KL fake Loss: 0.008364
Classification Train Epoch: 42 [12800/50000 (26%)]	Loss: 0.003979, KL fake Loss: 0.006520
Classification Train Epoch: 42 [19200/50000 (38%)]	Loss: 0.099842, KL fake Loss: 0.008038
Classification Train Epoch: 42 [25600/50000 (51%)]	Loss: 0.083178, KL fake Loss: 0.007668
Classification Train Epoch: 42 [32000/50000 (64%)]	Loss: 0.058077, KL fake Loss: 0.001583
Classification Train Epoch: 42 [38400/50000 (77%)]	Loss: 0.014311, KL fake Loss: 0.010733
Classification Train Epoch: 42 [44800/50000 (90%)]	Loss: 0.014410, KL fake Loss: 0.003520

Test set: Average loss: 3.6148, Accuracy: 6572/10000 (66%)

Classification Train Epoch: 43 [0/50000 (0%)]	Loss: 0.001604, KL fake Loss: 0.001861
 44%|████▍     | 44/100 [2:00:07<2:32:52, 163.79s/it] 43%|████▎     | 43/100 [2:32:53<3:22:38, 213.31s/it] 45%|████▌     | 45/100 [2:02:50<2:30:08, 163.79s/it] 44%|████▍     | 44/100 [2:36:26<3:19:05, 213.31s/it] 46%|████▌     | 46/100 [2:05:34<2:27:24, 163.78s/it] 45%|████▌     | 45/100 [2:39:59<3:15:32, 213.31s/it] 47%|████▋     | 47/100 [2:08:18<2:24:40, 163.79s/it] 48%|████▊     | 48/100 [2:11:02<2:21:56, 163.78s/it] 46%|████▌     | 46/100 [2:43:33<3:11:58, 213.31s/it] 49%|████▉     | 49/100 [2:13:46<2:19:12, 163.78s/it] 47%|████▋     | 47/100 [2:47:06<3:08:24, 213.30s/it] 50%|█████     | 50/100 [2:16:29<2:16:29, 163.79s/it] 48%|████▊     | 48/100 [2:50:39<3:04:51, 213.30s/it] 51%|█████     | 51/100 [2:19:13<2:13:45, 163.78s/it] 52%|█████▏    | 52/100 [2:21:57<2:11:01, 163.78s/it] 49%|████▉     | 49/100 [2:54:13<3:01:18, 213.31s/it]Classification Train Epoch: 43 [0/48000 (0%)]	Loss: 0.001081, KL fake Loss: 12.856301
Classification Train Epoch: 43 [6400/48000 (13%)]	Loss: 0.029101, KL fake Loss: 11.698643
Classification Train Epoch: 43 [12800/48000 (27%)]	Loss: 0.022422, KL fake Loss: 11.926540
Classification Train Epoch: 43 [19200/48000 (40%)]	Loss: 0.030093, KL fake Loss: 12.355837
Classification Train Epoch: 43 [25600/48000 (53%)]	Loss: 0.011718, KL fake Loss: 13.158821
Classification Train Epoch: 43 [32000/48000 (67%)]	Loss: 0.022502, KL fake Loss: 12.235275
Classification Train Epoch: 43 [38400/48000 (80%)]	Loss: 0.023738, KL fake Loss: 12.244072
Classification Train Epoch: 43 [44800/48000 (93%)]	Loss: 0.023019, KL fake Loss: 12.524970

Test set: Average loss: 0.3869, Accuracy: 7397/8000 (92%)

Classification Train Epoch: 44 [0/48000 (0%)]	Loss: 0.004545, KL fake Loss: 13.064178
Classification Train Epoch: 44 [6400/48000 (13%)]	Loss: 0.000755, KL fake Loss: 12.953558
Classification Train Epoch: 44 [12800/48000 (27%)]	Loss: 0.053959, KL fake Loss: 12.409651
Classification Train Epoch: 44 [19200/48000 (40%)]	Loss: 0.013110, KL fake Loss: 12.315106
Classification Train Epoch: 44 [25600/48000 (53%)]	Loss: 0.004605, KL fake Loss: 13.421527
Classification Train Epoch: 44 [32000/48000 (67%)]	Loss: 0.006078, KL fake Loss: 12.287117
Classification Train Epoch: 44 [38400/48000 (80%)]	Loss: 0.003118, KL fake Loss: 12.750889
Classification Train Epoch: 44 [44800/48000 (93%)]	Loss: 0.001124, KL fake Loss: 11.716283

Test set: Average loss: 0.4463, Accuracy: 7404/8000 (93%)

Classification Train Epoch: 45 [0/48000 (0%)]	Loss: 0.005987, KL fake Loss: 13.078517
Classification Train Epoch: 45 [6400/48000 (13%)]	Loss: 0.010085, KL fake Loss: 12.699895
Classification Train Epoch: 45 [12800/48000 (27%)]	Loss: 0.006587, KL fake Loss: 11.421239
Classification Train Epoch: 45 [19200/48000 (40%)]	Loss: 0.007914, KL fake Loss: 12.960428
Classification Train Epoch: 45 [25600/48000 (53%)]	Loss: 0.035561, KL fake Loss: 12.390649
Classification Train Epoch: 45 [32000/48000 (67%)]	Loss: 0.015889, KL fake Loss: 11.409432
Classification Train Epoch: 45 [38400/48000 (80%)]	Loss: 0.199351, KL fake Loss: 12.834930
Classification Train Epoch: 45 [44800/48000 (93%)]	Loss: 0.007675, KL fake Loss: 13.107823

Test set: Average loss: 0.4299, Accuracy: 7419/8000 (93%)

Classification Train Epoch: 46 [0/48000 (0%)]	Loss: 0.000709, KL fake Loss: 12.394323
Classification Train Epoch: 46 [6400/48000 (13%)]	Loss: 0.004050, KL fake Loss: 12.451859
Classification Train Epoch: 46 [12800/48000 (27%)]	Loss: 0.002223, KL fake Loss: 12.693727
Classification Train Epoch: 46 [19200/48000 (40%)]	Loss: 0.034473, KL fake Loss: 12.436529
Classification Train Epoch: 46 [25600/48000 (53%)]	Loss: 0.065327, KL fake Loss: 12.314903
Classification Train Epoch: 46 [32000/48000 (67%)]	Loss: 0.010411, KL fake Loss: 13.008674
Classification Train Epoch: 46 [38400/48000 (80%)]	Loss: 0.001488, KL fake Loss: 12.743635
Classification Train Epoch: 46 [44800/48000 (93%)]	Loss: 0.005322, KL fake Loss: 11.951888

Test set: Average loss: 0.4206, Accuracy: 7399/8000 (92%)

Classification Train Epoch: 47 [0/48000 (0%)]	Loss: 0.001358, KL fake Loss: 12.667006
Classification Train Epoch: 47 [6400/48000 (13%)]	Loss: 0.000831, KL fake Loss: 12.746265
Classification Train Epoch: 47 [12800/48000 (27%)]	Loss: 0.003150, KL fake Loss: 12.940511
Classification Train Epoch: 47 [19200/48000 (40%)]	Loss: 0.001890, KL fake Loss: 12.895323
Classification Train Epoch: 47 [25600/48000 (53%)]	Loss: 0.027791, KL fake Loss: 12.972288
Classification Train Epoch: 47 [32000/48000 (67%)]	Loss: 0.019030, KL fake Loss: 13.128897
Classification Train Epoch: 47 [38400/48000 (80%)]	Loss: 0.004140, KL fake Loss: 11.911113
Classification Train Epoch: 47 [44800/48000 (93%)]	Loss: 0.030220, KL fake Loss: 13.025436

Test set: Average loss: 0.4183, Accuracy: 7403/8000 (93%)

Classification Train Epoch: 48 [0/48000 (0%)]	Loss: 0.001326, KL fake Loss: 12.623499
Classification Train Epoch: 48 [6400/48000 (13%)]	Loss: 0.053766, KL fake Loss: 12.595900
Classification Train Epoch: 48 [12800/48000 (27%)]	Loss: 0.001653, KL fake Loss: 12.930493
Classification Train Epoch: 48 [19200/48000 (40%)]	Loss: 0.011169, KL fake Loss: 12.133139
Classification Train Epoch: 48 [25600/48000 (53%)]	Loss: 0.007021, KL fake Loss: 12.059004
Classification Train Epoch: 48 [32000/48000 (67%)]	Loss: 0.003440, KL fake Loss: 12.561138
Classification Train Epoch: 48 [38400/48000 (80%)]	Loss: 0.002668, KL fake Loss: 13.381080
Classification Train Epoch: 48 [44800/48000 (93%)]	Loss: 0.005518, KL fake Loss: 12.501541

Test set: Average loss: 0.3991, Accuracy: 7412/8000 (93%)

Classification Train Epoch: 49 [0/48000 (0%)]	Loss: 0.005210, KL fake Loss: 12.098930
Classification Train Epoch: 49 [6400/48000 (13%)]	Loss: 0.003613, KL fake Loss: 12.152115
Classification Train Epoch: 49 [12800/48000 (27%)]	Loss: 0.008452, KL fake Loss: 12.989134
Classification Train Epoch: 49 [19200/48000 (40%)]	Loss: 0.008654, KL fake Loss: 12.810352
Classification Train Epoch: 49 [25600/48000 (53%)]	Loss: 0.000916, KL fake Loss: 12.137603
Classification Train Epoch: 49 [32000/48000 (67%)]	Loss: 0.002125, KL fake Loss: 12.973400
Classification Train Epoch: 49 [38400/48000 (80%)]	Loss: 0.007539, KL fake Loss: 12.551332
Classification Train Epoch: 49 [44800/48000 (93%)]	Loss: 0.046710, KL fake Loss: 12.307129

Test set: Average loss: 0.4103, Accuracy: 7437/8000 (93%)

Classification Train Epoch: 50 [0/48000 (0%)]	Loss: 0.001206, KL fake Loss: 13.163486
Classification Train Epoch: 50 [6400/48000 (13%)]	Loss: 0.018439, KL fake Loss: 12.374399
Classification Train Epoch: 50 [12800/48000 (27%)]	Loss: 0.004830, KL fake Loss: 12.010725
Classification Train Epoch: 50 [19200/48000 (40%)]	Loss: 0.002520, KL fake Loss: 12.573881
Classification Train Epoch: 50 [25600/48000 (53%)]	Loss: 0.040735, KL fake Loss: 12.163178
Classification Train Epoch: 50 [32000/48000 (67%)]	Loss: 0.038104, KL fake Loss: 12.316769
Classification Train Epoch: 50 [38400/48000 (80%)]	Loss: 0.003157, KL fake Loss: 13.017015
Classification Train Epoch: 50 [44800/48000 (93%)]	Loss: 0.005197, KL fake Loss: 12.432821

Test set: Average loss: 0.4215, Accuracy: 7423/8000 (93%)

Classification Train Epoch: 51 [0/48000 (0%)]	Loss: 0.010043, KL fake Loss: 13.021762
Classification Train Epoch: 51 [6400/48000 (13%)]	Loss: 0.001594, KL fake Loss: 11.561875
Classification Train Epoch: 51 [12800/48000 (27%)]	Loss: 0.001078, KL fake Loss: 12.339808
Classification Train Epoch: 51 [19200/48000 (40%)]	Loss: 0.040843, KL fake Loss: 12.266787
Classification Train Epoch: 51 [25600/48000 (53%)]	Loss: 0.002387, KL fake Loss: 12.197708
Classification Train Epoch: 51 [32000/48000 (67%)]	Loss: 0.000343, KL fake Loss: 12.124848
Classification Train Epoch: 51 [38400/48000 (80%)]	Loss: 0.000476, KL fake Loss: 13.320359
Classification Train Epoch: 51 [44800/48000 (93%)]	Loss: 0.001061, KL fake Loss: 12.800544

Test set: Average loss: 0.4581, Accuracy: 7421/8000 (93%)

Classification Train Epoch: 52 [0/48000 (0%)]	Loss: 0.031893, KL fake Loss: 13.232868
Classification Train Epoch: 52 [6400/48000 (13%)]	Loss: 0.001727, KL fake Loss: 12.953605
Classification Train Epoch: 52 [12800/48000 (27%)]	Loss: 0.030181, KL fake Loss: 13.199167
Classification Train Epoch: 52 [19200/48000 (40%)]	Loss: 0.000409, KL fake Loss: 12.934437
Classification Train Epoch: 52 [25600/48000 (53%)]	Loss: 0.001128, KL fake Loss: 12.872584
Classification Train Epoch: 52 [32000/48000 (67%)]	Loss: 0.079277, KL fake Loss: 12.939249
Classification Train Epoch: 52 [38400/48000 (80%)]	Loss: 0.001161, KL fake Loss: 12.841144
Classification Train Epoch: 52 [44800/48000 (93%)]	Loss: 0.010567, KL fake Loss: 12.267858

Test set: Average loss: 0.4743, Accuracy: 7356/8000 (92%)

Classification Train Epoch: 53 [0/48000 (0%)]	Loss: 0.029942, KL fake Loss: 12.721638
Classification Train Epoch: 53 [6400/48000 (13%)]	Loss: 0.015190, KL fake Loss: 11.866067
Classification Train Epoch: 53 [12800/48000 (27%)]	Loss: 0.000809, KL fake Loss: 12.385455
Classification Train Epoch: 53 [19200/48000 (40%)]	Loss: 0.004127, KL fake Loss: 13.041658
 53%|█████▎    | 53/100 [2:24:41<2:08:17, 163.78s/it] 50%|█████     | 50/100 [2:57:46<2:57:45, 213.31s/it] 54%|█████▍    | 54/100 [2:27:24<2:05:33, 163.78s/it] 51%|█████     | 51/100 [3:01:19<2:54:12, 213.32s/it] 55%|█████▌    | 55/100 [2:30:08<2:02:50, 163.78s/it] 56%|█████▌    | 56/100 [2:32:52<2:00:06, 163.78s/it] 52%|█████▏    | 52/100 [3:04:53<2:50:39, 213.33s/it]Classification Train Epoch: 43 [6400/50000 (13%)]	Loss: 0.007686, KL fake Loss: 0.003355
Classification Train Epoch: 43 [12800/50000 (26%)]	Loss: 0.015565, KL fake Loss: 0.000887
Classification Train Epoch: 43 [19200/50000 (38%)]	Loss: 0.009067, KL fake Loss: 0.005946
Classification Train Epoch: 43 [25600/50000 (51%)]	Loss: 0.012324, KL fake Loss: 0.003535
Classification Train Epoch: 43 [32000/50000 (64%)]	Loss: 0.017375, KL fake Loss: 0.004964
Classification Train Epoch: 43 [38400/50000 (77%)]	Loss: 0.082025, KL fake Loss: 0.058274
Classification Train Epoch: 43 [44800/50000 (90%)]	Loss: 0.010337, KL fake Loss: 0.018639

Test set: Average loss: 3.0340, Accuracy: 6741/10000 (67%)

Classification Train Epoch: 44 [0/50000 (0%)]	Loss: 0.037086, KL fake Loss: 0.016635
Classification Train Epoch: 44 [6400/50000 (13%)]	Loss: 0.031214, KL fake Loss: 0.003971
Classification Train Epoch: 44 [12800/50000 (26%)]	Loss: 0.036981, KL fake Loss: 0.005459
Classification Train Epoch: 44 [19200/50000 (38%)]	Loss: 0.041348, KL fake Loss: 0.007035
Classification Train Epoch: 44 [25600/50000 (51%)]	Loss: 0.005180, KL fake Loss: 0.003837
Classification Train Epoch: 44 [32000/50000 (64%)]	Loss: 0.000959, KL fake Loss: 0.012289
Classification Train Epoch: 44 [38400/50000 (77%)]	Loss: 0.018495, KL fake Loss: 0.012238
Classification Train Epoch: 44 [44800/50000 (90%)]	Loss: 0.012837, KL fake Loss: 0.012025

Test set: Average loss: 3.8048, Accuracy: 6103/10000 (61%)

Classification Train Epoch: 45 [0/50000 (0%)]	Loss: 0.018357, KL fake Loss: 0.014270
Classification Train Epoch: 45 [6400/50000 (13%)]	Loss: 0.005058, KL fake Loss: 0.007035
Classification Train Epoch: 45 [12800/50000 (26%)]	Loss: 0.021155, KL fake Loss: 0.024307
Classification Train Epoch: 45 [19200/50000 (38%)]	Loss: 0.134535, KL fake Loss: 0.015449
Classification Train Epoch: 45 [25600/50000 (51%)]	Loss: 0.016086, KL fake Loss: 0.011833
Classification Train Epoch: 45 [32000/50000 (64%)]	Loss: 0.069266, KL fake Loss: 0.011348
Classification Train Epoch: 45 [38400/50000 (77%)]	Loss: 0.027433, KL fake Loss: 0.014267
Classification Train Epoch: 45 [44800/50000 (90%)]	Loss: 0.005754, KL fake Loss: 0.008153

Test set: Average loss: 5.9040, Accuracy: 4991/10000 (50%)

Classification Train Epoch: 46 [0/50000 (0%)]	Loss: 0.002724, KL fake Loss: 0.007062
Classification Train Epoch: 46 [6400/50000 (13%)]	Loss: 0.009666, KL fake Loss: 0.008166
Classification Train Epoch: 46 [12800/50000 (26%)]	Loss: 0.014966, KL fake Loss: 0.007334
Classification Train Epoch: 46 [19200/50000 (38%)]	Loss: 0.014792, KL fake Loss: 0.006284
Classification Train Epoch: 46 [25600/50000 (51%)]	Loss: 0.017596, KL fake Loss: 0.007795
Classification Train Epoch: 46 [32000/50000 (64%)]	Loss: 0.045541, KL fake Loss: 0.005584
Classification Train Epoch: 46 [38400/50000 (77%)]	Loss: 0.009336, KL fake Loss: 0.003892
Classification Train Epoch: 46 [44800/50000 (90%)]	Loss: 0.006276, KL fake Loss: 0.007042

Test set: Average loss: 3.7967, Accuracy: 6419/10000 (64%)

Classification Train Epoch: 47 [0/50000 (0%)]	Loss: 0.028129, KL fake Loss: 0.002640
Classification Train Epoch: 47 [6400/50000 (13%)]	Loss: 0.011861, KL fake Loss: 0.004405
Classification Train Epoch: 47 [12800/50000 (26%)]	Loss: 0.004408, KL fake Loss: 0.005382
Classification Train Epoch: 47 [19200/50000 (38%)]	Loss: 0.072868, KL fake Loss: 0.002177
Classification Train Epoch: 47 [25600/50000 (51%)]	Loss: 0.077434, KL fake Loss: 0.008748
Classification Train Epoch: 47 [32000/50000 (64%)]	Loss: 0.014928, KL fake Loss: 0.002333
Classification Train Epoch: 47 [38400/50000 (77%)]	Loss: 0.009598, KL fake Loss: 0.009448
Classification Train Epoch: 47 [44800/50000 (90%)]	Loss: 0.034934, KL fake Loss: 0.004059

Test set: Average loss: 3.3667, Accuracy: 6752/10000 (68%)

Classification Train Epoch: 48 [0/50000 (0%)]	Loss: 0.014737, KL fake Loss: 0.006380
Classification Train Epoch: 48 [6400/50000 (13%)]	Loss: 0.018454, KL fake Loss: 0.007235
Classification Train Epoch: 48 [12800/50000 (26%)]	Loss: 0.015281, KL fake Loss: 0.000382
Classification Train Epoch: 48 [19200/50000 (38%)]	Loss: 0.026257, KL fake Loss: 0.003823
Classification Train Epoch: 48 [25600/50000 (51%)]	Loss: 0.004698, KL fake Loss: 0.002315
Classification Train Epoch: 48 [32000/50000 (64%)]	Loss: 0.099604, KL fake Loss: 0.002415
Classification Train Epoch: 48 [38400/50000 (77%)]	Loss: 0.007171, KL fake Loss: 0.004052
Classification Train Epoch: 48 [44800/50000 (90%)]	Loss: 0.004933, KL fake Loss: 0.002895

Test set: Average loss: 5.0476, Accuracy: 5835/10000 (58%)

Classification Train Epoch: 49 [0/50000 (0%)]	Loss: 0.008377, KL fake Loss: 0.000854
Classification Train Epoch: 49 [6400/50000 (13%)]	Loss: 0.010653, KL fake Loss: 0.003018
Classification Train Epoch: 49 [12800/50000 (26%)]	Loss: 0.030907, KL fake Loss: 0.005366
Classification Train Epoch: 49 [19200/50000 (38%)]	Loss: 0.002649, KL fake Loss: 0.000839
Classification Train Epoch: 49 [25600/50000 (51%)]	Loss: 0.205121, KL fake Loss: 0.002664
Classification Train Epoch: 49 [32000/50000 (64%)]	Loss: 0.006976, KL fake Loss: 0.013116
Classification Train Epoch: 49 [38400/50000 (77%)]	Loss: 0.007162, KL fake Loss: 0.008379
Classification Train Epoch: 49 [44800/50000 (90%)]	Loss: 0.033579, KL fake Loss: 0.003709

Test set: Average loss: 4.1447, Accuracy: 6229/10000 (62%)

Classification Train Epoch: 50 [0/50000 (0%)]	Loss: 0.011943, KL fake Loss: 0.010151
Classification Train Epoch: 50 [6400/50000 (13%)]	Loss: 0.005118, KL fake Loss: 0.002134
Classification Train Epoch: 50 [12800/50000 (26%)]	Loss: 0.064968, KL fake Loss: 0.005510
Classification Train Epoch: 50 [19200/50000 (38%)]	Loss: 0.005265, KL fake Loss: 0.000950
Classification Train Epoch: 50 [25600/50000 (51%)]	Loss: 0.006887, KL fake Loss: 0.001344
Classification Train Epoch: 50 [32000/50000 (64%)]	Loss: 0.007118, KL fake Loss: 0.005802
Classification Train Epoch: 50 [38400/50000 (77%)]	Loss: 0.028865, KL fake Loss: 0.002717
Classification Train Epoch: 50 [44800/50000 (90%)]	Loss: 0.035576, KL fake Loss: 0.000577

Test set: Average loss: 4.8501, Accuracy: 5521/10000 (55%)

Classification Train Epoch: 51 [0/50000 (0%)]	Loss: 0.003489, KL fake Loss: 0.013018
Classification Train Epoch: 51 [6400/50000 (13%)]	Loss: 0.002844, KL fake Loss: 0.002641
Classification Train Epoch: 51 [12800/50000 (26%)]	Loss: 0.000534, KL fake Loss: 0.000313
Classification Train Epoch: 51 [19200/50000 (38%)]	Loss: 0.009385, KL fake Loss: 0.007805
Classification Train Epoch: 51 [25600/50000 (51%)]	Loss: 0.002292, KL fake Loss: 0.001835
Classification Train Epoch: 51 [32000/50000 (64%)]	Loss: 0.040833, KL fake Loss: 0.001008
Classification Train Epoch: 51 [38400/50000 (77%)]	Loss: 0.004024, KL fake Loss: 0.003133
Classification Train Epoch: 51 [44800/50000 (90%)]	Loss: 0.059454, KL fake Loss: 0.004924

Test set: Average loss: 3.9390, Accuracy: 6239/10000 (62%)

Classification Train Epoch: 52 [0/50000 (0%)]	Loss: 0.031868, KL fake Loss: 0.002428
Classification Train Epoch: 52 [6400/50000 (13%)]	Loss: 0.027140, KL fake Loss: 0.003572
Classification Train Epoch: 52 [12800/50000 (26%)]	Loss: 0.009513, KL fake Loss: 0.009521
Classification Train Epoch: 52 [19200/50000 (38%)]	Loss: 0.111463, KL fake Loss: 0.000723
Classification Train Epoch: 52 [25600/50000 (51%)]	Loss: 0.037406, KL fake Loss: 0.000679
Classification Train Epoch: 52 [32000/50000 (64%)]	Loss: 0.002616, KL fake Loss: 0.003112
Classification Train Epoch: 52 [38400/50000 (77%)]	Loss: 0.006410, KL fake Loss: 0.000984
Classification Train Epoch: 52 [44800/50000 (90%)]	Loss: 0.013603, KL fake Loss: 0.001879

Test set: Average loss: 3.8192, Accuracy: 6367/10000 (64%)

Classification Train Epoch: 53 [0/50000 (0%)]	Loss: 0.005866, KL fake Loss: 0.002748
Classification Train Epoch: 53 [6400/50000 (13%)]	Loss: 0.003306, KL fake Loss: 0.002157
Classification Train Epoch: 53 [12800/50000 (26%)]	Loss: 0.041761, KL fake Loss: 0.004139
Classification Train Epoch: 53 [19200/50000 (38%)]	Loss: 0.006583, KL fake Loss: 0.005700
Classification Train Epoch: 53 [25600/50000 (51%)]	Loss: 0.014819, KL fake Loss: 0.005371
 57%|█████▋    | 57/100 [2:35:36<1:57:22, 163.77s/it] 53%|█████▎    | 53/100 [3:08:26<2:47:06, 213.33s/it] 58%|█████▊    | 58/100 [2:38:20<1:54:38, 163.78s/it] 54%|█████▍    | 54/100 [3:11:59<2:43:32, 213.32s/it] 59%|█████▉    | 59/100 [2:41:03<1:51:54, 163.77s/it] 55%|█████▌    | 55/100 [3:15:33<2:39:59, 213.33s/it] 60%|██████    | 60/100 [2:43:47<1:49:11, 163.79s/it] 61%|██████    | 61/100 [2:46:31<1:46:28, 163.80s/it] 56%|█████▌    | 56/100 [3:19:06<2:36:26, 213.33s/it] 62%|██████▏   | 62/100 [2:49:15<1:43:44, 163.79s/it] 57%|█████▋    | 57/100 [3:22:39<2:32:52, 213.32s/it]Classification Train Epoch: 53 [25600/48000 (53%)]	Loss: 0.088453, KL fake Loss: 13.670310
Classification Train Epoch: 53 [32000/48000 (67%)]	Loss: 0.001084, KL fake Loss: 12.488483
Classification Train Epoch: 53 [38400/48000 (80%)]	Loss: 0.002053, KL fake Loss: 12.514210
Classification Train Epoch: 53 [44800/48000 (93%)]	Loss: 0.006512, KL fake Loss: 12.360196

Test set: Average loss: 0.4010, Accuracy: 7415/8000 (93%)

Classification Train Epoch: 54 [0/48000 (0%)]	Loss: 0.011507, KL fake Loss: 12.639726
Classification Train Epoch: 54 [6400/48000 (13%)]	Loss: 0.024719, KL fake Loss: 13.178036
Classification Train Epoch: 54 [12800/48000 (27%)]	Loss: 0.004128, KL fake Loss: 11.982532
Classification Train Epoch: 54 [19200/48000 (40%)]	Loss: 0.002797, KL fake Loss: 12.779278
Classification Train Epoch: 54 [25600/48000 (53%)]	Loss: 0.001012, KL fake Loss: 12.289179
Classification Train Epoch: 54 [32000/48000 (67%)]	Loss: 0.004563, KL fake Loss: 13.079428
Classification Train Epoch: 54 [38400/48000 (80%)]	Loss: 0.099504, KL fake Loss: 12.215113
Classification Train Epoch: 54 [44800/48000 (93%)]	Loss: 0.016716, KL fake Loss: 11.993761

Test set: Average loss: 0.4025, Accuracy: 7438/8000 (93%)

Classification Train Epoch: 55 [0/48000 (0%)]	Loss: 0.009806, KL fake Loss: 12.460161
Classification Train Epoch: 55 [6400/48000 (13%)]	Loss: 0.072497, KL fake Loss: 12.089098
Classification Train Epoch: 55 [12800/48000 (27%)]	Loss: 0.000951, KL fake Loss: 12.271696
Classification Train Epoch: 55 [19200/48000 (40%)]	Loss: 0.000156, KL fake Loss: 12.262384
Classification Train Epoch: 55 [25600/48000 (53%)]	Loss: 0.000056, KL fake Loss: 13.001409
Classification Train Epoch: 55 [32000/48000 (67%)]	Loss: 0.000131, KL fake Loss: 12.705971
Classification Train Epoch: 55 [38400/48000 (80%)]	Loss: 0.015383, KL fake Loss: 12.156267
Classification Train Epoch: 55 [44800/48000 (93%)]	Loss: 0.070732, KL fake Loss: 12.530396

Test set: Average loss: 0.4471, Accuracy: 7418/8000 (93%)

Classification Train Epoch: 56 [0/48000 (0%)]	Loss: 0.007260, KL fake Loss: 12.032287
Classification Train Epoch: 56 [6400/48000 (13%)]	Loss: 0.001418, KL fake Loss: 11.931546
Classification Train Epoch: 56 [12800/48000 (27%)]	Loss: 0.050821, KL fake Loss: 12.556543
Classification Train Epoch: 56 [19200/48000 (40%)]	Loss: 0.001289, KL fake Loss: 12.270780
Classification Train Epoch: 56 [25600/48000 (53%)]	Loss: 0.043529, KL fake Loss: 12.746795
Classification Train Epoch: 56 [32000/48000 (67%)]	Loss: 0.022408, KL fake Loss: 12.937183
Classification Train Epoch: 56 [38400/48000 (80%)]	Loss: 0.011537, KL fake Loss: 12.563288
Classification Train Epoch: 56 [44800/48000 (93%)]	Loss: 0.001384, KL fake Loss: 13.004032

Test set: Average loss: 0.4025, Accuracy: 7450/8000 (93%)

Classification Train Epoch: 57 [0/48000 (0%)]	Loss: 0.000649, KL fake Loss: 12.531069
Classification Train Epoch: 57 [6400/48000 (13%)]	Loss: 0.003346, KL fake Loss: 12.738288
Classification Train Epoch: 57 [12800/48000 (27%)]	Loss: 0.000804, KL fake Loss: 12.688526
Classification Train Epoch: 57 [19200/48000 (40%)]	Loss: 0.008263, KL fake Loss: 12.140447
Classification Train Epoch: 57 [25600/48000 (53%)]	Loss: 0.002502, KL fake Loss: 12.326609
Classification Train Epoch: 57 [32000/48000 (67%)]	Loss: 0.008642, KL fake Loss: 13.192949
Classification Train Epoch: 57 [38400/48000 (80%)]	Loss: 0.003375, KL fake Loss: 12.958992
Classification Train Epoch: 57 [44800/48000 (93%)]	Loss: 0.007021, KL fake Loss: 12.915322

Test set: Average loss: 0.4613, Accuracy: 7385/8000 (92%)

Classification Train Epoch: 58 [0/48000 (0%)]	Loss: 0.000293, KL fake Loss: 12.467642
Classification Train Epoch: 58 [6400/48000 (13%)]	Loss: 0.000263, KL fake Loss: 12.568565
Classification Train Epoch: 58 [12800/48000 (27%)]	Loss: 0.001378, KL fake Loss: 13.068511
Classification Train Epoch: 58 [19200/48000 (40%)]	Loss: 0.004496, KL fake Loss: 12.802883
Classification Train Epoch: 58 [25600/48000 (53%)]	Loss: 0.000229, KL fake Loss: 12.285649
Classification Train Epoch: 58 [32000/48000 (67%)]	Loss: 0.021458, KL fake Loss: 12.398090
Classification Train Epoch: 58 [38400/48000 (80%)]	Loss: 0.000917, KL fake Loss: 12.406699
Classification Train Epoch: 58 [44800/48000 (93%)]	Loss: 0.005256, KL fake Loss: 12.707541

Test set: Average loss: 0.4298, Accuracy: 7408/8000 (93%)

Classification Train Epoch: 59 [0/48000 (0%)]	Loss: 0.000299, KL fake Loss: 12.915878
Classification Train Epoch: 59 [6400/48000 (13%)]	Loss: 0.022536, KL fake Loss: 12.465205
Classification Train Epoch: 59 [12800/48000 (27%)]	Loss: 0.000196, KL fake Loss: 13.400966
Classification Train Epoch: 59 [19200/48000 (40%)]	Loss: 0.003498, KL fake Loss: 13.028684
Classification Train Epoch: 59 [25600/48000 (53%)]	Loss: 0.007144, KL fake Loss: 13.370914
Classification Train Epoch: 59 [32000/48000 (67%)]	Loss: 0.002620, KL fake Loss: 12.378085
Classification Train Epoch: 59 [38400/48000 (80%)]	Loss: 0.007875, KL fake Loss: 12.613871
Classification Train Epoch: 59 [44800/48000 (93%)]	Loss: 0.129350, KL fake Loss: 12.625903

Test set: Average loss: 0.4301, Accuracy: 7399/8000 (92%)

Classification Train Epoch: 60 [0/48000 (0%)]	Loss: 0.044549, KL fake Loss: 13.060101
Classification Train Epoch: 60 [6400/48000 (13%)]	Loss: 0.002334, KL fake Loss: 13.300816
Classification Train Epoch: 60 [12800/48000 (27%)]	Loss: 0.007432, KL fake Loss: 12.546203
Classification Train Epoch: 60 [19200/48000 (40%)]	Loss: 0.005381, KL fake Loss: 13.003273
Classification Train Epoch: 60 [25600/48000 (53%)]	Loss: 0.026474, KL fake Loss: 12.828744
Classification Train Epoch: 60 [32000/48000 (67%)]	Loss: 0.049076, KL fake Loss: 12.840049
Classification Train Epoch: 60 [38400/48000 (80%)]	Loss: 0.007252, KL fake Loss: 11.942810
Classification Train Epoch: 60 [44800/48000 (93%)]	Loss: 0.002944, KL fake Loss: 13.264816

Test set: Average loss: 0.4558, Accuracy: 7404/8000 (93%)

Classification Train Epoch: 61 [0/48000 (0%)]	Loss: 0.033748, KL fake Loss: 12.640957
Classification Train Epoch: 61 [6400/48000 (13%)]	Loss: 0.001702, KL fake Loss: 12.450588
Classification Train Epoch: 61 [12800/48000 (27%)]	Loss: 0.015709, KL fake Loss: 13.000448
Classification Train Epoch: 61 [19200/48000 (40%)]	Loss: 0.000586, KL fake Loss: 12.571558
Classification Train Epoch: 61 [25600/48000 (53%)]	Loss: 0.002227, KL fake Loss: 12.116304
Classification Train Epoch: 61 [32000/48000 (67%)]	Loss: 0.001198, KL fake Loss: 13.119295
Classification Train Epoch: 61 [38400/48000 (80%)]	Loss: 0.000759, KL fake Loss: 13.436626
Classification Train Epoch: 61 [44800/48000 (93%)]	Loss: 0.000037, KL fake Loss: 13.581964

Test set: Average loss: 0.4055, Accuracy: 7464/8000 (93%)

Classification Train Epoch: 62 [0/48000 (0%)]	Loss: 0.000707, KL fake Loss: 12.784049
Classification Train Epoch: 62 [6400/48000 (13%)]	Loss: 0.000412, KL fake Loss: 13.582764
Classification Train Epoch: 62 [12800/48000 (27%)]	Loss: 0.000072, KL fake Loss: 13.883020
Classification Train Epoch: 62 [19200/48000 (40%)]	Loss: 0.000048, KL fake Loss: 13.554495
Classification Train Epoch: 62 [25600/48000 (53%)]	Loss: 0.000272, KL fake Loss: 12.856829
Classification Train Epoch: 62 [32000/48000 (67%)]	Loss: 0.000905, KL fake Loss: 13.150399
Classification Train Epoch: 62 [38400/48000 (80%)]	Loss: 0.002096, KL fake Loss: 13.275122
Classification Train Epoch: 62 [44800/48000 (93%)]	Loss: 0.000284, KL fake Loss: 13.851642

Test set: Average loss: 0.4061, Accuracy: 7459/8000 (93%)

Classification Train Epoch: 63 [0/48000 (0%)]	Loss: 0.000255, KL fake Loss: 13.069107
Classification Train Epoch: 63 [6400/48000 (13%)]	Loss: 0.000508, KL fake Loss: 13.600559
Classification Train Epoch: 63 [12800/48000 (27%)]	Loss: 0.000137, KL fake Loss: 12.836454
Classification Train Epoch: 63 [19200/48000 (40%)]	Loss: 0.000677, KL fake Loss: 13.592376
Classification Train Epoch: 63 [25600/48000 (53%)]	Loss: 0.000641, KL fake Loss: 13.449501
Classification Train Epoch: 63 [32000/48000 (67%)]	Loss: 0.000188, KL fake Loss: 13.322088
Classification Train Epoch: 63 [38400/48000 (80%)]	Loss: 0.000202, KL fake Loss: 12.611309
Classification Train Epoch: 63 [44800/48000 (93%)]	Loss: 0.000171, KL fake Loss: 13.636745
 63%|██████▎   | 63/100 [2:51:59<1:41:00, 163.79s/it] 58%|█████▊    | 58/100 [3:26:12<2:29:19, 213.32s/it] 64%|██████▍   | 64/100 [2:54:42<1:38:16, 163.79s/it] 65%|██████▌   | 65/100 [2:57:26<1:35:32, 163.79s/it] 59%|█████▉    | 59/100 [3:29:46<2:25:46, 213.32s/it] 66%|██████▌   | 66/100 [3:00:10<1:32:48, 163.78s/it] 60%|██████    | 60/100 [3:33:19<2:22:14, 213.35s/it] 67%|██████▋   | 67/100 [3:02:54<1:30:04, 163.79s/it] 61%|██████    | 61/100 [3:36:53<2:18:40, 213.34s/it] 68%|██████▊   | 68/100 [3:05:37<1:27:21, 163.79s/it] 69%|██████▉   | 69/100 [3:08:21<1:24:37, 163.79s/it] 62%|██████▏   | 62/100 [3:40:26<2:15:06, 213.33s/it] 70%|███████   | 70/100 [3:11:05<1:21:53, 163.79s/it] 63%|██████▎   | 63/100 [3:43:59<2:11:33, 213.33s/it]Classification Train Epoch: 53 [32000/50000 (64%)]	Loss: 0.025662, KL fake Loss: 0.004369
Classification Train Epoch: 53 [38400/50000 (77%)]	Loss: 0.008593, KL fake Loss: 0.037840
Classification Train Epoch: 53 [44800/50000 (90%)]	Loss: 0.020047, KL fake Loss: 0.036187

Test set: Average loss: 4.2299, Accuracy: 6021/10000 (60%)

Classification Train Epoch: 54 [0/50000 (0%)]	Loss: 0.003029, KL fake Loss: 0.215422
Classification Train Epoch: 54 [6400/50000 (13%)]	Loss: 0.027178, KL fake Loss: 0.003523
Classification Train Epoch: 54 [12800/50000 (26%)]	Loss: 0.025900, KL fake Loss: 0.005478
Classification Train Epoch: 54 [19200/50000 (38%)]	Loss: 0.126295, KL fake Loss: 0.000951
Classification Train Epoch: 54 [25600/50000 (51%)]	Loss: 0.003788, KL fake Loss: 0.011625
Classification Train Epoch: 54 [32000/50000 (64%)]	Loss: 0.014482, KL fake Loss: 0.005507
Classification Train Epoch: 54 [38400/50000 (77%)]	Loss: 0.012285, KL fake Loss: 0.001976
Classification Train Epoch: 54 [44800/50000 (90%)]	Loss: 0.024372, KL fake Loss: 0.002875

Test set: Average loss: 4.5970, Accuracy: 5390/10000 (54%)

Classification Train Epoch: 55 [0/50000 (0%)]	Loss: 0.015588, KL fake Loss: 0.005888
Classification Train Epoch: 55 [6400/50000 (13%)]	Loss: 0.025331, KL fake Loss: 0.001290
Classification Train Epoch: 55 [12800/50000 (26%)]	Loss: 0.009349, KL fake Loss: 0.001800
Classification Train Epoch: 55 [19200/50000 (38%)]	Loss: 0.001829, KL fake Loss: 0.001644
Classification Train Epoch: 55 [25600/50000 (51%)]	Loss: 0.020217, KL fake Loss: 0.006091
Classification Train Epoch: 55 [32000/50000 (64%)]	Loss: 0.026842, KL fake Loss: 0.005245
Classification Train Epoch: 55 [38400/50000 (77%)]	Loss: 0.017997, KL fake Loss: 0.007555
Classification Train Epoch: 55 [44800/50000 (90%)]	Loss: 0.036388, KL fake Loss: 0.022258

Test set: Average loss: 4.9480, Accuracy: 5377/10000 (54%)

Classification Train Epoch: 56 [0/50000 (0%)]	Loss: 0.011255, KL fake Loss: 0.010937
Classification Train Epoch: 56 [6400/50000 (13%)]	Loss: 0.001469, KL fake Loss: 0.002778
Classification Train Epoch: 56 [12800/50000 (26%)]	Loss: 0.002195, KL fake Loss: 0.003600
Classification Train Epoch: 56 [19200/50000 (38%)]	Loss: 0.006639, KL fake Loss: 0.001396
Classification Train Epoch: 56 [25600/50000 (51%)]	Loss: 0.001314, KL fake Loss: 0.006495
Classification Train Epoch: 56 [32000/50000 (64%)]	Loss: 0.048286, KL fake Loss: 0.002771
Classification Train Epoch: 56 [38400/50000 (77%)]	Loss: 0.037416, KL fake Loss: 0.002017
Classification Train Epoch: 56 [44800/50000 (90%)]	Loss: 0.001796, KL fake Loss: 0.000434

Test set: Average loss: 4.9576, Accuracy: 5388/10000 (54%)

Classification Train Epoch: 57 [0/50000 (0%)]	Loss: 0.005658, KL fake Loss: 0.002375
Classification Train Epoch: 57 [6400/50000 (13%)]	Loss: 0.008448, KL fake Loss: 0.004429
Classification Train Epoch: 57 [12800/50000 (26%)]	Loss: 0.007046, KL fake Loss: 0.002292
Classification Train Epoch: 57 [19200/50000 (38%)]	Loss: 0.162720, KL fake Loss: 0.001013
Classification Train Epoch: 57 [25600/50000 (51%)]	Loss: 0.010235, KL fake Loss: 0.001033
Classification Train Epoch: 57 [32000/50000 (64%)]	Loss: 0.050394, KL fake Loss: 0.001855
Classification Train Epoch: 57 [38400/50000 (77%)]	Loss: 0.010632, KL fake Loss: 0.005035
Classification Train Epoch: 57 [44800/50000 (90%)]	Loss: 0.058347, KL fake Loss: 0.002484

Test set: Average loss: 4.4326, Accuracy: 5933/10000 (59%)

Classification Train Epoch: 58 [0/50000 (0%)]	Loss: 0.006295, KL fake Loss: 0.006962
Classification Train Epoch: 58 [6400/50000 (13%)]	Loss: 0.025374, KL fake Loss: 0.011455
Classification Train Epoch: 58 [12800/50000 (26%)]	Loss: 0.012607, KL fake Loss: 0.000725
Classification Train Epoch: 58 [19200/50000 (38%)]	Loss: 0.023367, KL fake Loss: 0.001785
Classification Train Epoch: 58 [25600/50000 (51%)]	Loss: 0.003901, KL fake Loss: 0.003618
Classification Train Epoch: 58 [32000/50000 (64%)]	Loss: 0.004733, KL fake Loss: 0.010641
Classification Train Epoch: 58 [38400/50000 (77%)]	Loss: 0.006409, KL fake Loss: 0.007490
Classification Train Epoch: 58 [44800/50000 (90%)]	Loss: 0.009710, KL fake Loss: 0.011090

Test set: Average loss: 3.0461, Accuracy: 6190/10000 (62%)

Classification Train Epoch: 59 [0/50000 (0%)]	Loss: 0.011136, KL fake Loss: 0.022970
Classification Train Epoch: 59 [6400/50000 (13%)]	Loss: 0.007792, KL fake Loss: 0.011482
Classification Train Epoch: 59 [12800/50000 (26%)]	Loss: 0.004922, KL fake Loss: 0.004168
Classification Train Epoch: 59 [19200/50000 (38%)]	Loss: 0.008683, KL fake Loss: 0.004280
Classification Train Epoch: 59 [25600/50000 (51%)]	Loss: 0.034975, KL fake Loss: 0.005037
Classification Train Epoch: 59 [32000/50000 (64%)]	Loss: 0.064071, KL fake Loss: 0.005310
Classification Train Epoch: 59 [38400/50000 (77%)]	Loss: 0.008977, KL fake Loss: 0.068101
Classification Train Epoch: 59 [44800/50000 (90%)]	Loss: 0.020893, KL fake Loss: 0.001303

Test set: Average loss: 5.0096, Accuracy: 5558/10000 (56%)

Classification Train Epoch: 60 [0/50000 (0%)]	Loss: 0.033338, KL fake Loss: 0.005349
Classification Train Epoch: 60 [6400/50000 (13%)]	Loss: 0.008175, KL fake Loss: 0.001154
Classification Train Epoch: 60 [12800/50000 (26%)]	Loss: 0.103319, KL fake Loss: 0.000350
Classification Train Epoch: 60 [19200/50000 (38%)]	Loss: 0.008057, KL fake Loss: 0.001716
Classification Train Epoch: 60 [25600/50000 (51%)]	Loss: 0.047601, KL fake Loss: 0.007391
Classification Train Epoch: 60 [32000/50000 (64%)]	Loss: 0.029937, KL fake Loss: 0.001580
Classification Train Epoch: 60 [38400/50000 (77%)]	Loss: 0.006534, KL fake Loss: 0.001677
Classification Train Epoch: 60 [44800/50000 (90%)]	Loss: 0.041964, KL fake Loss: 0.026940

Test set: Average loss: 5.1424, Accuracy: 5966/10000 (60%)

Classification Train Epoch: 61 [0/50000 (0%)]	Loss: 0.006786, KL fake Loss: 0.008880
Classification Train Epoch: 61 [6400/50000 (13%)]	Loss: 0.002866, KL fake Loss: 0.000060
Classification Train Epoch: 61 [12800/50000 (26%)]	Loss: 0.000941, KL fake Loss: 0.000135
Classification Train Epoch: 61 [19200/50000 (38%)]	Loss: 0.001135, KL fake Loss: 0.000028
Classification Train Epoch: 61 [25600/50000 (51%)]	Loss: 0.001568, KL fake Loss: 0.000026
Classification Train Epoch: 61 [32000/50000 (64%)]	Loss: 0.012320, KL fake Loss: 0.000073
Classification Train Epoch: 61 [38400/50000 (77%)]	Loss: 0.004226, KL fake Loss: 0.000020
Classification Train Epoch: 61 [44800/50000 (90%)]	Loss: 0.013654, KL fake Loss: 0.000076

Test set: Average loss: 4.8630, Accuracy: 6093/10000 (61%)

Classification Train Epoch: 62 [0/50000 (0%)]	Loss: 0.000512, KL fake Loss: 0.000029
Classification Train Epoch: 62 [6400/50000 (13%)]	Loss: 0.002767, KL fake Loss: 0.000073
Classification Train Epoch: 62 [12800/50000 (26%)]	Loss: 0.000473, KL fake Loss: 0.000004
Classification Train Epoch: 62 [19200/50000 (38%)]	Loss: 0.001125, KL fake Loss: 0.000024
Classification Train Epoch: 62 [25600/50000 (51%)]	Loss: 0.009341, KL fake Loss: 0.000022
Classification Train Epoch: 62 [32000/50000 (64%)]	Loss: 0.000816, KL fake Loss: 0.000015
Classification Train Epoch: 62 [38400/50000 (77%)]	Loss: 0.005006, KL fake Loss: 0.000003
Classification Train Epoch: 62 [44800/50000 (90%)]	Loss: 0.001163, KL fake Loss: 0.000076

Test set: Average loss: 4.9015, Accuracy: 6122/10000 (61%)

Classification Train Epoch: 63 [0/50000 (0%)]	Loss: 0.005335, KL fake Loss: 0.000005
Classification Train Epoch: 63 [6400/50000 (13%)]	Loss: 0.000182, KL fake Loss: 0.000019
Classification Train Epoch: 63 [12800/50000 (26%)]	Loss: 0.000281, KL fake Loss: 0.000002
Classification Train Epoch: 63 [19200/50000 (38%)]	Loss: 0.001175, KL fake Loss: 0.000007
Classification Train Epoch: 63 [25600/50000 (51%)]	Loss: 0.002823, KL fake Loss: 0.000011
Classification Train Epoch: 63 [32000/50000 (64%)]	Loss: 0.002420, KL fake Loss: 0.000007
Classification Train Epoch: 63 [38400/50000 (77%)]	Loss: 0.000145, KL fake Loss: 0.000027
Classification Train Epoch: 63 [44800/50000 (90%)]	Loss: 0.000995, KL fake Loss: 0.000003

Test set: Average loss: 4.9510, Accuracy: 6063/10000 (61%)

Classification Train Epoch: 64 [0/50000 (0%)]	Loss: 0.000360, KL fake Loss: 0.000001
 71%|███████   | 71/100 [3:13:49<1:19:09, 163.78s/it] 64%|██████▍   | 64/100 [3:47:32<2:07:59, 213.33s/it] 72%|███████▏  | 72/100 [3:16:33<1:16:26, 163.79s/it] 65%|██████▌   | 65/100 [3:51:06<2:04:26, 213.33s/it] 73%|███████▎  | 73/100 [3:19:16<1:13:42, 163.79s/it]
Test set: Average loss: 0.4078, Accuracy: 7463/8000 (93%)

Classification Train Epoch: 64 [0/48000 (0%)]	Loss: 0.000058, KL fake Loss: 13.930578
Classification Train Epoch: 64 [6400/48000 (13%)]	Loss: 0.000210, KL fake Loss: 12.818943
Classification Train Epoch: 64 [12800/48000 (27%)]	Loss: 0.000285, KL fake Loss: 12.825644
Classification Train Epoch: 64 [19200/48000 (40%)]	Loss: 0.000335, KL fake Loss: 13.308403
Classification Train Epoch: 64 [25600/48000 (53%)]	Loss: 0.000432, KL fake Loss: 13.305079
Classification Train Epoch: 64 [32000/48000 (67%)]	Loss: 0.000615, KL fake Loss: 12.922462
Classification Train Epoch: 64 [38400/48000 (80%)]	Loss: 0.000096, KL fake Loss: 13.278010
Classification Train Epoch: 64 [44800/48000 (93%)]	Loss: 0.000732, KL fake Loss: 14.132931

Test set: Average loss: 0.4140, Accuracy: 7471/8000 (93%)

Classification Train Epoch: 65 [0/48000 (0%)]	Loss: 0.000118, KL fake Loss: 12.610541
Classification Train Epoch: 65 [6400/48000 (13%)]	Loss: 0.000126, KL fake Loss: 13.184616
Classification Train Epoch: 65 [12800/48000 (27%)]	Loss: 0.000054, KL fake Loss: 12.767190
Classification Train Epoch: 65 [19200/48000 (40%)]	Loss: 0.000055, KL fake Loss: 12.306517
Classification Train Epoch: 65 [25600/48000 (53%)]	Loss: 0.000190, KL fake Loss: 12.158932
Classification Train Epoch: 65 [32000/48000 (67%)]	Loss: 0.000179, KL fake Loss: 13.472490
Classification Train Epoch: 65 [38400/48000 (80%)]	Loss: 0.000407, KL fake Loss: 12.868154
Classification Train Epoch: 65 [44800/48000 (93%)]	Loss: 0.000125, KL fake Loss: 12.058640

Test set: Average loss: 0.4181, Accuracy: 7465/8000 (93%)

Classification Train Epoch: 66 [0/48000 (0%)]	Loss: 0.000180, KL fake Loss: 12.569191
Classification Train Epoch: 66 [6400/48000 (13%)]	Loss: 0.000281, KL fake Loss: 12.361315
Classification Train Epoch: 66 [12800/48000 (27%)]	Loss: 0.000097, KL fake Loss: 13.133093
Classification Train Epoch: 66 [19200/48000 (40%)]	Loss: 0.000194, KL fake Loss: 13.062759
Classification Train Epoch: 66 [25600/48000 (53%)]	Loss: 0.000194, KL fake Loss: 13.115841
Classification Train Epoch: 66 [32000/48000 (67%)]	Loss: 0.000639, KL fake Loss: 12.118793
Classification Train Epoch: 66 [38400/48000 (80%)]	Loss: 0.000129, KL fake Loss: 12.039528
Classification Train Epoch: 66 [44800/48000 (93%)]	Loss: 0.000119, KL fake Loss: 12.761783

Test set: Average loss: 0.4164, Accuracy: 7470/8000 (93%)

Classification Train Epoch: 67 [0/48000 (0%)]	Loss: 0.000235, KL fake Loss: 12.721214
Classification Train Epoch: 67 [6400/48000 (13%)]	Loss: 0.000125, KL fake Loss: 11.973734
Classification Train Epoch: 67 [12800/48000 (27%)]	Loss: 0.000418, KL fake Loss: 12.796930
Classification Train Epoch: 67 [19200/48000 (40%)]	Loss: 0.001176, KL fake Loss: 13.800826
Classification Train Epoch: 67 [25600/48000 (53%)]	Loss: 0.000039, KL fake Loss: 12.488111
Classification Train Epoch: 67 [32000/48000 (67%)]	Loss: 0.000097, KL fake Loss: 12.791141
Classification Train Epoch: 67 [38400/48000 (80%)]	Loss: 0.000098, KL fake Loss: 12.057732
Classification Train Epoch: 67 [44800/48000 (93%)]	Loss: 0.000199, KL fake Loss: 12.844903

Test set: Average loss: 0.4215, Accuracy: 7477/8000 (93%)

Classification Train Epoch: 68 [0/48000 (0%)]	Loss: 0.000115, KL fake Loss: 12.786008
Classification Train Epoch: 68 [6400/48000 (13%)]	Loss: 0.000151, KL fake Loss: 11.729811
Classification Train Epoch: 68 [12800/48000 (27%)]	Loss: 0.000041, KL fake Loss: 12.209084
Classification Train Epoch: 68 [19200/48000 (40%)]	Loss: 0.000155, KL fake Loss: 11.705543
Classification Train Epoch: 68 [25600/48000 (53%)]	Loss: 0.000139, KL fake Loss: 12.849752
Classification Train Epoch: 68 [32000/48000 (67%)]	Loss: 0.000056, KL fake Loss: 11.858647
Classification Train Epoch: 68 [38400/48000 (80%)]	Loss: 0.000119, KL fake Loss: 11.986853
Classification Train Epoch: 68 [44800/48000 (93%)]	Loss: 0.000639, KL fake Loss: 12.634819

Test set: Average loss: 0.4295, Accuracy: 7464/8000 (93%)

Classification Train Epoch: 69 [0/48000 (0%)]	Loss: 0.000047, KL fake Loss: 11.669748
Classification Train Epoch: 69 [6400/48000 (13%)]	Loss: 0.000061, KL fake Loss: 12.214166
Classification Train Epoch: 69 [12800/48000 (27%)]	Loss: 0.000291, KL fake Loss: 12.071083
Classification Train Epoch: 69 [19200/48000 (40%)]	Loss: 0.000138, KL fake Loss: 11.970502
Classification Train Epoch: 69 [25600/48000 (53%)]	Loss: 0.000296, KL fake Loss: 11.978268
Classification Train Epoch: 69 [32000/48000 (67%)]	Loss: 0.000756, KL fake Loss: 12.333805
Classification Train Epoch: 69 [38400/48000 (80%)]	Loss: 0.000139, KL fake Loss: 12.212955
Classification Train Epoch: 69 [44800/48000 (93%)]	Loss: 0.000219, KL fake Loss: 12.521168

Test set: Average loss: 0.4272, Accuracy: 7470/8000 (93%)

Classification Train Epoch: 70 [0/48000 (0%)]	Loss: 0.000220, KL fake Loss: 12.034710
Classification Train Epoch: 70 [6400/48000 (13%)]	Loss: 0.000144, KL fake Loss: 11.994394
Classification Train Epoch: 70 [12800/48000 (27%)]	Loss: 0.000417, KL fake Loss: 12.067257
Classification Train Epoch: 70 [19200/48000 (40%)]	Loss: 0.000080, KL fake Loss: 12.078611
Classification Train Epoch: 70 [25600/48000 (53%)]	Loss: 0.000069, KL fake Loss: 11.584485
Classification Train Epoch: 70 [32000/48000 (67%)]	Loss: 0.000072, KL fake Loss: 11.976583
Classification Train Epoch: 70 [38400/48000 (80%)]	Loss: 0.000121, KL fake Loss: 11.075640
Classification Train Epoch: 70 [44800/48000 (93%)]	Loss: 0.000027, KL fake Loss: 11.873118

Test set: Average loss: 0.4336, Accuracy: 7462/8000 (93%)

Classification Train Epoch: 71 [0/48000 (0%)]	Loss: 0.001107, KL fake Loss: 12.046515
Classification Train Epoch: 71 [6400/48000 (13%)]	Loss: 0.000048, KL fake Loss: 12.140373
Classification Train Epoch: 71 [12800/48000 (27%)]	Loss: 0.000153, KL fake Loss: 10.873877
Classification Train Epoch: 71 [19200/48000 (40%)]	Loss: 0.000160, KL fake Loss: 11.109533
Classification Train Epoch: 71 [25600/48000 (53%)]	Loss: 0.000141, KL fake Loss: 11.431293
Classification Train Epoch: 71 [32000/48000 (67%)]	Loss: 0.000048, KL fake Loss: 10.986678
Classification Train Epoch: 71 [38400/48000 (80%)]	Loss: 0.000322, KL fake Loss: 11.346078
Classification Train Epoch: 71 [44800/48000 (93%)]	Loss: 0.000217, KL fake Loss: 11.873569

Test set: Average loss: 0.4261, Accuracy: 7464/8000 (93%)

Classification Train Epoch: 72 [0/48000 (0%)]	Loss: 0.000158, KL fake Loss: 11.517138
Classification Train Epoch: 72 [6400/48000 (13%)]	Loss: 0.000207, KL fake Loss: 10.724005
Classification Train Epoch: 72 [12800/48000 (27%)]	Loss: 0.000144, KL fake Loss: 11.511715
Classification Train Epoch: 72 [19200/48000 (40%)]	Loss: 0.000167, KL fake Loss: 12.025367
Classification Train Epoch: 72 [25600/48000 (53%)]	Loss: 0.000142, KL fake Loss: 11.685117
Classification Train Epoch: 72 [32000/48000 (67%)]	Loss: 0.000138, KL fake Loss: 10.757657
Classification Train Epoch: 72 [38400/48000 (80%)]	Loss: 0.000116, KL fake Loss: 10.427010
Classification Train Epoch: 72 [44800/48000 (93%)]	Loss: 0.000057, KL fake Loss: 10.919275

Test set: Average loss: 0.4208, Accuracy: 7471/8000 (93%)

Classification Train Epoch: 73 [0/48000 (0%)]	Loss: 0.000033, KL fake Loss: 11.148640
Classification Train Epoch: 73 [6400/48000 (13%)]	Loss: 0.000082, KL fake Loss: 10.420259
Classification Train Epoch: 73 [12800/48000 (27%)]	Loss: 0.000036, KL fake Loss: 11.420889
Classification Train Epoch: 73 [19200/48000 (40%)]	Loss: 0.000074, KL fake Loss: 11.068011
Classification Train Epoch: 73 [25600/48000 (53%)]	Loss: 0.000259, KL fake Loss: 11.163126
Classification Train Epoch: 73 [32000/48000 (67%)]	Loss: 0.000031, KL fake Loss: 10.446829
Classification Train Epoch: 73 [38400/48000 (80%)]	Loss: 0.000096, KL fake Loss: 10.914665
Classification Train Epoch: 73 [44800/48000 (93%)]	Loss: 0.000088, KL fake Loss: 11.373666

Test set: Average loss: 0.4279, Accuracy: 7477/8000 (93%)

Classification Train Epoch: 74 [0/48000 (0%)]	Loss: 0.000169, KL fake Loss: 10.967329
Classification Train Epoch: 74 [6400/48000 (13%)]	Loss: 0.000037, KL fake Loss: 10.851301
Classification Train Epoch: 74 [12800/48000 (27%)]	Loss: 0.000125, KL fake Loss: 10.211370
 74%|███████▍  | 74/100 [3:22:00<1:10:58, 163.78s/it] 66%|██████▌   | 66/100 [3:54:39<2:00:52, 213.32s/it] 75%|███████▌  | 75/100 [3:24:44<1:08:14, 163.78s/it] 67%|██████▋   | 67/100 [3:58:12<1:57:19, 213.31s/it] 76%|███████▌  | 76/100 [3:27:28<1:05:30, 163.78s/it] 68%|██████▊   | 68/100 [4:01:46<1:53:45, 213.31s/it] 77%|███████▋  | 77/100 [3:30:12<1:02:46, 163.78s/it] 78%|███████▊  | 78/100 [3:32:55<1:00:03, 163.79s/it] 69%|██████▉   | 69/100 [4:05:19<1:50:12, 213.32s/it] 79%|███████▉  | 79/100 [3:35:39<57:19, 163.79s/it]   70%|███████   | 70/100 [4:08:52<1:46:40, 213.33s/it] 80%|████████  | 80/100 [3:38:23<54:36, 163.82s/it] 71%|███████   | 71/100 [4:12:26<1:43:06, 213.33s/it] 81%|████████  | 81/100 [3:41:07<51:52, 163.81s/it] 82%|████████▏ | 82/100 [3:43:51<49:08, 163.80s/it] 72%|███████▏  | 72/100 [4:15:59<1:39:33, 213.33s/it] 83%|████████▎ | 83/100 [3:46:34<46:24, 163.80s/it] 73%|███████▎  | 73/100 [4:19:32<1:35:59, 213.32s/it]Classification Train Epoch: 74 [19200/48000 (40%)]	Loss: 0.000060, KL fake Loss: 9.928406
Classification Train Epoch: 74 [25600/48000 (53%)]	Loss: 0.000084, KL fake Loss: 10.854717
Classification Train Epoch: 74 [32000/48000 (67%)]	Loss: 0.001307, KL fake Loss: 10.857238
Classification Train Epoch: 74 [38400/48000 (80%)]	Loss: 0.000066, KL fake Loss: 10.609705
Classification Train Epoch: 74 [44800/48000 (93%)]	Loss: 0.000563, KL fake Loss: 10.507784

Test set: Average loss: 0.4281, Accuracy: 7473/8000 (93%)

Classification Train Epoch: 75 [0/48000 (0%)]	Loss: 0.000109, KL fake Loss: 10.397462
Classification Train Epoch: 75 [6400/48000 (13%)]	Loss: 0.000043, KL fake Loss: 10.311438
Classification Train Epoch: 75 [12800/48000 (27%)]	Loss: 0.000115, KL fake Loss: 10.925832
Classification Train Epoch: 75 [19200/48000 (40%)]	Loss: 0.000095, KL fake Loss: 10.881229
Classification Train Epoch: 75 [25600/48000 (53%)]	Loss: 0.000077, KL fake Loss: 10.927005
Classification Train Epoch: 75 [32000/48000 (67%)]	Loss: 0.000053, KL fake Loss: 10.429125
Classification Train Epoch: 75 [38400/48000 (80%)]	Loss: 0.000128, KL fake Loss: 10.495354
Classification Train Epoch: 75 [44800/48000 (93%)]	Loss: 0.000184, KL fake Loss: 10.506824

Test set: Average loss: 0.4147, Accuracy: 7468/8000 (93%)

Classification Train Epoch: 76 [0/48000 (0%)]	Loss: 0.000120, KL fake Loss: 10.296156
Classification Train Epoch: 76 [6400/48000 (13%)]	Loss: 0.000202, KL fake Loss: 10.038327
Classification Train Epoch: 76 [12800/48000 (27%)]	Loss: 0.000363, KL fake Loss: 10.159647
Classification Train Epoch: 76 [19200/48000 (40%)]	Loss: 0.000086, KL fake Loss: 9.962718
Classification Train Epoch: 76 [25600/48000 (53%)]	Loss: 0.000382, KL fake Loss: 10.853033
Classification Train Epoch: 76 [32000/48000 (67%)]	Loss: 0.000121, KL fake Loss: 9.558752
Classification Train Epoch: 76 [38400/48000 (80%)]	Loss: 0.000154, KL fake Loss: 9.822365
Classification Train Epoch: 76 [44800/48000 (93%)]	Loss: 0.000120, KL fake Loss: 9.798088

Test set: Average loss: 0.4114, Accuracy: 7460/8000 (93%)

Classification Train Epoch: 77 [0/48000 (0%)]	Loss: 0.000123, KL fake Loss: 10.939209
Classification Train Epoch: 77 [6400/48000 (13%)]	Loss: 0.000098, KL fake Loss: 10.446666
Classification Train Epoch: 77 [12800/48000 (27%)]	Loss: 0.000459, KL fake Loss: 10.156137
Classification Train Epoch: 77 [19200/48000 (40%)]	Loss: 0.000094, KL fake Loss: 10.012532
Classification Train Epoch: 77 [25600/48000 (53%)]	Loss: 0.000083, KL fake Loss: 10.217439
Classification Train Epoch: 77 [32000/48000 (67%)]	Loss: 0.000051, KL fake Loss: 9.733236
Classification Train Epoch: 77 [38400/48000 (80%)]	Loss: 0.000103, KL fake Loss: 9.949408
Classification Train Epoch: 77 [44800/48000 (93%)]	Loss: 0.000060, KL fake Loss: 10.066084

Test set: Average loss: 0.3998, Accuracy: 7478/8000 (93%)

Classification Train Epoch: 78 [0/48000 (0%)]	Loss: 0.000141, KL fake Loss: 10.056126
Classification Train Epoch: 78 [6400/48000 (13%)]	Loss: 0.000212, KL fake Loss: 9.645594
Classification Train Epoch: 78 [12800/48000 (27%)]	Loss: 0.000069, KL fake Loss: 9.975121
Classification Train Epoch: 78 [19200/48000 (40%)]	Loss: 0.000099, KL fake Loss: 9.928596
Classification Train Epoch: 78 [25600/48000 (53%)]	Loss: 0.000069, KL fake Loss: 9.931722
Classification Train Epoch: 78 [32000/48000 (67%)]	Loss: 0.000173, KL fake Loss: 9.257411
Classification Train Epoch: 78 [38400/48000 (80%)]	Loss: 0.000134, KL fake Loss: 9.483231
Classification Train Epoch: 78 [44800/48000 (93%)]	Loss: 0.000075, KL fake Loss: 8.959051

Test set: Average loss: 0.3965, Accuracy: 7481/8000 (94%)

Classification Train Epoch: 79 [0/48000 (0%)]	Loss: 0.000047, KL fake Loss: 9.651911
Classification Train Epoch: 79 [6400/48000 (13%)]	Loss: 0.000100, KL fake Loss: 9.291597
Classification Train Epoch: 79 [12800/48000 (27%)]	Loss: 0.000137, KL fake Loss: 9.717118
Classification Train Epoch: 79 [19200/48000 (40%)]	Loss: 0.000080, KL fake Loss: 9.475998
Classification Train Epoch: 79 [25600/48000 (53%)]	Loss: 0.000069, KL fake Loss: 8.159003
Classification Train Epoch: 79 [32000/48000 (67%)]	Loss: 0.000122, KL fake Loss: 9.242733
Classification Train Epoch: 79 [38400/48000 (80%)]	Loss: 0.000078, KL fake Loss: 10.122546
Classification Train Epoch: 79 [44800/48000 (93%)]	Loss: 0.000124, KL fake Loss: 9.683931

Test set: Average loss: 0.4036, Accuracy: 7465/8000 (93%)

Classification Train Epoch: 80 [0/48000 (0%)]	Loss: 0.000222, KL fake Loss: 9.722264
Classification Train Epoch: 80 [6400/48000 (13%)]	Loss: 0.000167, KL fake Loss: 9.568913
Classification Train Epoch: 80 [12800/48000 (27%)]	Loss: 0.000080, KL fake Loss: 9.356487
Classification Train Epoch: 80 [19200/48000 (40%)]	Loss: 0.000148, KL fake Loss: 9.133227
Classification Train Epoch: 80 [25600/48000 (53%)]	Loss: 0.000137, KL fake Loss: 9.112263
Classification Train Epoch: 80 [32000/48000 (67%)]	Loss: 0.000143, KL fake Loss: 9.407760
Classification Train Epoch: 80 [38400/48000 (80%)]	Loss: 0.000233, KL fake Loss: 8.710930
Classification Train Epoch: 80 [44800/48000 (93%)]	Loss: 0.000085, KL fake Loss: 9.596918

Test set: Average loss: 0.3940, Accuracy: 7465/8000 (93%)

Classification Train Epoch: 81 [0/48000 (0%)]	Loss: 0.000080, KL fake Loss: 8.709944
Classification Train Epoch: 81 [6400/48000 (13%)]	Loss: 0.000286, KL fake Loss: 9.136738
Classification Train Epoch: 81 [12800/48000 (27%)]	Loss: 0.000154, KL fake Loss: 8.953354
Classification Train Epoch: 81 [19200/48000 (40%)]	Loss: 0.000070, KL fake Loss: 9.040157
Classification Train Epoch: 81 [25600/48000 (53%)]	Loss: 0.000226, KL fake Loss: 9.424372
Classification Train Epoch: 81 [32000/48000 (67%)]	Loss: 0.000121, KL fake Loss: 9.121022
Classification Train Epoch: 81 [38400/48000 (80%)]	Loss: 0.000075, KL fake Loss: 9.078119
Classification Train Epoch: 81 [44800/48000 (93%)]	Loss: 0.000105, KL fake Loss: 9.403624

Test set: Average loss: 0.3881, Accuracy: 7476/8000 (93%)

Classification Train Epoch: 82 [0/48000 (0%)]	Loss: 0.000127, KL fake Loss: 9.359294
Classification Train Epoch: 82 [6400/48000 (13%)]	Loss: 0.000215, KL fake Loss: 8.691381
Classification Train Epoch: 82 [12800/48000 (27%)]	Loss: 0.000185, KL fake Loss: 9.231257
Classification Train Epoch: 82 [19200/48000 (40%)]	Loss: 0.000050, KL fake Loss: 9.214779
Classification Train Epoch: 82 [25600/48000 (53%)]	Loss: 0.000165, KL fake Loss: 8.401325
Classification Train Epoch: 82 [32000/48000 (67%)]	Loss: 0.000066, KL fake Loss: 8.802505
Classification Train Epoch: 82 [38400/48000 (80%)]	Loss: 0.000042, KL fake Loss: 8.921703
Classification Train Epoch: 82 [44800/48000 (93%)]	Loss: 0.000154, KL fake Loss: 8.387191

Test set: Average loss: 0.3792, Accuracy: 7483/8000 (94%)

Classification Train Epoch: 83 [0/48000 (0%)]	Loss: 0.000091, KL fake Loss: 9.412150
Classification Train Epoch: 83 [6400/48000 (13%)]	Loss: 0.000080, KL fake Loss: 9.016891
Classification Train Epoch: 83 [12800/48000 (27%)]	Loss: 0.000088, KL fake Loss: 7.938840
Classification Train Epoch: 83 [19200/48000 (40%)]	Loss: 0.000110, KL fake Loss: 8.098570
Classification Train Epoch: 83 [25600/48000 (53%)]	Loss: 0.000167, KL fake Loss: 8.774192
Classification Train Epoch: 83 [32000/48000 (67%)]	Loss: 0.000121, KL fake Loss: 8.442693
Classification Train Epoch: 83 [38400/48000 (80%)]	Loss: 0.000085, KL fake Loss: 8.852898
Classification Train Epoch: 83 [44800/48000 (93%)]	Loss: 0.000063, KL fake Loss: 8.491690

Test set: Average loss: 0.3714, Accuracy: 7483/8000 (94%)

Classification Train Epoch: 84 [0/48000 (0%)]	Loss: 0.000406, KL fake Loss: 8.492533
Classification Train Epoch: 84 [6400/48000 (13%)]	Loss: 0.000065, KL fake Loss: 8.453296
Classification Train Epoch: 84 [12800/48000 (27%)]	Loss: 0.000084, KL fake Loss: 8.464249
Classification Train Epoch: 84 [19200/48000 (40%)]	Loss: 0.000376, KL fake Loss: 8.415113
Classification Train Epoch: 84 [25600/48000 (53%)]	Loss: 0.000156, KL fake Loss: 8.546044
Classification Train Epoch: 84 [32000/48000 (67%)]	Loss: 0.000166, KL fake Loss: 7.430136
Classification Train Epoch: 84 [38400/48000 (80%)]	Loss: 0.000190, KL fake Loss: 8.275852
 84%|████████▍ | 84/100 [3:49:18<43:40, 163.79s/it]Classification Train Epoch: 64 [6400/50000 (13%)]	Loss: 0.000253, KL fake Loss: 0.000030
Classification Train Epoch: 64 [12800/50000 (26%)]	Loss: 0.000274, KL fake Loss: 0.000042
Classification Train Epoch: 64 [19200/50000 (38%)]	Loss: 0.000114, KL fake Loss: 0.000005
Classification Train Epoch: 64 [25600/50000 (51%)]	Loss: 0.001206, KL fake Loss: 0.000003
Classification Train Epoch: 64 [32000/50000 (64%)]	Loss: 0.000073, KL fake Loss: 0.000001
Classification Train Epoch: 64 [38400/50000 (77%)]	Loss: 0.001743, KL fake Loss: 0.000001
Classification Train Epoch: 64 [44800/50000 (90%)]	Loss: 0.000207, KL fake Loss: 0.000024

Test set: Average loss: 5.0778, Accuracy: 5979/10000 (60%)

Classification Train Epoch: 65 [0/50000 (0%)]	Loss: 0.002473, KL fake Loss: 0.000004
Classification Train Epoch: 65 [6400/50000 (13%)]	Loss: 0.000307, KL fake Loss: 0.000003
Classification Train Epoch: 65 [12800/50000 (26%)]	Loss: 0.000159, KL fake Loss: 0.000022
Classification Train Epoch: 65 [19200/50000 (38%)]	Loss: 0.000156, KL fake Loss: 0.000000
Classification Train Epoch: 65 [25600/50000 (51%)]	Loss: 0.000188, KL fake Loss: 0.000010
Classification Train Epoch: 65 [32000/50000 (64%)]	Loss: 0.001221, KL fake Loss: 0.000001
Classification Train Epoch: 65 [38400/50000 (77%)]	Loss: 0.000347, KL fake Loss: 0.000002
Classification Train Epoch: 65 [44800/50000 (90%)]	Loss: 0.000362, KL fake Loss: 0.000018

Test set: Average loss: 5.2092, Accuracy: 6028/10000 (60%)

Classification Train Epoch: 66 [0/50000 (0%)]	Loss: 0.000170, KL fake Loss: 0.000051
Classification Train Epoch: 66 [6400/50000 (13%)]	Loss: 0.000142, KL fake Loss: 0.000005
Classification Train Epoch: 66 [12800/50000 (26%)]	Loss: 0.000044, KL fake Loss: 0.000001
Classification Train Epoch: 66 [19200/50000 (38%)]	Loss: 0.000077, KL fake Loss: 0.000072
Classification Train Epoch: 66 [25600/50000 (51%)]	Loss: 0.001135, KL fake Loss: 0.000015
Classification Train Epoch: 66 [32000/50000 (64%)]	Loss: 0.001934, KL fake Loss: 0.000002
Classification Train Epoch: 66 [38400/50000 (77%)]	Loss: 0.000112, KL fake Loss: 0.000008
Classification Train Epoch: 66 [44800/50000 (90%)]	Loss: 0.000077, KL fake Loss: 0.000388

Test set: Average loss: 4.6494, Accuracy: 6245/10000 (62%)

Classification Train Epoch: 67 [0/50000 (0%)]	Loss: 0.000694, KL fake Loss: 0.000080
Classification Train Epoch: 67 [6400/50000 (13%)]	Loss: 0.000179, KL fake Loss: 0.000142
Classification Train Epoch: 67 [12800/50000 (26%)]	Loss: 0.000368, KL fake Loss: 0.000014
Classification Train Epoch: 67 [19200/50000 (38%)]	Loss: 0.000166, KL fake Loss: 0.000002
Classification Train Epoch: 67 [25600/50000 (51%)]	Loss: 0.001323, KL fake Loss: 0.000003
Classification Train Epoch: 67 [32000/50000 (64%)]	Loss: 0.000487, KL fake Loss: 0.000004
Classification Train Epoch: 67 [38400/50000 (77%)]	Loss: 0.000471, KL fake Loss: 0.000000
Classification Train Epoch: 67 [44800/50000 (90%)]	Loss: 0.000082, KL fake Loss: 0.000001

Test set: Average loss: 4.7326, Accuracy: 6247/10000 (62%)

Classification Train Epoch: 68 [0/50000 (0%)]	Loss: 0.000170, KL fake Loss: 0.000001
Classification Train Epoch: 68 [6400/50000 (13%)]	Loss: 0.001022, KL fake Loss: 0.000000
Classification Train Epoch: 68 [12800/50000 (26%)]	Loss: 0.000467, KL fake Loss: 0.001022
Classification Train Epoch: 68 [19200/50000 (38%)]	Loss: 0.000601, KL fake Loss: 0.000785
Classification Train Epoch: 68 [25600/50000 (51%)]	Loss: 0.006235, KL fake Loss: 0.000002
Classification Train Epoch: 68 [32000/50000 (64%)]	Loss: 0.000183, KL fake Loss: 0.000001
Classification Train Epoch: 68 [38400/50000 (77%)]	Loss: 0.000456, KL fake Loss: 0.000000
Classification Train Epoch: 68 [44800/50000 (90%)]	Loss: 0.000090, KL fake Loss: 0.000000

Test set: Average loss: 4.3659, Accuracy: 6426/10000 (64%)

Classification Train Epoch: 69 [0/50000 (0%)]	Loss: 0.000221, KL fake Loss: 0.000003
Classification Train Epoch: 69 [6400/50000 (13%)]	Loss: 0.000196, KL fake Loss: 0.000003
Classification Train Epoch: 69 [12800/50000 (26%)]	Loss: 0.000076, KL fake Loss: 0.000009
Classification Train Epoch: 69 [19200/50000 (38%)]	Loss: 0.000567, KL fake Loss: 0.004303
Classification Train Epoch: 69 [25600/50000 (51%)]	Loss: 0.000400, KL fake Loss: 0.000011
Classification Train Epoch: 69 [32000/50000 (64%)]	Loss: 0.000061, KL fake Loss: 0.000003
Classification Train Epoch: 69 [38400/50000 (77%)]	Loss: 0.000541, KL fake Loss: 0.000000
Classification Train Epoch: 69 [44800/50000 (90%)]	Loss: 0.000245, KL fake Loss: 0.000010

Test set: Average loss: 2.5065, Accuracy: 7275/10000 (73%)

Classification Train Epoch: 70 [0/50000 (0%)]	Loss: 0.000284, KL fake Loss: 0.000066
Classification Train Epoch: 70 [6400/50000 (13%)]	Loss: 0.000206, KL fake Loss: 0.000006
Classification Train Epoch: 70 [12800/50000 (26%)]	Loss: 0.000073, KL fake Loss: 0.000002
Classification Train Epoch: 70 [19200/50000 (38%)]	Loss: 0.000106, KL fake Loss: 0.000008
Classification Train Epoch: 70 [25600/50000 (51%)]	Loss: 0.000124, KL fake Loss: 0.000010
Classification Train Epoch: 70 [32000/50000 (64%)]	Loss: 0.000120, KL fake Loss: 0.000149
Classification Train Epoch: 70 [38400/50000 (77%)]	Loss: 0.000169, KL fake Loss: 0.000015
Classification Train Epoch: 70 [44800/50000 (90%)]	Loss: 0.000323, KL fake Loss: 0.000276

Test set: Average loss: 2.4036, Accuracy: 7157/10000 (72%)

Classification Train Epoch: 71 [0/50000 (0%)]	Loss: 0.000077, KL fake Loss: 0.000002
Classification Train Epoch: 71 [6400/50000 (13%)]	Loss: 0.000061, KL fake Loss: 0.000000
Classification Train Epoch: 71 [12800/50000 (26%)]	Loss: 0.000086, KL fake Loss: 0.000001
Classification Train Epoch: 71 [19200/50000 (38%)]	Loss: 0.000096, KL fake Loss: 0.000169
Classification Train Epoch: 71 [25600/50000 (51%)]	Loss: 0.000297, KL fake Loss: 0.000514
Classification Train Epoch: 71 [32000/50000 (64%)]	Loss: 0.000171, KL fake Loss: 0.002687
Classification Train Epoch: 71 [38400/50000 (77%)]	Loss: 0.000102, KL fake Loss: 0.000020
Classification Train Epoch: 71 [44800/50000 (90%)]	Loss: 0.000122, KL fake Loss: 0.000011

Test set: Average loss: 2.4796, Accuracy: 7193/10000 (72%)

Classification Train Epoch: 72 [0/50000 (0%)]	Loss: 0.000337, KL fake Loss: 0.000032
Classification Train Epoch: 72 [6400/50000 (13%)]	Loss: 0.000211, KL fake Loss: 0.000002
Classification Train Epoch: 72 [12800/50000 (26%)]	Loss: 0.000930, KL fake Loss: 0.000000
Classification Train Epoch: 72 [19200/50000 (38%)]	Loss: 0.000309, KL fake Loss: 0.000001
Classification Train Epoch: 72 [25600/50000 (51%)]	Loss: 0.000141, KL fake Loss: 0.000006
Classification Train Epoch: 72 [32000/50000 (64%)]	Loss: 0.000100, KL fake Loss: 0.000008
Classification Train Epoch: 72 [38400/50000 (77%)]	Loss: 0.000185, KL fake Loss: 0.000165
Classification Train Epoch: 72 [44800/50000 (90%)]	Loss: 0.000129, KL fake Loss: 0.000000

Test set: Average loss: 3.2642, Accuracy: 6833/10000 (68%)

Classification Train Epoch: 73 [0/50000 (0%)]	Loss: 0.000138, KL fake Loss: 0.000002
Classification Train Epoch: 73 [6400/50000 (13%)]	Loss: 0.000377, KL fake Loss: 0.000062
Classification Train Epoch: 73 [12800/50000 (26%)]	Loss: 0.000114, KL fake Loss: 0.000027
Classification Train Epoch: 73 [19200/50000 (38%)]	Loss: 0.002575, KL fake Loss: 0.000008
Classification Train Epoch: 73 [25600/50000 (51%)]	Loss: 0.000599, KL fake Loss: 0.000001
Classification Train Epoch: 73 [32000/50000 (64%)]	Loss: 0.000213, KL fake Loss: 0.000000
Classification Train Epoch: 73 [38400/50000 (77%)]	Loss: 0.000108, KL fake Loss: 0.000004
Classification Train Epoch: 73 [44800/50000 (90%)]	Loss: 0.000217, KL fake Loss: 0.000001

Test set: Average loss: 2.9731, Accuracy: 6954/10000 (70%)

Classification Train Epoch: 74 [0/50000 (0%)]	Loss: 0.000024, KL fake Loss: 0.000006
Classification Train Epoch: 74 [6400/50000 (13%)]	Loss: 0.000144, KL fake Loss: 0.000016
Classification Train Epoch: 74 [12800/50000 (26%)]	Loss: 0.000026, KL fake Loss: 0.000001
Classification Train Epoch: 74 [19200/50000 (38%)]	Loss: 0.000114, KL fake Loss: 0.000001
Classification Train Epoch: 74 [25600/50000 (51%)]	Loss: 0.000396, KL fake Loss: 0.000000
 74%|███████▍  | 74/100 [4:23:06<1:32:26, 213.32s/it] 85%|████████▌ | 85/100 [3:52:02<40:56, 163.78s/it] 86%|████████▌ | 86/100 [3:54:46<38:12, 163.78s/it] 75%|███████▌  | 75/100 [4:26:39<1:28:52, 213.31s/it] 87%|████████▋ | 87/100 [3:57:29<35:29, 163.78s/it] 76%|███████▌  | 76/100 [4:30:12<1:25:19, 213.31s/it] 88%|████████▊ | 88/100 [4:00:13<32:45, 163.78s/it] 77%|███████▋  | 77/100 [4:33:46<1:21:46, 213.32s/it] 89%|████████▉ | 89/100 [4:02:57<30:01, 163.78s/it] 78%|███████▊  | 78/100 [4:37:19<1:18:12, 213.31s/it] 90%|█████████ | 90/100 [4:05:41<27:17, 163.78s/it] 91%|█████████ | 91/100 [4:08:25<24:33, 163.78s/it] 79%|███████▉  | 79/100 [4:40:52<1:14:39, 213.31s/it] 92%|█████████▏| 92/100 [4:11:08<21:50, 163.78s/it] 80%|████████  | 80/100 [4:44:26<1:11:06, 213.34s/it] 93%|█████████▎| 93/100 [4:13:52<19:06, 163.77s/it] 81%|████████  | 81/100 [4:47:59<1:07:33, 213.34s/it] 94%|█████████▍| 94/100 [4:16:36<16:22, 163.78s/it]Classification Train Epoch: 84 [44800/48000 (93%)]	Loss: 0.000071, KL fake Loss: 7.850816

Test set: Average loss: 0.3779, Accuracy: 7471/8000 (93%)

Classification Train Epoch: 85 [0/48000 (0%)]	Loss: 0.000077, KL fake Loss: 7.253727
Classification Train Epoch: 85 [6400/48000 (13%)]	Loss: 0.000244, KL fake Loss: 8.432463
Classification Train Epoch: 85 [12800/48000 (27%)]	Loss: 0.000094, KL fake Loss: 8.198741
Classification Train Epoch: 85 [19200/48000 (40%)]	Loss: 0.000094, KL fake Loss: 8.235021
Classification Train Epoch: 85 [25600/48000 (53%)]	Loss: 0.000267, KL fake Loss: 8.240120
Classification Train Epoch: 85 [32000/48000 (67%)]	Loss: 0.000217, KL fake Loss: 8.945995
Classification Train Epoch: 85 [38400/48000 (80%)]	Loss: 0.000111, KL fake Loss: 8.701668
Classification Train Epoch: 85 [44800/48000 (93%)]	Loss: 0.000106, KL fake Loss: 7.493870

Test set: Average loss: 0.3765, Accuracy: 7479/8000 (93%)

Classification Train Epoch: 86 [0/48000 (0%)]	Loss: 0.000296, KL fake Loss: 8.382681
Classification Train Epoch: 86 [6400/48000 (13%)]	Loss: 0.000056, KL fake Loss: 8.009829
Classification Train Epoch: 86 [12800/48000 (27%)]	Loss: 0.000119, KL fake Loss: 7.871027
Classification Train Epoch: 86 [19200/48000 (40%)]	Loss: 0.000079, KL fake Loss: 7.992434
Classification Train Epoch: 86 [25600/48000 (53%)]	Loss: 0.000077, KL fake Loss: 8.439060
Classification Train Epoch: 86 [32000/48000 (67%)]	Loss: 0.000096, KL fake Loss: 7.417192
Classification Train Epoch: 86 [38400/48000 (80%)]	Loss: 0.000039, KL fake Loss: 8.261291
Classification Train Epoch: 86 [44800/48000 (93%)]	Loss: 0.000128, KL fake Loss: 8.143419

Test set: Average loss: 0.3734, Accuracy: 7479/8000 (93%)

Classification Train Epoch: 87 [0/48000 (0%)]	Loss: 0.000079, KL fake Loss: 8.167553
Classification Train Epoch: 87 [6400/48000 (13%)]	Loss: 0.000065, KL fake Loss: 7.927663
Classification Train Epoch: 87 [12800/48000 (27%)]	Loss: 0.000294, KL fake Loss: 7.785193
Classification Train Epoch: 87 [19200/48000 (40%)]	Loss: 0.000292, KL fake Loss: 7.430227
Classification Train Epoch: 87 [25600/48000 (53%)]	Loss: 0.000052, KL fake Loss: 7.431500
Classification Train Epoch: 87 [32000/48000 (67%)]	Loss: 0.000088, KL fake Loss: 7.806915
Classification Train Epoch: 87 [38400/48000 (80%)]	Loss: 0.000196, KL fake Loss: 7.936970
Classification Train Epoch: 87 [44800/48000 (93%)]	Loss: 0.000110, KL fake Loss: 8.095949

Test set: Average loss: 0.3691, Accuracy: 7485/8000 (94%)

Classification Train Epoch: 88 [0/48000 (0%)]	Loss: 0.000076, KL fake Loss: 7.310216
Classification Train Epoch: 88 [6400/48000 (13%)]	Loss: 0.000107, KL fake Loss: 7.245571
Classification Train Epoch: 88 [12800/48000 (27%)]	Loss: 0.000069, KL fake Loss: 7.567635
Classification Train Epoch: 88 [19200/48000 (40%)]	Loss: 0.000080, KL fake Loss: 7.642116
Classification Train Epoch: 88 [25600/48000 (53%)]	Loss: 0.000096, KL fake Loss: 8.121786
Classification Train Epoch: 88 [32000/48000 (67%)]	Loss: 0.000175, KL fake Loss: 7.839862
Classification Train Epoch: 88 [38400/48000 (80%)]	Loss: 0.000097, KL fake Loss: 7.386532
Classification Train Epoch: 88 [44800/48000 (93%)]	Loss: 0.000099, KL fake Loss: 6.815799

Test set: Average loss: 0.3682, Accuracy: 7477/8000 (93%)

Classification Train Epoch: 89 [0/48000 (0%)]	Loss: 0.000176, KL fake Loss: 8.200329
Classification Train Epoch: 89 [6400/48000 (13%)]	Loss: 0.000123, KL fake Loss: 7.701665
Classification Train Epoch: 89 [12800/48000 (27%)]	Loss: 0.000093, KL fake Loss: 7.929885
Classification Train Epoch: 89 [19200/48000 (40%)]	Loss: 0.000142, KL fake Loss: 8.055694
Classification Train Epoch: 89 [25600/48000 (53%)]	Loss: 0.000058, KL fake Loss: 7.840528
Classification Train Epoch: 89 [32000/48000 (67%)]	Loss: 0.000104, KL fake Loss: 7.813011
Classification Train Epoch: 89 [38400/48000 (80%)]	Loss: 0.000169, KL fake Loss: 7.503347
Classification Train Epoch: 89 [44800/48000 (93%)]	Loss: 0.000076, KL fake Loss: 7.319024

Test set: Average loss: 0.3659, Accuracy: 7479/8000 (93%)

Classification Train Epoch: 90 [0/48000 (0%)]	Loss: 0.000174, KL fake Loss: 7.578622
Classification Train Epoch: 90 [6400/48000 (13%)]	Loss: 0.000074, KL fake Loss: 7.858325
Classification Train Epoch: 90 [12800/48000 (27%)]	Loss: 0.000118, KL fake Loss: 7.802858
Classification Train Epoch: 90 [19200/48000 (40%)]	Loss: 0.000118, KL fake Loss: 7.463916
Classification Train Epoch: 90 [25600/48000 (53%)]	Loss: 0.000093, KL fake Loss: 7.076200
Classification Train Epoch: 90 [32000/48000 (67%)]	Loss: 0.000078, KL fake Loss: 7.487533
Classification Train Epoch: 90 [38400/48000 (80%)]	Loss: 0.000083, KL fake Loss: 7.519432
Classification Train Epoch: 90 [44800/48000 (93%)]	Loss: 0.000117, KL fake Loss: 7.554530

Test set: Average loss: 0.3639, Accuracy: 7489/8000 (94%)

Classification Train Epoch: 91 [0/48000 (0%)]	Loss: 0.000087, KL fake Loss: 7.123754
Classification Train Epoch: 91 [6400/48000 (13%)]	Loss: 0.000094, KL fake Loss: 7.924768
Classification Train Epoch: 91 [12800/48000 (27%)]	Loss: 0.000110, KL fake Loss: 7.646522
Classification Train Epoch: 91 [19200/48000 (40%)]	Loss: 0.000085, KL fake Loss: 7.069320
Classification Train Epoch: 91 [25600/48000 (53%)]	Loss: 0.000057, KL fake Loss: 7.461410
Classification Train Epoch: 91 [32000/48000 (67%)]	Loss: 0.000094, KL fake Loss: 7.302598
Classification Train Epoch: 91 [38400/48000 (80%)]	Loss: 0.000078, KL fake Loss: 7.139146
Classification Train Epoch: 91 [44800/48000 (93%)]	Loss: 0.000096, KL fake Loss: 6.743567

Test set: Average loss: 0.3666, Accuracy: 7478/8000 (93%)

Classification Train Epoch: 92 [0/48000 (0%)]	Loss: 0.000055, KL fake Loss: 7.155656
Classification Train Epoch: 92 [6400/48000 (13%)]	Loss: 0.000058, KL fake Loss: 6.926717
Classification Train Epoch: 92 [12800/48000 (27%)]	Loss: 0.000081, KL fake Loss: 7.213984
Classification Train Epoch: 92 [19200/48000 (40%)]	Loss: 0.000078, KL fake Loss: 8.020452
Classification Train Epoch: 92 [25600/48000 (53%)]	Loss: 0.000048, KL fake Loss: 6.488317
Classification Train Epoch: 92 [32000/48000 (67%)]	Loss: 0.000560, KL fake Loss: 6.994630
Classification Train Epoch: 92 [38400/48000 (80%)]	Loss: 0.000116, KL fake Loss: 7.518622
Classification Train Epoch: 92 [44800/48000 (93%)]	Loss: 0.000098, KL fake Loss: 7.457977

Test set: Average loss: 0.3633, Accuracy: 7492/8000 (94%)

Classification Train Epoch: 93 [0/48000 (0%)]	Loss: 0.000035, KL fake Loss: 6.911041
Classification Train Epoch: 93 [6400/48000 (13%)]	Loss: 0.000174, KL fake Loss: 7.546764
Classification Train Epoch: 93 [12800/48000 (27%)]	Loss: 0.000103, KL fake Loss: 7.722814
Classification Train Epoch: 93 [19200/48000 (40%)]	Loss: 0.000150, KL fake Loss: 6.640131
Classification Train Epoch: 93 [25600/48000 (53%)]	Loss: 0.000086, KL fake Loss: 6.838621
Classification Train Epoch: 93 [32000/48000 (67%)]	Loss: 0.000067, KL fake Loss: 6.608313
Classification Train Epoch: 93 [38400/48000 (80%)]	Loss: 0.000725, KL fake Loss: 6.823462
Classification Train Epoch: 93 [44800/48000 (93%)]	Loss: 0.000085, KL fake Loss: 6.837956

Test set: Average loss: 0.3668, Accuracy: 7480/8000 (94%)

Classification Train Epoch: 94 [0/48000 (0%)]	Loss: 0.000062, KL fake Loss: 6.575668
Classification Train Epoch: 94 [6400/48000 (13%)]	Loss: 0.000112, KL fake Loss: 6.744928
Classification Train Epoch: 94 [12800/48000 (27%)]	Loss: 0.000050, KL fake Loss: 7.384563
Classification Train Epoch: 94 [19200/48000 (40%)]	Loss: 0.000129, KL fake Loss: 6.581186
Classification Train Epoch: 94 [25600/48000 (53%)]	Loss: 0.000145, KL fake Loss: 6.964544
Classification Train Epoch: 94 [32000/48000 (67%)]	Loss: 0.000103, KL fake Loss: 6.986967
Classification Train Epoch: 94 [38400/48000 (80%)]	Loss: 0.000111, KL fake Loss: 6.690554
Classification Train Epoch: 94 [44800/48000 (93%)]	Loss: 0.000043, KL fake Loss: 7.392280

Test set: Average loss: 0.3669, Accuracy: 7475/8000 (93%)

Classification Train Epoch: 95 [0/48000 (0%)]	Loss: 0.000066, KL fake Loss: 6.381173
Classification Train Epoch: 95 [6400/48000 (13%)]	Loss: 0.000091, KL fake Loss: 6.592927
Classification Train Epoch: 95 [12800/48000 (27%)]	Loss: 0.000171, KL fake Loss: 6.344834
 95%|█████████▌| 95/100 [4:19:20<13:38, 163.78s/it] 82%|████████▏ | 82/100 [4:51:32<1:04:00, 213.34s/it] 96%|█████████▌| 96/100 [4:22:03<10:55, 163.78s/it] 83%|████████▎ | 83/100 [4:55:06<1:00:26, 213.33s/it] 97%|█████████▋| 97/100 [4:24:47<08:11, 163.79s/it] 84%|████████▍ | 84/100 [4:58:39<56:53, 213.31s/it]  Classification Train Epoch: 74 [32000/50000 (64%)]	Loss: 0.000114, KL fake Loss: 0.000001
Classification Train Epoch: 74 [38400/50000 (77%)]	Loss: 0.008143, KL fake Loss: 0.001055
Classification Train Epoch: 74 [44800/50000 (90%)]	Loss: 0.000040, KL fake Loss: 0.000000

Test set: Average loss: 2.6850, Accuracy: 6961/10000 (70%)

Classification Train Epoch: 75 [0/50000 (0%)]	Loss: 0.000040, KL fake Loss: 0.000002
Classification Train Epoch: 75 [6400/50000 (13%)]	Loss: 0.011820, KL fake Loss: 0.000001
Classification Train Epoch: 75 [12800/50000 (26%)]	Loss: 0.000116, KL fake Loss: 0.000000
Classification Train Epoch: 75 [19200/50000 (38%)]	Loss: 0.000091, KL fake Loss: 0.000003
Classification Train Epoch: 75 [25600/50000 (51%)]	Loss: 0.000185, KL fake Loss: 0.000023
Classification Train Epoch: 75 [32000/50000 (64%)]	Loss: 0.000047, KL fake Loss: 0.000031
Classification Train Epoch: 75 [38400/50000 (77%)]	Loss: 0.000477, KL fake Loss: 0.000014
Classification Train Epoch: 75 [44800/50000 (90%)]	Loss: 0.000159, KL fake Loss: 0.000012

Test set: Average loss: 2.6231, Accuracy: 6996/10000 (70%)

Classification Train Epoch: 76 [0/50000 (0%)]	Loss: 0.000024, KL fake Loss: 0.000002
Classification Train Epoch: 76 [6400/50000 (13%)]	Loss: 0.000147, KL fake Loss: 0.000001
Classification Train Epoch: 76 [12800/50000 (26%)]	Loss: 0.000086, KL fake Loss: 0.000110
Classification Train Epoch: 76 [19200/50000 (38%)]	Loss: 0.000162, KL fake Loss: 0.000000
Classification Train Epoch: 76 [25600/50000 (51%)]	Loss: 0.000034, KL fake Loss: 0.000001
Classification Train Epoch: 76 [32000/50000 (64%)]	Loss: 0.000208, KL fake Loss: 0.000001
Classification Train Epoch: 76 [38400/50000 (77%)]	Loss: 0.000042, KL fake Loss: 0.000010
Classification Train Epoch: 76 [44800/50000 (90%)]	Loss: 0.000103, KL fake Loss: 0.000000

Test set: Average loss: 2.6034, Accuracy: 7045/10000 (70%)

Classification Train Epoch: 77 [0/50000 (0%)]	Loss: 0.000061, KL fake Loss: 0.000001
Classification Train Epoch: 77 [6400/50000 (13%)]	Loss: 0.000228, KL fake Loss: 0.000000
Classification Train Epoch: 77 [12800/50000 (26%)]	Loss: 0.000472, KL fake Loss: 0.000006
Classification Train Epoch: 77 [19200/50000 (38%)]	Loss: 0.000012, KL fake Loss: 0.000001
Classification Train Epoch: 77 [25600/50000 (51%)]	Loss: 0.000049, KL fake Loss: 0.000003
Classification Train Epoch: 77 [32000/50000 (64%)]	Loss: 0.000700, KL fake Loss: 0.000248
Classification Train Epoch: 77 [38400/50000 (77%)]	Loss: 0.000144, KL fake Loss: 0.000003
Classification Train Epoch: 77 [44800/50000 (90%)]	Loss: 0.000045, KL fake Loss: 0.000104

Test set: Average loss: 2.1766, Accuracy: 7446/10000 (74%)

Classification Train Epoch: 78 [0/50000 (0%)]	Loss: 0.000337, KL fake Loss: 0.000028
Classification Train Epoch: 78 [6400/50000 (13%)]	Loss: 0.000023, KL fake Loss: 0.000000
Classification Train Epoch: 78 [12800/50000 (26%)]	Loss: 0.000489, KL fake Loss: 0.000001
Classification Train Epoch: 78 [19200/50000 (38%)]	Loss: 0.000032, KL fake Loss: 0.000001
Classification Train Epoch: 78 [25600/50000 (51%)]	Loss: 0.000027, KL fake Loss: 0.000003
Classification Train Epoch: 78 [32000/50000 (64%)]	Loss: 0.000163, KL fake Loss: 0.000000
Classification Train Epoch: 78 [38400/50000 (77%)]	Loss: 0.000022, KL fake Loss: 0.000000
Classification Train Epoch: 78 [44800/50000 (90%)]	Loss: 0.000064, KL fake Loss: 0.000001

Test set: Average loss: 2.0008, Accuracy: 7562/10000 (76%)

Classification Train Epoch: 79 [0/50000 (0%)]	Loss: 0.000057, KL fake Loss: 0.000003
Classification Train Epoch: 79 [6400/50000 (13%)]	Loss: 0.000047, KL fake Loss: 0.000254
Classification Train Epoch: 79 [12800/50000 (26%)]	Loss: 0.000013, KL fake Loss: 0.000000
Classification Train Epoch: 79 [19200/50000 (38%)]	Loss: 0.000020, KL fake Loss: 0.000002
Classification Train Epoch: 79 [25600/50000 (51%)]	Loss: 0.000010, KL fake Loss: 0.000002
Classification Train Epoch: 79 [32000/50000 (64%)]	Loss: 0.000032, KL fake Loss: 0.000001
Classification Train Epoch: 79 [38400/50000 (77%)]	Loss: 0.000036, KL fake Loss: 0.000000
Classification Train Epoch: 79 [44800/50000 (90%)]	Loss: 0.000094, KL fake Loss: 0.000003

Test set: Average loss: 2.1706, Accuracy: 7514/10000 (75%)

Classification Train Epoch: 80 [0/50000 (0%)]	Loss: 0.000291, KL fake Loss: 0.000030
Classification Train Epoch: 80 [6400/50000 (13%)]	Loss: 0.000078, KL fake Loss: 0.000004
Classification Train Epoch: 80 [12800/50000 (26%)]	Loss: 0.000743, KL fake Loss: 0.000007
Classification Train Epoch: 80 [19200/50000 (38%)]	Loss: 0.000114, KL fake Loss: 0.000000
Classification Train Epoch: 80 [25600/50000 (51%)]	Loss: 0.000017, KL fake Loss: 0.000000
Classification Train Epoch: 80 [32000/50000 (64%)]	Loss: 0.000010, KL fake Loss: 0.000000
Classification Train Epoch: 80 [38400/50000 (77%)]	Loss: 0.000038, KL fake Loss: 0.000001
Classification Train Epoch: 80 [44800/50000 (90%)]	Loss: 0.000021, KL fake Loss: 0.000000

Test set: Average loss: 2.0842, Accuracy: 7509/10000 (75%)

Classification Train Epoch: 81 [0/50000 (0%)]	Loss: 0.000123, KL fake Loss: 0.000560
Classification Train Epoch: 81 [6400/50000 (13%)]	Loss: 0.000287, KL fake Loss: 0.000260
Classification Train Epoch: 81 [12800/50000 (26%)]	Loss: 0.000052, KL fake Loss: 0.000119
Classification Train Epoch: 81 [19200/50000 (38%)]	Loss: 0.000122, KL fake Loss: 0.000946
Classification Train Epoch: 81 [25600/50000 (51%)]	Loss: 0.000025, KL fake Loss: 0.000001
Classification Train Epoch: 81 [32000/50000 (64%)]	Loss: 0.000043, KL fake Loss: 0.000267
Classification Train Epoch: 81 [38400/50000 (77%)]	Loss: 0.000071, KL fake Loss: 0.000026
Classification Train Epoch: 81 [44800/50000 (90%)]	Loss: 0.000097, KL fake Loss: 0.001313

Test set: Average loss: 2.7502, Accuracy: 7109/10000 (71%)

Classification Train Epoch: 82 [0/50000 (0%)]	Loss: 0.001168, KL fake Loss: 0.000006
Classification Train Epoch: 82 [6400/50000 (13%)]	Loss: 0.000087, KL fake Loss: 0.000009
Classification Train Epoch: 82 [12800/50000 (26%)]	Loss: 0.000038, KL fake Loss: 0.000000
Classification Train Epoch: 82 [19200/50000 (38%)]	Loss: 0.000127, KL fake Loss: 0.000000
Classification Train Epoch: 82 [25600/50000 (51%)]	Loss: 0.000059, KL fake Loss: 0.000009
Classification Train Epoch: 82 [32000/50000 (64%)]	Loss: 0.000644, KL fake Loss: 0.000001
Classification Train Epoch: 82 [38400/50000 (77%)]	Loss: 0.000096, KL fake Loss: 0.000000
Classification Train Epoch: 82 [44800/50000 (90%)]	Loss: 0.000023, KL fake Loss: 0.000000

Test set: Average loss: 2.7727, Accuracy: 7101/10000 (71%)

Classification Train Epoch: 83 [0/50000 (0%)]	Loss: 0.000179, KL fake Loss: 0.000008
Classification Train Epoch: 83 [6400/50000 (13%)]	Loss: 0.000039, KL fake Loss: 0.000001
Classification Train Epoch: 83 [12800/50000 (26%)]	Loss: 0.000611, KL fake Loss: -0.000000
Classification Train Epoch: 83 [19200/50000 (38%)]	Loss: 0.000039, KL fake Loss: 0.000000
Classification Train Epoch: 83 [25600/50000 (51%)]	Loss: 0.000062, KL fake Loss: 0.000000
Classification Train Epoch: 83 [32000/50000 (64%)]	Loss: 0.000035, KL fake Loss: 0.000001
Classification Train Epoch: 83 [38400/50000 (77%)]	Loss: 0.000113, KL fake Loss: 0.000000
Classification Train Epoch: 83 [44800/50000 (90%)]	Loss: 0.000065, KL fake Loss: 0.000037

Test set: Average loss: 2.9308, Accuracy: 6792/10000 (68%)

Classification Train Epoch: 84 [0/50000 (0%)]	Loss: 0.000018, KL fake Loss: 0.004819
Classification Train Epoch: 84 [6400/50000 (13%)]	Loss: 0.000014, KL fake Loss: 0.000042
Classification Train Epoch: 84 [12800/50000 (26%)]	Loss: 0.000034, KL fake Loss: 0.000000
Classification Train Epoch: 84 [19200/50000 (38%)]	Loss: 0.000090, KL fake Loss: 0.000000
Classification Train Epoch: 84 [25600/50000 (51%)]	Loss: 0.000102, KL fake Loss: 0.000000
Classification Train Epoch: 84 [32000/50000 (64%)]	Loss: 0.000172, KL fake Loss: 0.000000
Classification Train Epoch: 84 [38400/50000 (77%)]	Loss: 0.000043, KL fake Loss: 0.000000
Classification Train Epoch: 84 [44800/50000 (90%)]	Loss: 0.000119, KL fake Loss: 0.000016

Test set: Average loss: 3.2437, Accuracy: 6878/10000 (69%)

Classification Train Epoch: 85 [0/50000 (0%)]	Loss: 0.000105, KL fake Loss: 0.000603
 98%|█████████▊| 98/100 [4:27:31<05:27, 163.78s/it] 99%|█████████▉| 99/100 [4:30:15<02:43, 163.78s/it] 85%|████████▌ | 85/100 [5:02:12<53:19, 213.32s/it]100%|██████████| 100/100 [4:32:59<00:00, 163.82s/it]100%|██████████| 100/100 [4:32:59<00:00, 163.79s/it]
Classification Train Epoch: 95 [19200/48000 (40%)]	Loss: 0.000150, KL fake Loss: 6.761378
Classification Train Epoch: 95 [25600/48000 (53%)]	Loss: 0.000073, KL fake Loss: 7.142205
Classification Train Epoch: 95 [32000/48000 (67%)]	Loss: 0.000061, KL fake Loss: 6.140811
Classification Train Epoch: 95 [38400/48000 (80%)]	Loss: 0.000090, KL fake Loss: 6.327718
Classification Train Epoch: 95 [44800/48000 (93%)]	Loss: 0.000076, KL fake Loss: 6.997675

Test set: Average loss: 0.3640, Accuracy: 7473/8000 (93%)

Classification Train Epoch: 96 [0/48000 (0%)]	Loss: 0.000130, KL fake Loss: 6.938667
Classification Train Epoch: 96 [6400/48000 (13%)]	Loss: 0.000110, KL fake Loss: 5.995238
Classification Train Epoch: 96 [12800/48000 (27%)]	Loss: 0.000097, KL fake Loss: 6.131608
Classification Train Epoch: 96 [19200/48000 (40%)]	Loss: 0.000080, KL fake Loss: 6.780015
Classification Train Epoch: 96 [25600/48000 (53%)]	Loss: 0.000045, KL fake Loss: 5.707175
Classification Train Epoch: 96 [32000/48000 (67%)]	Loss: 0.000128, KL fake Loss: 7.006279
Classification Train Epoch: 96 [38400/48000 (80%)]	Loss: 0.000126, KL fake Loss: 6.006721
Classification Train Epoch: 96 [44800/48000 (93%)]	Loss: 0.000111, KL fake Loss: 6.111281

Test set: Average loss: 0.3663, Accuracy: 7474/8000 (93%)

Classification Train Epoch: 97 [0/48000 (0%)]	Loss: 0.000080, KL fake Loss: 6.493216
Classification Train Epoch: 97 [6400/48000 (13%)]	Loss: 0.000040, KL fake Loss: 6.148516
Classification Train Epoch: 97 [12800/48000 (27%)]	Loss: 0.000363, KL fake Loss: 6.223275
Classification Train Epoch: 97 [19200/48000 (40%)]	Loss: 0.000092, KL fake Loss: 5.772321
Classification Train Epoch: 97 [25600/48000 (53%)]	Loss: 0.000310, KL fake Loss: 5.866408
Classification Train Epoch: 97 [32000/48000 (67%)]	Loss: 0.000053, KL fake Loss: 6.904949
Classification Train Epoch: 97 [38400/48000 (80%)]	Loss: 0.000199, KL fake Loss: 6.849369
Classification Train Epoch: 97 [44800/48000 (93%)]	Loss: 0.000057, KL fake Loss: 6.197097

Test set: Average loss: 0.3777, Accuracy: 7465/8000 (93%)

Classification Train Epoch: 98 [0/48000 (0%)]	Loss: 0.000151, KL fake Loss: 5.327840
Classification Train Epoch: 98 [6400/48000 (13%)]	Loss: 0.000035, KL fake Loss: 5.656459
Classification Train Epoch: 98 [12800/48000 (27%)]	Loss: 0.000162, KL fake Loss: 5.724730
Classification Train Epoch: 98 [19200/48000 (40%)]	Loss: 0.000072, KL fake Loss: 5.598506
Classification Train Epoch: 98 [25600/48000 (53%)]	Loss: 0.000090, KL fake Loss: 6.116171
Classification Train Epoch: 98 [32000/48000 (67%)]	Loss: 0.000195, KL fake Loss: 5.759279
Classification Train Epoch: 98 [38400/48000 (80%)]	Loss: 0.000181, KL fake Loss: 5.423856
Classification Train Epoch: 98 [44800/48000 (93%)]	Loss: 0.000100, KL fake Loss: 5.805743

Test set: Average loss: 0.3703, Accuracy: 7467/8000 (93%)

Classification Train Epoch: 99 [0/48000 (0%)]	Loss: 0.000071, KL fake Loss: 5.632165
Classification Train Epoch: 99 [6400/48000 (13%)]	Loss: 0.000086, KL fake Loss: 5.671857
Classification Train Epoch: 99 [12800/48000 (27%)]	Loss: 0.000107, KL fake Loss: 5.226362
Classification Train Epoch: 99 [19200/48000 (40%)]	Loss: 0.000044, KL fake Loss: 5.749920
Classification Train Epoch: 99 [25600/48000 (53%)]	Loss: 0.000059, KL fake Loss: 4.795666
Classification Train Epoch: 99 [32000/48000 (67%)]	Loss: 0.000113, KL fake Loss: 5.970490
Classification Train Epoch: 99 [38400/48000 (80%)]	Loss: 0.000046, KL fake Loss: 5.250354
Classification Train Epoch: 99 [44800/48000 (93%)]	Loss: 0.000069, KL fake Loss: 4.881079

Test set: Average loss: 0.3670, Accuracy: 7471/8000 (93%)

Classification Train Epoch: 100 [0/48000 (0%)]	Loss: 0.000327, KL fake Loss: 4.844121
Classification Train Epoch: 100 [6400/48000 (13%)]	Loss: 0.000032, KL fake Loss: 4.577229
Classification Train Epoch: 100 [12800/48000 (27%)]	Loss: 0.000033, KL fake Loss: 5.381956
Classification Train Epoch: 100 [19200/48000 (40%)]	Loss: 0.000047, KL fake Loss: 5.592262
Classification Train Epoch: 100 [25600/48000 (53%)]	Loss: 0.000088, KL fake Loss: 5.420036
Classification Train Epoch: 100 [32000/48000 (67%)]	Loss: 0.000066, KL fake Loss: 5.217563
Classification Train Epoch: 100 [38400/48000 (80%)]	Loss: 0.000067, KL fake Loss: 5.098870
Classification Train Epoch: 100 [44800/48000 (93%)]	Loss: 0.000058, KL fake Loss: 5.910204

Test set: Average loss: 0.3644, Accuracy: 7473/8000 (93%)

Namespace(batch_size=128, no_cuda=False, seed=1, dataset='FashionMNIST', dataroot='./data', imageSize=32, outf='./results/joint_confidence_loss/FM-0.0001/', out_dataset='FashionMNIST', num_classes=8, num_channels=1, pre_trained_net='results/joint_confidence_loss/FM-0.0001/model_epoch_100.pth')
Random Seed:  1
Load model
DenseNet3(
  (conv1): Conv2d(1, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock(
    (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock(
    (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(342, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (fc): Linear(in_features=342, out_features=8, bias=True)
  (softmax): Softmax(dim=1)
)ic| len(dset): 60000
ic| len(dset): 10000
ic| len(dset): 60000
ic| len(dset): 10000
 86%|████████▌ | 86/100 [5:05:46<49:46, 213.31s/it]
load target data:  FashionMNIST
load non target data:  FashionMNIST
generate log from in-distribution data

 Final Accuracy: 7473/8000 (93.41%)

generate log  from out-of-distribution data
calculate metrics
  Performance of Baseline detector
TNR at TPR 95%:            34.182%
TNR at TPR 99%:             9.632%
AUROC:                     83.626%
Detection acc:             76.494%
AUPR In:                   84.373%
AUPR Out:                  81.918%
 87%|████████▋ | 87/100 [5:09:19<46:13, 213.31s/it] 88%|████████▊ | 88/100 [5:12:52<42:39, 213.30s/it] 89%|████████▉ | 89/100 [5:16:25<39:06, 213.30s/it] 90%|█████████ | 90/100 [5:19:59<35:33, 213.30s/it] 91%|█████████ | 91/100 [5:23:32<31:59, 213.30s/it] 92%|█████████▏| 92/100 [5:27:05<28:26, 213.31s/it] 93%|█████████▎| 93/100 [5:30:39<24:53, 213.32s/it] 94%|█████████▍| 94/100 [5:34:12<21:19, 213.32s/it]Classification Train Epoch: 85 [6400/50000 (13%)]	Loss: 0.000323, KL fake Loss: 0.000086
Classification Train Epoch: 85 [12800/50000 (26%)]	Loss: 0.000123, KL fake Loss: 0.000000
Classification Train Epoch: 85 [19200/50000 (38%)]	Loss: 0.000037, KL fake Loss: 0.000000
Classification Train Epoch: 85 [25600/50000 (51%)]	Loss: 0.000051, KL fake Loss: 0.000000
Classification Train Epoch: 85 [32000/50000 (64%)]	Loss: 0.000520, KL fake Loss: 0.000003
Classification Train Epoch: 85 [38400/50000 (77%)]	Loss: 0.000052, KL fake Loss: 0.000000
Classification Train Epoch: 85 [44800/50000 (90%)]	Loss: 0.000002, KL fake Loss: 0.000001

Test set: Average loss: 3.0789, Accuracy: 7049/10000 (70%)

Classification Train Epoch: 86 [0/50000 (0%)]	Loss: 0.000112, KL fake Loss: 0.001150
Classification Train Epoch: 86 [6400/50000 (13%)]	Loss: 0.000032, KL fake Loss: 0.005151
Classification Train Epoch: 86 [12800/50000 (26%)]	Loss: 0.000139, KL fake Loss: 0.000465
Classification Train Epoch: 86 [19200/50000 (38%)]	Loss: 0.000046, KL fake Loss: 0.000016
Classification Train Epoch: 86 [25600/50000 (51%)]	Loss: 0.000206, KL fake Loss: 0.000000
Classification Train Epoch: 86 [32000/50000 (64%)]	Loss: 0.000027, KL fake Loss: 0.000000
Classification Train Epoch: 86 [38400/50000 (77%)]	Loss: 0.000210, KL fake Loss: 0.000000
Classification Train Epoch: 86 [44800/50000 (90%)]	Loss: 0.000024, KL fake Loss: 0.000000

Test set: Average loss: 5.0720, Accuracy: 5701/10000 (57%)

Classification Train Epoch: 87 [0/50000 (0%)]	Loss: 0.000124, KL fake Loss: 0.000242
Classification Train Epoch: 87 [6400/50000 (13%)]	Loss: 0.000656, KL fake Loss: 0.000001
Classification Train Epoch: 87 [12800/50000 (26%)]	Loss: 0.000104, KL fake Loss: 0.000001
Classification Train Epoch: 87 [19200/50000 (38%)]	Loss: 0.002418, KL fake Loss: 0.000003
Classification Train Epoch: 87 [25600/50000 (51%)]	Loss: 0.000050, KL fake Loss: 0.000036
Classification Train Epoch: 87 [32000/50000 (64%)]	Loss: 0.000201, KL fake Loss: 0.000000
Classification Train Epoch: 87 [38400/50000 (77%)]	Loss: 0.000026, KL fake Loss: 0.000001
Classification Train Epoch: 87 [44800/50000 (90%)]	Loss: 0.000542, KL fake Loss: 0.000000

Test set: Average loss: 3.7047, Accuracy: 6581/10000 (66%)

Classification Train Epoch: 88 [0/50000 (0%)]	Loss: 0.000018, KL fake Loss: 0.000001
Classification Train Epoch: 88 [6400/50000 (13%)]	Loss: 0.000044, KL fake Loss: 0.000000
Classification Train Epoch: 88 [12800/50000 (26%)]	Loss: 0.000055, KL fake Loss: 0.000026
Classification Train Epoch: 88 [19200/50000 (38%)]	Loss: 0.000084, KL fake Loss: 0.000073
Classification Train Epoch: 88 [25600/50000 (51%)]	Loss: 0.000062, KL fake Loss: 0.000125
Classification Train Epoch: 88 [32000/50000 (64%)]	Loss: 0.000249, KL fake Loss: 0.000001
Classification Train Epoch: 88 [38400/50000 (77%)]	Loss: 0.000115, KL fake Loss: 0.000000
Classification Train Epoch: 88 [44800/50000 (90%)]	Loss: 0.000035, KL fake Loss: 0.000002

Test set: Average loss: 3.2552, Accuracy: 6778/10000 (68%)

Classification Train Epoch: 89 [0/50000 (0%)]	Loss: 0.000041, KL fake Loss: 0.000060
Classification Train Epoch: 89 [6400/50000 (13%)]	Loss: 0.000067, KL fake Loss: 0.000005
Classification Train Epoch: 89 [12800/50000 (26%)]	Loss: 0.000033, KL fake Loss: 0.000000
Classification Train Epoch: 89 [19200/50000 (38%)]	Loss: 0.000059, KL fake Loss: 0.000001
Classification Train Epoch: 89 [25600/50000 (51%)]	Loss: 0.000126, KL fake Loss: 0.000000
Classification Train Epoch: 89 [32000/50000 (64%)]	Loss: 0.000073, KL fake Loss: 0.000000
Classification Train Epoch: 89 [38400/50000 (77%)]	Loss: 0.000551, KL fake Loss: 0.000002
Classification Train Epoch: 89 [44800/50000 (90%)]	Loss: 0.000053, KL fake Loss: 0.000000

Test set: Average loss: 2.9816, Accuracy: 7019/10000 (70%)

Classification Train Epoch: 90 [0/50000 (0%)]	Loss: 0.000109, KL fake Loss: 0.000515
Classification Train Epoch: 90 [6400/50000 (13%)]	Loss: 0.001032, KL fake Loss: 0.000099
Classification Train Epoch: 90 [12800/50000 (26%)]	Loss: 0.000113, KL fake Loss: 0.000437
Classification Train Epoch: 90 [19200/50000 (38%)]	Loss: 0.000558, KL fake Loss: 0.018224
Classification Train Epoch: 90 [25600/50000 (51%)]	Loss: 0.000010, KL fake Loss: 0.000130
Classification Train Epoch: 90 [32000/50000 (64%)]	Loss: 0.000102, KL fake Loss: 0.000196
Classification Train Epoch: 90 [38400/50000 (77%)]	Loss: 0.000055, KL fake Loss: 0.000637
Classification Train Epoch: 90 [44800/50000 (90%)]	Loss: 0.000088, KL fake Loss: 0.000001

Test set: Average loss: 2.6413, Accuracy: 7255/10000 (73%)

Classification Train Epoch: 91 [0/50000 (0%)]	Loss: 0.000388, KL fake Loss: 0.000001
Classification Train Epoch: 91 [6400/50000 (13%)]	Loss: 0.000043, KL fake Loss: 0.000000
Classification Train Epoch: 91 [12800/50000 (26%)]	Loss: 0.000125, KL fake Loss: -0.000000
Classification Train Epoch: 91 [19200/50000 (38%)]	Loss: 0.000009, KL fake Loss: 0.000001
Classification Train Epoch: 91 [25600/50000 (51%)]	Loss: 0.000114, KL fake Loss: 0.000000
Classification Train Epoch: 91 [32000/50000 (64%)]	Loss: 0.000030, KL fake Loss: 0.000001
Classification Train Epoch: 91 [38400/50000 (77%)]	Loss: 0.000013, KL fake Loss: 0.000001
Classification Train Epoch: 91 [44800/50000 (90%)]	Loss: 0.000347, KL fake Loss: 0.000000

Test set: Average loss: 2.8485, Accuracy: 7240/10000 (72%)

Classification Train Epoch: 92 [0/50000 (0%)]	Loss: 0.000320, KL fake Loss: 0.000000
Classification Train Epoch: 92 [6400/50000 (13%)]	Loss: 0.000020, KL fake Loss: 0.000001
Classification Train Epoch: 92 [12800/50000 (26%)]	Loss: 0.000013, KL fake Loss: 0.000000
Classification Train Epoch: 92 [19200/50000 (38%)]	Loss: 0.000009, KL fake Loss: 0.000000
Classification Train Epoch: 92 [25600/50000 (51%)]	Loss: 0.000038, KL fake Loss: 0.000319
Classification Train Epoch: 92 [32000/50000 (64%)]	Loss: 0.000059, KL fake Loss: 0.000012
Classification Train Epoch: 92 [38400/50000 (77%)]	Loss: 0.000013, KL fake Loss: 0.000000
Classification Train Epoch: 92 [44800/50000 (90%)]	Loss: 0.000223, KL fake Loss: 0.000000

Test set: Average loss: 3.5705, Accuracy: 6749/10000 (67%)

Classification Train Epoch: 93 [0/50000 (0%)]	Loss: 0.000009, KL fake Loss: 0.000001
Classification Train Epoch: 93 [6400/50000 (13%)]	Loss: 0.000231, KL fake Loss: 0.000000
Classification Train Epoch: 93 [12800/50000 (26%)]	Loss: 0.000234, KL fake Loss: 0.000000
Classification Train Epoch: 93 [19200/50000 (38%)]	Loss: 0.000127, KL fake Loss: 0.000000
Classification Train Epoch: 93 [25600/50000 (51%)]	Loss: 0.000021, KL fake Loss: -0.000000
Classification Train Epoch: 93 [32000/50000 (64%)]	Loss: 0.000016, KL fake Loss: 0.009705
Classification Train Epoch: 93 [38400/50000 (77%)]	Loss: 0.000015, KL fake Loss: 0.013463
Classification Train Epoch: 93 [44800/50000 (90%)]	Loss: 0.000024, KL fake Loss: 0.000000

Test set: Average loss: 4.4642, Accuracy: 6575/10000 (66%)

Classification Train Epoch: 94 [0/50000 (0%)]	Loss: 0.000020, KL fake Loss: 0.000018
Classification Train Epoch: 94 [6400/50000 (13%)]	Loss: 0.000019, KL fake Loss: 0.000034
Classification Train Epoch: 94 [12800/50000 (26%)]	Loss: 0.000078, KL fake Loss: 0.005085
Classification Train Epoch: 94 [19200/50000 (38%)]	Loss: 0.000044, KL fake Loss: 0.000011
Classification Train Epoch: 94 [25600/50000 (51%)]	Loss: 0.000154, KL fake Loss: 0.000000
Classification Train Epoch: 94 [32000/50000 (64%)]	Loss: 0.000061, KL fake Loss: 0.000000
Classification Train Epoch: 94 [38400/50000 (77%)]	Loss: 0.000033, KL fake Loss: 0.000000
Classification Train Epoch: 94 [44800/50000 (90%)]	Loss: 0.000156, KL fake Loss: 0.000012

Test set: Average loss: 3.5422, Accuracy: 6802/10000 (68%)

Classification Train Epoch: 95 [0/50000 (0%)]	Loss: 0.000051, KL fake Loss: 0.000027
Classification Train Epoch: 95 [6400/50000 (13%)]	Loss: 0.000055, KL fake Loss: 0.000001
Classification Train Epoch: 95 [12800/50000 (26%)]	Loss: 0.000076, KL fake Loss: 0.000000
Classification Train Epoch: 95 [19200/50000 (38%)]	Loss: 0.000018, KL fake Loss: 0.000156
Classification Train Epoch: 95 [25600/50000 (51%)]	Loss: 0.000026, KL fake Loss: 0.000003
 95%|█████████▌| 95/100 [5:37:45<17:46, 213.32s/it] 96%|█████████▌| 96/100 [5:41:19<14:13, 213.32s/it] 97%|█████████▋| 97/100 [5:44:52<10:39, 213.32s/it] 98%|█████████▊| 98/100 [5:48:25<07:06, 213.32s/it] 99%|█████████▉| 99/100 [5:51:59<03:33, 213.31s/it]100%|██████████| 100/100 [5:55:32<00:00, 213.34s/it]100%|██████████| 100/100 [5:55:32<00:00, 213.33s/it]
Classification Train Epoch: 95 [32000/50000 (64%)]	Loss: 0.000028, KL fake Loss: 0.000000
Classification Train Epoch: 95 [38400/50000 (77%)]	Loss: 0.000058, KL fake Loss: 0.000001
Classification Train Epoch: 95 [44800/50000 (90%)]	Loss: 0.000052, KL fake Loss: 0.000000

Test set: Average loss: 5.0625, Accuracy: 5964/10000 (60%)

Classification Train Epoch: 96 [0/50000 (0%)]	Loss: 0.002963, KL fake Loss: 0.000001
Classification Train Epoch: 96 [6400/50000 (13%)]	Loss: 0.000018, KL fake Loss: 0.000175
Classification Train Epoch: 96 [12800/50000 (26%)]	Loss: 0.000011, KL fake Loss: 0.006238
Classification Train Epoch: 96 [19200/50000 (38%)]	Loss: 0.000162, KL fake Loss: 0.000035
Classification Train Epoch: 96 [25600/50000 (51%)]	Loss: 0.000303, KL fake Loss: 0.002134
Classification Train Epoch: 96 [32000/50000 (64%)]	Loss: 0.000043, KL fake Loss: 0.000001
Classification Train Epoch: 96 [38400/50000 (77%)]	Loss: 0.000012, KL fake Loss: 0.000004
Classification Train Epoch: 96 [44800/50000 (90%)]	Loss: 0.000086, KL fake Loss: 0.000003

Test set: Average loss: 6.8618, Accuracy: 4790/10000 (48%)

Classification Train Epoch: 97 [0/50000 (0%)]	Loss: 0.000093, KL fake Loss: 0.000047
Classification Train Epoch: 97 [6400/50000 (13%)]	Loss: 0.000249, KL fake Loss: 0.000003
Classification Train Epoch: 97 [12800/50000 (26%)]	Loss: 0.000249, KL fake Loss: 0.000000
Classification Train Epoch: 97 [19200/50000 (38%)]	Loss: 0.000004, KL fake Loss: 0.000000
Classification Train Epoch: 97 [25600/50000 (51%)]	Loss: 0.000019, KL fake Loss: 0.000031
Classification Train Epoch: 97 [32000/50000 (64%)]	Loss: 0.000016, KL fake Loss: 0.000000
Classification Train Epoch: 97 [38400/50000 (77%)]	Loss: 0.000094, KL fake Loss: 0.000001
Classification Train Epoch: 97 [44800/50000 (90%)]	Loss: 0.000008, KL fake Loss: 0.000000

Test set: Average loss: 5.4033, Accuracy: 5430/10000 (54%)

Classification Train Epoch: 98 [0/50000 (0%)]	Loss: 0.000008, KL fake Loss: 0.000005
Classification Train Epoch: 98 [6400/50000 (13%)]	Loss: 0.000671, KL fake Loss: 0.000000
Classification Train Epoch: 98 [12800/50000 (26%)]	Loss: 0.000021, KL fake Loss: 0.000000
Classification Train Epoch: 98 [19200/50000 (38%)]	Loss: 0.000209, KL fake Loss: 0.000000
Classification Train Epoch: 98 [25600/50000 (51%)]	Loss: 0.000047, KL fake Loss: 0.000001
Classification Train Epoch: 98 [32000/50000 (64%)]	Loss: 0.001293, KL fake Loss: 0.000000
Classification Train Epoch: 98 [38400/50000 (77%)]	Loss: 0.000068, KL fake Loss: 0.000001
Classification Train Epoch: 98 [44800/50000 (90%)]	Loss: 0.000133, KL fake Loss: 0.000013

Test set: Average loss: 5.1246, Accuracy: 5907/10000 (59%)

Classification Train Epoch: 99 [0/50000 (0%)]	Loss: 0.000137, KL fake Loss: 0.000159
Classification Train Epoch: 99 [6400/50000 (13%)]	Loss: 0.000033, KL fake Loss: 0.000003
Classification Train Epoch: 99 [12800/50000 (26%)]	Loss: 0.000030, KL fake Loss: 0.000000
Classification Train Epoch: 99 [19200/50000 (38%)]	Loss: 0.001557, KL fake Loss: 0.000001
Classification Train Epoch: 99 [25600/50000 (51%)]	Loss: 0.000057, KL fake Loss: 0.000000
Classification Train Epoch: 99 [32000/50000 (64%)]	Loss: 0.000012, KL fake Loss: 0.000000
Classification Train Epoch: 99 [38400/50000 (77%)]	Loss: 0.000073, KL fake Loss: 0.000000
Classification Train Epoch: 99 [44800/50000 (90%)]	Loss: 0.000056, KL fake Loss: 0.000000

Test set: Average loss: 4.4037, Accuracy: 5879/10000 (59%)

Classification Train Epoch: 100 [0/50000 (0%)]	Loss: 0.000026, KL fake Loss: 0.000333
Classification Train Epoch: 100 [6400/50000 (13%)]	Loss: 0.000482, KL fake Loss: 0.000000
Classification Train Epoch: 100 [12800/50000 (26%)]	Loss: 0.000021, KL fake Loss: 0.000003
Classification Train Epoch: 100 [19200/50000 (38%)]	Loss: 0.001091, KL fake Loss: 0.000001
Classification Train Epoch: 100 [25600/50000 (51%)]	Loss: 0.000100, KL fake Loss: 0.000001
Classification Train Epoch: 100 [32000/50000 (64%)]	Loss: 0.000129, KL fake Loss: 0.000001
Classification Train Epoch: 100 [38400/50000 (77%)]	Loss: 0.000025, KL fake Loss: 0.000002
Classification Train Epoch: 100 [44800/50000 (90%)]	Loss: 0.000060, KL fake Loss: 0.000041

Test set: Average loss: 2.7967, Accuracy: 7001/10000 (70%)

Namespace(batch_size=128, no_cuda=False, seed=1, dataset='CIFAR10-SVHN', dataroot='./data', imageSize=32, outf='./results/joint_confidence_loss/CS-0.0001/', out_dataset='CIFAR10-SVHN', num_classes=10, num_channels=3, pre_trained_net='results/joint_confidence_loss/CS-0.0001/model_epoch_100.pth')
Random Seed:  1
Load model
DenseNet3(
  (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock(
    (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock(
    (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(342, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (fc): Linear(in_features=342, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)ic| len(dset): 73257
ic| len(dset): 73257

load target data:  CIFAR10-SVHN
Files already downloaded and verified
Files already downloaded and verified
Using downloaded and verified file: ./Datasets/SVHN/train_32x32.mat
Using downloaded and verified file: ./Datasets/SVHN/test_32x32.mat
load non target data:  CIFAR10-SVHN
Files already downloaded and verified
Files already downloaded and verified
Using downloaded and verified file: ./Datasets/SVHN/train_32x32.mat
Using downloaded and verified file: ./Datasets/SVHN/test_32x32.mat
generate log from in-distribution data

 Final Accuracy: 7001/10000 (70.01%)

generate log  from out-of-distribution data
calculate metrics
  Performance of Baseline detector
TNR at TPR 95%:             6.342%
TNR at TPR 99%:             0.762%
AUROC:                     61.585%
Detection acc:             63.652%
AUPR In:                   65.385%
AUPR Out:                  58.770%

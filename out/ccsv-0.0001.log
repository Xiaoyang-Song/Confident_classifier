ic| len(dset): 73257
ic| len(dset): 26032
Namespace(batch_size=64, epochs=100, lr=0.001, no_cuda=False, seed=1, log_interval=100, dataset='SVHN', dataroot='./data', imageSize=32, outf='./results/joint_confidence_loss/SV-0.0001/', wd=0.0, droprate=0.1, decreasing_lr='60', num_classes=8, beta=0.0001, num_channels=3)
Random Seed:  1
load InD data for Experiment:  SVHN
Using downloaded and verified file: ./Datasets/SVHN/train_32x32.mat
Using downloaded and verified file: ./Datasets/SVHN/test_32x32.mat
Load model
DenseNet3(
  (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock(
    (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock(
    (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(342, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (fc): Linear(in_features=342, out_features=8, bias=True)
  (softmax): Softmax(dim=1)
)
load GAN
Setup optimizer
0.001
  0%|          | 0/100 [00:00<?, ?it/s]/home/xysong/.conda/envs/OoD/lib/python3.9/site-packages/torch/nn/functional.py:2904: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
  1%|          | 1/100 [04:24<7:16:26, 264.51s/it]  2%|▏         | 2/100 [08:48<7:11:45, 264.35s/it]  3%|▎         | 3/100 [13:12<7:07:16, 264.30s/it]  4%|▍         | 4/100 [17:37<7:02:50, 264.27s/it]  5%|▌         | 5/100 [22:01<6:58:24, 264.25s/it]  6%|▌         | 6/100 [26:25<6:53:58, 264.24s/it]  7%|▋         | 7/100 [30:49<6:49:33, 264.23s/it]  8%|▊         | 8/100 [35:14<6:45:09, 264.23s/it]Classification Train Epoch: 1 [0/63553 (0%)]	Loss: 2.080009, KL fake Loss: 0.030706
Classification Train Epoch: 1 [6400/63553 (10%)]	Loss: 1.784686, KL fake Loss: 0.048137
Classification Train Epoch: 1 [12800/63553 (20%)]	Loss: 1.377545, KL fake Loss: 0.104086
Classification Train Epoch: 1 [19200/63553 (30%)]	Loss: 0.589428, KL fake Loss: 0.710074
Classification Train Epoch: 1 [25600/63553 (40%)]	Loss: 0.367474, KL fake Loss: 0.698650
Classification Train Epoch: 1 [32000/63553 (50%)]	Loss: 0.437029, KL fake Loss: 0.611888
Classification Train Epoch: 1 [38400/63553 (60%)]	Loss: 0.274371, KL fake Loss: 0.566343
Classification Train Epoch: 1 [44800/63553 (70%)]	Loss: 0.260618, KL fake Loss: 0.658687
Classification Train Epoch: 1 [51200/63553 (80%)]	Loss: 0.242975, KL fake Loss: 0.446433
Classification Train Epoch: 1 [57600/63553 (91%)]	Loss: 0.370106, KL fake Loss: 0.511987

Test set: Average loss: 1.3725, Accuracy: 16884/22777 (74%)

Classification Train Epoch: 2 [0/63553 (0%)]	Loss: 0.158307, KL fake Loss: 0.432882
Classification Train Epoch: 2 [6400/63553 (10%)]	Loss: 0.425002, KL fake Loss: 0.404669
Classification Train Epoch: 2 [12800/63553 (20%)]	Loss: 0.212721, KL fake Loss: 0.373765
Classification Train Epoch: 2 [19200/63553 (30%)]	Loss: 0.285063, KL fake Loss: 0.338490
Classification Train Epoch: 2 [25600/63553 (40%)]	Loss: 0.265939, KL fake Loss: 0.404355
Classification Train Epoch: 2 [32000/63553 (50%)]	Loss: 0.427619, KL fake Loss: 0.297867
Classification Train Epoch: 2 [38400/63553 (60%)]	Loss: 0.198324, KL fake Loss: 0.309998
Classification Train Epoch: 2 [44800/63553 (70%)]	Loss: 0.173289, KL fake Loss: 0.376566
Classification Train Epoch: 2 [51200/63553 (80%)]	Loss: 0.303234, KL fake Loss: 0.320994
Classification Train Epoch: 2 [57600/63553 (91%)]	Loss: 0.108091, KL fake Loss: 0.297849

Test set: Average loss: 1.0033, Accuracy: 17797/22777 (78%)

Classification Train Epoch: 3 [0/63553 (0%)]	Loss: 0.165336, KL fake Loss: 0.323690
Classification Train Epoch: 3 [6400/63553 (10%)]	Loss: 0.119355, KL fake Loss: 0.306704
Classification Train Epoch: 3 [12800/63553 (20%)]	Loss: 0.098355, KL fake Loss: 0.277029
Classification Train Epoch: 3 [19200/63553 (30%)]	Loss: 0.212747, KL fake Loss: 0.314195
Classification Train Epoch: 3 [25600/63553 (40%)]	Loss: 0.163184, KL fake Loss: 0.318248
Classification Train Epoch: 3 [32000/63553 (50%)]	Loss: 0.149390, KL fake Loss: 0.280981
Classification Train Epoch: 3 [38400/63553 (60%)]	Loss: 0.386315, KL fake Loss: 0.242313
Classification Train Epoch: 3 [44800/63553 (70%)]	Loss: 0.298339, KL fake Loss: 0.384796
Classification Train Epoch: 3 [51200/63553 (80%)]	Loss: 0.298360, KL fake Loss: 0.276885
Classification Train Epoch: 3 [57600/63553 (91%)]	Loss: 0.119802, KL fake Loss: 0.242219

Test set: Average loss: 0.8133, Accuracy: 18844/22777 (83%)

Classification Train Epoch: 4 [0/63553 (0%)]	Loss: 0.431555, KL fake Loss: 0.308347
Classification Train Epoch: 4 [6400/63553 (10%)]	Loss: 0.225414, KL fake Loss: 0.352775
Classification Train Epoch: 4 [12800/63553 (20%)]	Loss: 0.158920, KL fake Loss: 0.231934
Classification Train Epoch: 4 [19200/63553 (30%)]	Loss: 0.064408, KL fake Loss: 0.190423
Classification Train Epoch: 4 [25600/63553 (40%)]	Loss: 0.318730, KL fake Loss: 0.303899
Classification Train Epoch: 4 [32000/63553 (50%)]	Loss: 0.199201, KL fake Loss: 0.325436
Classification Train Epoch: 4 [38400/63553 (60%)]	Loss: 0.135787, KL fake Loss: 0.249809
Classification Train Epoch: 4 [44800/63553 (70%)]	Loss: 0.190528, KL fake Loss: 0.191727
Classification Train Epoch: 4 [51200/63553 (80%)]	Loss: 0.344594, KL fake Loss: 0.368896
Classification Train Epoch: 4 [57600/63553 (91%)]	Loss: 0.109894, KL fake Loss: 0.198876

Test set: Average loss: 0.8019, Accuracy: 19436/22777 (85%)

Classification Train Epoch: 5 [0/63553 (0%)]	Loss: 0.078905, KL fake Loss: 0.385794
Classification Train Epoch: 5 [6400/63553 (10%)]	Loss: 0.144333, KL fake Loss: 0.200205
Classification Train Epoch: 5 [12800/63553 (20%)]	Loss: 0.131852, KL fake Loss: 0.198762
Classification Train Epoch: 5 [19200/63553 (30%)]	Loss: 0.116631, KL fake Loss: 0.189195
Classification Train Epoch: 5 [25600/63553 (40%)]	Loss: 0.253588, KL fake Loss: 0.239836
Classification Train Epoch: 5 [32000/63553 (50%)]	Loss: 0.144888, KL fake Loss: 0.137674
Classification Train Epoch: 5 [38400/63553 (60%)]	Loss: 0.186047, KL fake Loss: 0.161675
Classification Train Epoch: 5 [44800/63553 (70%)]	Loss: 0.045909, KL fake Loss: 0.123839
Classification Train Epoch: 5 [51200/63553 (80%)]	Loss: 0.213977, KL fake Loss: 0.267833
Classification Train Epoch: 5 [57600/63553 (91%)]	Loss: 0.181859, KL fake Loss: 0.092131

Test set: Average loss: 0.9141, Accuracy: 19199/22777 (84%)

Classification Train Epoch: 6 [0/63553 (0%)]	Loss: 0.054859, KL fake Loss: 0.115944
Classification Train Epoch: 6 [6400/63553 (10%)]	Loss: 0.116424, KL fake Loss: 0.121378
Classification Train Epoch: 6 [12800/63553 (20%)]	Loss: 0.156981, KL fake Loss: 0.099678
Classification Train Epoch: 6 [19200/63553 (30%)]	Loss: 0.021692, KL fake Loss: 0.131314
Classification Train Epoch: 6 [25600/63553 (40%)]	Loss: 0.062246, KL fake Loss: 0.172102
Classification Train Epoch: 6 [32000/63553 (50%)]	Loss: 0.151917, KL fake Loss: 0.092887
Classification Train Epoch: 6 [38400/63553 (60%)]	Loss: 0.155224, KL fake Loss: 0.144034
Classification Train Epoch: 6 [44800/63553 (70%)]	Loss: 0.067704, KL fake Loss: 0.102399
Classification Train Epoch: 6 [51200/63553 (80%)]	Loss: 0.037124, KL fake Loss: 0.099651
Classification Train Epoch: 6 [57600/63553 (91%)]	Loss: 0.168542, KL fake Loss: 0.147352

Test set: Average loss: 0.4201, Accuracy: 20686/22777 (91%)

Classification Train Epoch: 7 [0/63553 (0%)]	Loss: 0.170705, KL fake Loss: 0.159920
Classification Train Epoch: 7 [6400/63553 (10%)]	Loss: 0.162343, KL fake Loss: 0.132039
Classification Train Epoch: 7 [12800/63553 (20%)]	Loss: 0.005838, KL fake Loss: 0.129309
Classification Train Epoch: 7 [19200/63553 (30%)]	Loss: 0.151063, KL fake Loss: 0.104464
Classification Train Epoch: 7 [25600/63553 (40%)]	Loss: 0.087919, KL fake Loss: 0.093456
Classification Train Epoch: 7 [32000/63553 (50%)]	Loss: 0.041842, KL fake Loss: 0.060062
Classification Train Epoch: 7 [38400/63553 (60%)]	Loss: 0.013132, KL fake Loss: 0.085944
Classification Train Epoch: 7 [44800/63553 (70%)]	Loss: 0.048115, KL fake Loss: 0.064379
Classification Train Epoch: 7 [51200/63553 (80%)]	Loss: 0.052098, KL fake Loss: 0.098406
Classification Train Epoch: 7 [57600/63553 (91%)]	Loss: 0.088090, KL fake Loss: 0.032592

Test set: Average loss: 0.3323, Accuracy: 21066/22777 (92%)

Classification Train Epoch: 8 [0/63553 (0%)]	Loss: 0.166724, KL fake Loss: 0.140641
Classification Train Epoch: 8 [6400/63553 (10%)]	Loss: 0.029279, KL fake Loss: 0.068171
Classification Train Epoch: 8 [12800/63553 (20%)]	Loss: 0.138117, KL fake Loss: 0.048074
Classification Train Epoch: 8 [19200/63553 (30%)]	Loss: 0.087925, KL fake Loss: 0.051982
Classification Train Epoch: 8 [25600/63553 (40%)]	Loss: 0.129956, KL fake Loss: 0.054350
Classification Train Epoch: 8 [32000/63553 (50%)]	Loss: 0.119036, KL fake Loss: 0.079647
Classification Train Epoch: 8 [38400/63553 (60%)]	Loss: 0.056249, KL fake Loss: 0.038988
Classification Train Epoch: 8 [44800/63553 (70%)]	Loss: 0.439893, KL fake Loss: 0.044133
Classification Train Epoch: 8 [51200/63553 (80%)]	Loss: 0.091951, KL fake Loss: 0.029007
Classification Train Epoch: 8 [57600/63553 (91%)]	Loss: 0.051555, KL fake Loss: 0.033546

Test set: Average loss: 0.5577, Accuracy: 20185/22777 (89%)

Classification Train Epoch: 9 [0/63553 (0%)]	Loss: 0.250901, KL fake Loss: 0.095862
Classification Train Epoch: 9 [6400/63553 (10%)]	Loss: 0.026749, KL fake Loss: 0.029632
Classification Train Epoch: 9 [12800/63553 (20%)]	Loss: 0.128583, KL fake Loss: 0.025981
Classification Train Epoch: 9 [19200/63553 (30%)]	Loss: 0.108730, KL fake Loss: 0.028337
Classification Train Epoch: 9 [25600/63553 (40%)]	Loss: 0.077900, KL fake Loss: 0.023893
Classification Train Epoch: 9 [32000/63553 (50%)]	Loss: 0.127467, KL fake Loss: 0.025611
Classification Train Epoch: 9 [38400/63553 (60%)]	Loss: 0.216612, KL fake Loss: 0.020910
  9%|▉         | 9/100 [39:38<6:40:45, 264.23s/it] 10%|█         | 10/100 [44:02<6:36:21, 264.23s/it] 11%|█         | 11/100 [48:26<6:31:56, 264.23s/it] 12%|█▏        | 12/100 [52:50<6:27:31, 264.22s/it] 13%|█▎        | 13/100 [57:15<6:23:07, 264.22s/it] 14%|█▍        | 14/100 [1:01:39<6:18:43, 264.22s/it] 15%|█▌        | 15/100 [1:06:03<6:14:18, 264.22s/it] 16%|█▌        | 16/100 [1:10:27<6:09:54, 264.22s/it] 17%|█▋        | 17/100 [1:14:52<6:05:30, 264.23s/it]Classification Train Epoch: 9 [44800/63553 (70%)]	Loss: 0.143593, KL fake Loss: 0.043855
Classification Train Epoch: 9 [51200/63553 (80%)]	Loss: 0.067391, KL fake Loss: 0.035991
Classification Train Epoch: 9 [57600/63553 (91%)]	Loss: 0.024783, KL fake Loss: 0.035672

Test set: Average loss: 0.4432, Accuracy: 20500/22777 (90%)

Classification Train Epoch: 10 [0/63553 (0%)]	Loss: 0.060930, KL fake Loss: 0.025894
Classification Train Epoch: 10 [6400/63553 (10%)]	Loss: 0.055429, KL fake Loss: 0.022060
Classification Train Epoch: 10 [12800/63553 (20%)]	Loss: 0.089955, KL fake Loss: 0.034682
Classification Train Epoch: 10 [19200/63553 (30%)]	Loss: 0.017158, KL fake Loss: 0.041121
Classification Train Epoch: 10 [25600/63553 (40%)]	Loss: 0.064514, KL fake Loss: 0.032906
Classification Train Epoch: 10 [32000/63553 (50%)]	Loss: 0.062415, KL fake Loss: 0.019119
Classification Train Epoch: 10 [38400/63553 (60%)]	Loss: 0.111477, KL fake Loss: 0.016436
Classification Train Epoch: 10 [44800/63553 (70%)]	Loss: 0.015269, KL fake Loss: 0.012978
Classification Train Epoch: 10 [51200/63553 (80%)]	Loss: 0.073255, KL fake Loss: 0.026046
Classification Train Epoch: 10 [57600/63553 (91%)]	Loss: 0.116552, KL fake Loss: 0.024093

Test set: Average loss: 0.5074, Accuracy: 20276/22777 (89%)

Classification Train Epoch: 11 [0/63553 (0%)]	Loss: 0.063766, KL fake Loss: 0.051788
Classification Train Epoch: 11 [6400/63553 (10%)]	Loss: 0.021932, KL fake Loss: 0.019122
Classification Train Epoch: 11 [12800/63553 (20%)]	Loss: 0.020518, KL fake Loss: 0.014675
Classification Train Epoch: 11 [19200/63553 (30%)]	Loss: 0.064946, KL fake Loss: 0.014486
Classification Train Epoch: 11 [25600/63553 (40%)]	Loss: 0.157239, KL fake Loss: 0.009408
Classification Train Epoch: 11 [32000/63553 (50%)]	Loss: 0.091409, KL fake Loss: 0.014572
Classification Train Epoch: 11 [38400/63553 (60%)]	Loss: 0.179213, KL fake Loss: 0.013254
Classification Train Epoch: 11 [44800/63553 (70%)]	Loss: 0.085069, KL fake Loss: 0.011213
Classification Train Epoch: 11 [51200/63553 (80%)]	Loss: 0.047057, KL fake Loss: 0.017424
Classification Train Epoch: 11 [57600/63553 (91%)]	Loss: 0.183512, KL fake Loss: 0.015966

Test set: Average loss: 0.4226, Accuracy: 20850/22777 (92%)

Classification Train Epoch: 12 [0/63553 (0%)]	Loss: 0.131640, KL fake Loss: 0.016506
Classification Train Epoch: 12 [6400/63553 (10%)]	Loss: 0.117611, KL fake Loss: 0.006107
Classification Train Epoch: 12 [12800/63553 (20%)]	Loss: 0.054240, KL fake Loss: 0.007940
Classification Train Epoch: 12 [19200/63553 (30%)]	Loss: 0.034230, KL fake Loss: 0.012414
Classification Train Epoch: 12 [25600/63553 (40%)]	Loss: 0.014034, KL fake Loss: 0.005601
Classification Train Epoch: 12 [32000/63553 (50%)]	Loss: 0.057127, KL fake Loss: 0.007566
Classification Train Epoch: 12 [38400/63553 (60%)]	Loss: 0.169420, KL fake Loss: 0.016455
Classification Train Epoch: 12 [44800/63553 (70%)]	Loss: 0.035376, KL fake Loss: 0.008581
Classification Train Epoch: 12 [51200/63553 (80%)]	Loss: 0.060076, KL fake Loss: 0.008018
Classification Train Epoch: 12 [57600/63553 (91%)]	Loss: 0.129271, KL fake Loss: 0.008064

Test set: Average loss: 0.5650, Accuracy: 20188/22777 (89%)

Classification Train Epoch: 13 [0/63553 (0%)]	Loss: 0.004021, KL fake Loss: 0.007510
Classification Train Epoch: 13 [6400/63553 (10%)]	Loss: 0.029228, KL fake Loss: 0.004603
Classification Train Epoch: 13 [12800/63553 (20%)]	Loss: 0.012888, KL fake Loss: 0.004637
Classification Train Epoch: 13 [19200/63553 (30%)]	Loss: 0.131993, KL fake Loss: 0.008055
Classification Train Epoch: 13 [25600/63553 (40%)]	Loss: 0.127815, KL fake Loss: 0.006597
Classification Train Epoch: 13 [32000/63553 (50%)]	Loss: 0.021750, KL fake Loss: 0.012196
Classification Train Epoch: 13 [38400/63553 (60%)]	Loss: 0.015316, KL fake Loss: 0.005287
Classification Train Epoch: 13 [44800/63553 (70%)]	Loss: 0.143802, KL fake Loss: 0.009761
Classification Train Epoch: 13 [51200/63553 (80%)]	Loss: 0.103494, KL fake Loss: 0.008171
Classification Train Epoch: 13 [57600/63553 (91%)]	Loss: 0.106277, KL fake Loss: 0.011001

Test set: Average loss: 0.5116, Accuracy: 20131/22777 (88%)

Classification Train Epoch: 14 [0/63553 (0%)]	Loss: 0.041186, KL fake Loss: 0.008030
Classification Train Epoch: 14 [6400/63553 (10%)]	Loss: 0.053289, KL fake Loss: 0.004350
Classification Train Epoch: 14 [12800/63553 (20%)]	Loss: 0.010400, KL fake Loss: 0.005003
Classification Train Epoch: 14 [19200/63553 (30%)]	Loss: 0.018159, KL fake Loss: 0.005709
Classification Train Epoch: 14 [25600/63553 (40%)]	Loss: 0.027191, KL fake Loss: 0.005746
Classification Train Epoch: 14 [32000/63553 (50%)]	Loss: 0.015829, KL fake Loss: 0.005875
Classification Train Epoch: 14 [38400/63553 (60%)]	Loss: 0.060339, KL fake Loss: 0.002821
Classification Train Epoch: 14 [44800/63553 (70%)]	Loss: 0.022557, KL fake Loss: 0.002209
Classification Train Epoch: 14 [51200/63553 (80%)]	Loss: 0.082959, KL fake Loss: 0.004884
Classification Train Epoch: 14 [57600/63553 (91%)]	Loss: 0.151319, KL fake Loss: 0.008783

Test set: Average loss: 0.3999, Accuracy: 20977/22777 (92%)

Classification Train Epoch: 15 [0/63553 (0%)]	Loss: 0.058746, KL fake Loss: 0.008617
Classification Train Epoch: 15 [6400/63553 (10%)]	Loss: 0.048871, KL fake Loss: 0.003633
Classification Train Epoch: 15 [12800/63553 (20%)]	Loss: 0.044004, KL fake Loss: 0.002586
Classification Train Epoch: 15 [19200/63553 (30%)]	Loss: 0.004889, KL fake Loss: 0.003928
Classification Train Epoch: 15 [25600/63553 (40%)]	Loss: 0.013536, KL fake Loss: 0.002066
Classification Train Epoch: 15 [32000/63553 (50%)]	Loss: 0.025957, KL fake Loss: 0.005041
Classification Train Epoch: 15 [38400/63553 (60%)]	Loss: 0.111526, KL fake Loss: 0.004805
Classification Train Epoch: 15 [44800/63553 (70%)]	Loss: 0.158969, KL fake Loss: 0.003152
Classification Train Epoch: 15 [51200/63553 (80%)]	Loss: 0.051651, KL fake Loss: 0.004159
Classification Train Epoch: 15 [57600/63553 (91%)]	Loss: 0.003260, KL fake Loss: 0.012314

Test set: Average loss: 0.5614, Accuracy: 20064/22777 (88%)

Classification Train Epoch: 16 [0/63553 (0%)]	Loss: 0.026713, KL fake Loss: 0.048852
Classification Train Epoch: 16 [6400/63553 (10%)]	Loss: 0.079412, KL fake Loss: 0.003812
Classification Train Epoch: 16 [12800/63553 (20%)]	Loss: 0.023874, KL fake Loss: 0.001948
Classification Train Epoch: 16 [19200/63553 (30%)]	Loss: 0.021231, KL fake Loss: 0.010041
Classification Train Epoch: 16 [25600/63553 (40%)]	Loss: 0.132352, KL fake Loss: 0.001594
Classification Train Epoch: 16 [32000/63553 (50%)]	Loss: 0.035500, KL fake Loss: 0.004372
Classification Train Epoch: 16 [38400/63553 (60%)]	Loss: 0.071969, KL fake Loss: 0.002423
Classification Train Epoch: 16 [44800/63553 (70%)]	Loss: 0.029493, KL fake Loss: 0.006002
Classification Train Epoch: 16 [51200/63553 (80%)]	Loss: 0.109525, KL fake Loss: 0.014349
Classification Train Epoch: 16 [57600/63553 (91%)]	Loss: 0.122118, KL fake Loss: 0.003524

Test set: Average loss: 0.4812, Accuracy: 20302/22777 (89%)

Classification Train Epoch: 17 [0/63553 (0%)]	Loss: 0.040211, KL fake Loss: 0.014754
Classification Train Epoch: 17 [6400/63553 (10%)]	Loss: 0.002731, KL fake Loss: 0.001766
Classification Train Epoch: 17 [12800/63553 (20%)]	Loss: 0.008566, KL fake Loss: 0.001870
Classification Train Epoch: 17 [19200/63553 (30%)]	Loss: 0.007596, KL fake Loss: 0.001965
Classification Train Epoch: 17 [25600/63553 (40%)]	Loss: 0.074882, KL fake Loss: 0.003085
Classification Train Epoch: 17 [32000/63553 (50%)]	Loss: 0.014626, KL fake Loss: 0.002897
Classification Train Epoch: 17 [38400/63553 (60%)]	Loss: 0.026182, KL fake Loss: 0.001056
Classification Train Epoch: 17 [44800/63553 (70%)]	Loss: 0.044469, KL fake Loss: 0.001048
Classification Train Epoch: 17 [51200/63553 (80%)]	Loss: 0.216207, KL fake Loss: 0.003161
Classification Train Epoch: 17 [57600/63553 (91%)]	Loss: 0.014369, KL fake Loss: 0.001235

Test set: Average loss: 0.3708, Accuracy: 20939/22777 (92%)

Classification Train Epoch: 18 [0/63553 (0%)]	Loss: 0.007980, KL fake Loss: 0.001205
Classification Train Epoch: 18 [6400/63553 (10%)]	Loss: 0.004406, KL fake Loss: 0.001511
 18%|█▊        | 18/100 [1:19:16<6:01:06, 264.23s/it] 19%|█▉        | 19/100 [1:23:40<5:56:42, 264.22s/it] 20%|██        | 20/100 [1:28:04<5:52:21, 264.27s/it] 21%|██        | 21/100 [1:32:29<5:47:56, 264.26s/it] 22%|██▏       | 22/100 [1:36:53<5:43:31, 264.25s/it] 23%|██▎       | 23/100 [1:41:17<5:39:06, 264.25s/it] 24%|██▍       | 24/100 [1:45:41<5:34:41, 264.24s/it] 25%|██▌       | 25/100 [1:50:06<5:30:16, 264.23s/it]Classification Train Epoch: 18 [12800/63553 (20%)]	Loss: 0.030341, KL fake Loss: 0.000776
Classification Train Epoch: 18 [19200/63553 (30%)]	Loss: 0.021280, KL fake Loss: 0.001518
Classification Train Epoch: 18 [25600/63553 (40%)]	Loss: 0.004403, KL fake Loss: 0.001178
Classification Train Epoch: 18 [32000/63553 (50%)]	Loss: 0.002279, KL fake Loss: 0.011846
Classification Train Epoch: 18 [38400/63553 (60%)]	Loss: 0.009625, KL fake Loss: 0.008240
Classification Train Epoch: 18 [44800/63553 (70%)]	Loss: 0.064963, KL fake Loss: 0.001703
Classification Train Epoch: 18 [51200/63553 (80%)]	Loss: 0.076812, KL fake Loss: 0.001078
Classification Train Epoch: 18 [57600/63553 (91%)]	Loss: 0.005398, KL fake Loss: 0.001431

Test set: Average loss: 0.6437, Accuracy: 19622/22777 (86%)

Classification Train Epoch: 19 [0/63553 (0%)]	Loss: 0.158186, KL fake Loss: 0.011374
Classification Train Epoch: 19 [6400/63553 (10%)]	Loss: 0.015222, KL fake Loss: 0.000876
Classification Train Epoch: 19 [12800/63553 (20%)]	Loss: 0.007313, KL fake Loss: 0.001133
Classification Train Epoch: 19 [19200/63553 (30%)]	Loss: 0.003528, KL fake Loss: 0.002150
Classification Train Epoch: 19 [25600/63553 (40%)]	Loss: 0.080575, KL fake Loss: 0.002073
Classification Train Epoch: 19 [32000/63553 (50%)]	Loss: 0.058304, KL fake Loss: 0.000908
Classification Train Epoch: 19 [38400/63553 (60%)]	Loss: 0.011054, KL fake Loss: 0.000914
Classification Train Epoch: 19 [44800/63553 (70%)]	Loss: 0.048326, KL fake Loss: 0.001596
Classification Train Epoch: 19 [51200/63553 (80%)]	Loss: 0.047555, KL fake Loss: 0.001579
Classification Train Epoch: 19 [57600/63553 (91%)]	Loss: 0.014971, KL fake Loss: 0.002229

Test set: Average loss: 0.4998, Accuracy: 20386/22777 (90%)

Classification Train Epoch: 20 [0/63553 (0%)]	Loss: 0.004862, KL fake Loss: 0.004043
Classification Train Epoch: 20 [6400/63553 (10%)]	Loss: 0.041996, KL fake Loss: 0.000581
Classification Train Epoch: 20 [12800/63553 (20%)]	Loss: 0.020290, KL fake Loss: 0.001132
Classification Train Epoch: 20 [19200/63553 (30%)]	Loss: 0.009734, KL fake Loss: 0.001055
Classification Train Epoch: 20 [25600/63553 (40%)]	Loss: 0.063286, KL fake Loss: 0.001598
Classification Train Epoch: 20 [32000/63553 (50%)]	Loss: 0.002903, KL fake Loss: 0.001003
Classification Train Epoch: 20 [38400/63553 (60%)]	Loss: 0.021857, KL fake Loss: 0.000933
Classification Train Epoch: 20 [44800/63553 (70%)]	Loss: 0.136219, KL fake Loss: 0.000878
Classification Train Epoch: 20 [51200/63553 (80%)]	Loss: 0.038984, KL fake Loss: 0.001127
Classification Train Epoch: 20 [57600/63553 (91%)]	Loss: 0.036238, KL fake Loss: 0.000918

Test set: Average loss: 0.3729, Accuracy: 20897/22777 (92%)

Classification Train Epoch: 21 [0/63553 (0%)]	Loss: 0.005106, KL fake Loss: 0.020211
Classification Train Epoch: 21 [6400/63553 (10%)]	Loss: 0.032987, KL fake Loss: 0.001359
Classification Train Epoch: 21 [12800/63553 (20%)]	Loss: 0.009612, KL fake Loss: 0.000766
Classification Train Epoch: 21 [19200/63553 (30%)]	Loss: 0.003960, KL fake Loss: 0.000779
Classification Train Epoch: 21 [25600/63553 (40%)]	Loss: 0.011040, KL fake Loss: 0.000911
Classification Train Epoch: 21 [32000/63553 (50%)]	Loss: 0.004619, KL fake Loss: 0.000411
Classification Train Epoch: 21 [38400/63553 (60%)]	Loss: 0.031169, KL fake Loss: 0.005646
Classification Train Epoch: 21 [44800/63553 (70%)]	Loss: 0.061361, KL fake Loss: 0.000664
Classification Train Epoch: 21 [51200/63553 (80%)]	Loss: 0.002180, KL fake Loss: 0.000575
Classification Train Epoch: 21 [57600/63553 (91%)]	Loss: 0.023535, KL fake Loss: 0.002173

Test set: Average loss: 0.5206, Accuracy: 20655/22777 (91%)

Classification Train Epoch: 22 [0/63553 (0%)]	Loss: 0.009830, KL fake Loss: 0.019066
Classification Train Epoch: 22 [6400/63553 (10%)]	Loss: 0.004594, KL fake Loss: 0.000599
Classification Train Epoch: 22 [12800/63553 (20%)]	Loss: 0.008888, KL fake Loss: 0.000564
Classification Train Epoch: 22 [19200/63553 (30%)]	Loss: 0.023149, KL fake Loss: 0.000523
Classification Train Epoch: 22 [25600/63553 (40%)]	Loss: 0.049118, KL fake Loss: 0.000475
Classification Train Epoch: 22 [32000/63553 (50%)]	Loss: 0.008843, KL fake Loss: 0.000540
Classification Train Epoch: 22 [38400/63553 (60%)]	Loss: 0.049944, KL fake Loss: 0.001912
Classification Train Epoch: 22 [44800/63553 (70%)]	Loss: 0.005632, KL fake Loss: 0.002112
Classification Train Epoch: 22 [51200/63553 (80%)]	Loss: 0.029851, KL fake Loss: 0.000671
Classification Train Epoch: 22 [57600/63553 (91%)]	Loss: 0.016755, KL fake Loss: 0.000306

Test set: Average loss: 0.4113, Accuracy: 20902/22777 (92%)

Classification Train Epoch: 23 [0/63553 (0%)]	Loss: 0.013359, KL fake Loss: 0.015187
Classification Train Epoch: 23 [6400/63553 (10%)]	Loss: 0.016398, KL fake Loss: 0.001853
Classification Train Epoch: 23 [12800/63553 (20%)]	Loss: 0.018670, KL fake Loss: 0.001089
Classification Train Epoch: 23 [19200/63553 (30%)]	Loss: 0.023135, KL fake Loss: 0.000996
Classification Train Epoch: 23 [25600/63553 (40%)]	Loss: 0.017057, KL fake Loss: 0.000669
Classification Train Epoch: 23 [32000/63553 (50%)]	Loss: 0.076582, KL fake Loss: 0.000567
Classification Train Epoch: 23 [38400/63553 (60%)]	Loss: 0.005676, KL fake Loss: 0.000964
Classification Train Epoch: 23 [44800/63553 (70%)]	Loss: 0.009976, KL fake Loss: 0.005423
Classification Train Epoch: 23 [51200/63553 (80%)]	Loss: 0.014307, KL fake Loss: 0.000565
Classification Train Epoch: 23 [57600/63553 (91%)]	Loss: 0.006564, KL fake Loss: 0.000462

Test set: Average loss: 0.6628, Accuracy: 19489/22777 (86%)

Classification Train Epoch: 24 [0/63553 (0%)]	Loss: 0.039797, KL fake Loss: 0.012820
Classification Train Epoch: 24 [6400/63553 (10%)]	Loss: 0.091197, KL fake Loss: 0.000798
Classification Train Epoch: 24 [12800/63553 (20%)]	Loss: 0.006177, KL fake Loss: 0.027102
Classification Train Epoch: 24 [19200/63553 (30%)]	Loss: 0.007821, KL fake Loss: 0.001110
Classification Train Epoch: 24 [25600/63553 (40%)]	Loss: 0.037868, KL fake Loss: 0.004757
Classification Train Epoch: 24 [32000/63553 (50%)]	Loss: 0.003261, KL fake Loss: 0.000658
Classification Train Epoch: 24 [38400/63553 (60%)]	Loss: 0.013331, KL fake Loss: 0.000682
Classification Train Epoch: 24 [44800/63553 (70%)]	Loss: 0.016288, KL fake Loss: 0.005715
Classification Train Epoch: 24 [51200/63553 (80%)]	Loss: 0.003335, KL fake Loss: 0.000550
Classification Train Epoch: 24 [57600/63553 (91%)]	Loss: 0.007538, KL fake Loss: 0.004376

Test set: Average loss: 0.4639, Accuracy: 20580/22777 (90%)

Classification Train Epoch: 25 [0/63553 (0%)]	Loss: 0.024873, KL fake Loss: 0.006007
Classification Train Epoch: 25 [6400/63553 (10%)]	Loss: 0.001329, KL fake Loss: 0.000260
Classification Train Epoch: 25 [12800/63553 (20%)]	Loss: 0.014583, KL fake Loss: 0.000495
Classification Train Epoch: 25 [19200/63553 (30%)]	Loss: 0.040110, KL fake Loss: 0.000348
Classification Train Epoch: 25 [25600/63553 (40%)]	Loss: 0.082650, KL fake Loss: 0.052109
Classification Train Epoch: 25 [32000/63553 (50%)]	Loss: 0.000400, KL fake Loss: 0.000273
Classification Train Epoch: 25 [38400/63553 (60%)]	Loss: 0.005692, KL fake Loss: 0.052492
Classification Train Epoch: 25 [44800/63553 (70%)]	Loss: 0.028384, KL fake Loss: 0.000388
Classification Train Epoch: 25 [51200/63553 (80%)]	Loss: 0.056875, KL fake Loss: 0.000950
Classification Train Epoch: 25 [57600/63553 (91%)]	Loss: 0.020343, KL fake Loss: 0.000358

Test set: Average loss: 0.4125, Accuracy: 20938/22777 (92%)

Classification Train Epoch: 26 [0/63553 (0%)]	Loss: 0.011833, KL fake Loss: 0.000698
Classification Train Epoch: 26 [6400/63553 (10%)]	Loss: 0.002727, KL fake Loss: 0.000241
Classification Train Epoch: 26 [12800/63553 (20%)]	Loss: 0.008750, KL fake Loss: 0.003202
Classification Train Epoch: 26 [19200/63553 (30%)]	Loss: 0.014271, KL fake Loss: 0.000784
Classification Train Epoch: 26 [25600/63553 (40%)]	Loss: 0.004810, KL fake Loss: 0.000665
Classification Train Epoch: 26 [32000/63553 (50%)]	Loss: 0.036392, KL fake Loss: 0.000646
Classification Train Epoch: 26 [38400/63553 (60%)]	Loss: 0.007549, KL fake Loss: 0.001041
Classification Train Epoch: 26 [44800/63553 (70%)]	Loss: 0.060683, KL fake Loss: 0.002186
 26%|██▌       | 26/100 [1:54:30<5:25:52, 264.22s/it] 27%|██▋       | 27/100 [1:58:54<5:21:27, 264.22s/it] 28%|██▊       | 28/100 [2:03:18<5:17:04, 264.22s/it] 29%|██▉       | 29/100 [2:07:42<5:12:39, 264.22s/it] 30%|███       | 30/100 [2:12:07<5:08:15, 264.22s/it] 31%|███       | 31/100 [2:16:31<5:03:50, 264.21s/it] 32%|███▏      | 32/100 [2:20:55<4:59:26, 264.22s/it] 33%|███▎      | 33/100 [2:25:19<4:55:02, 264.21s/it] 34%|███▍      | 34/100 [2:29:43<4:50:37, 264.21s/it]Classification Train Epoch: 26 [51200/63553 (80%)]	Loss: 0.011994, KL fake Loss: 0.000641
Classification Train Epoch: 26 [57600/63553 (91%)]	Loss: 0.002581, KL fake Loss: 0.005942

Test set: Average loss: 0.6332, Accuracy: 20099/22777 (88%)

Classification Train Epoch: 27 [0/63553 (0%)]	Loss: 0.004778, KL fake Loss: 0.000550
Classification Train Epoch: 27 [6400/63553 (10%)]	Loss: 0.007530, KL fake Loss: 0.001530
Classification Train Epoch: 27 [12800/63553 (20%)]	Loss: 0.006462, KL fake Loss: 0.000303
Classification Train Epoch: 27 [19200/63553 (30%)]	Loss: 0.013263, KL fake Loss: 0.000421
Classification Train Epoch: 27 [25600/63553 (40%)]	Loss: 0.004004, KL fake Loss: 0.000331
Classification Train Epoch: 27 [32000/63553 (50%)]	Loss: 0.025803, KL fake Loss: 0.000458
Classification Train Epoch: 27 [38400/63553 (60%)]	Loss: 0.025324, KL fake Loss: 0.000882
Classification Train Epoch: 27 [44800/63553 (70%)]	Loss: 0.002641, KL fake Loss: 0.000346
Classification Train Epoch: 27 [51200/63553 (80%)]	Loss: 0.005974, KL fake Loss: 0.000324
Classification Train Epoch: 27 [57600/63553 (91%)]	Loss: 0.004271, KL fake Loss: 0.000972

Test set: Average loss: 0.5539, Accuracy: 20431/22777 (90%)

Classification Train Epoch: 28 [0/63553 (0%)]	Loss: 0.030563, KL fake Loss: 0.028427
Classification Train Epoch: 28 [6400/63553 (10%)]	Loss: 0.141410, KL fake Loss: 0.002275
Classification Train Epoch: 28 [12800/63553 (20%)]	Loss: 0.010925, KL fake Loss: 0.000609
Classification Train Epoch: 28 [19200/63553 (30%)]	Loss: 0.045999, KL fake Loss: 0.000296
Classification Train Epoch: 28 [25600/63553 (40%)]	Loss: 0.020079, KL fake Loss: 0.004304
Classification Train Epoch: 28 [32000/63553 (50%)]	Loss: 0.008885, KL fake Loss: 0.001539
Classification Train Epoch: 28 [38400/63553 (60%)]	Loss: 0.074536, KL fake Loss: 0.000628
Classification Train Epoch: 28 [44800/63553 (70%)]	Loss: 0.007705, KL fake Loss: 0.000372
Classification Train Epoch: 28 [51200/63553 (80%)]	Loss: 0.022750, KL fake Loss: 0.000352
Classification Train Epoch: 28 [57600/63553 (91%)]	Loss: 0.004545, KL fake Loss: 0.000891

Test set: Average loss: 0.5387, Accuracy: 20394/22777 (90%)

Classification Train Epoch: 29 [0/63553 (0%)]	Loss: 0.101202, KL fake Loss: 0.039230
Classification Train Epoch: 29 [6400/63553 (10%)]	Loss: 0.006015, KL fake Loss: 0.001643
Classification Train Epoch: 29 [12800/63553 (20%)]	Loss: 0.009936, KL fake Loss: 0.000491
Classification Train Epoch: 29 [19200/63553 (30%)]	Loss: 0.062214, KL fake Loss: 0.010658
Classification Train Epoch: 29 [25600/63553 (40%)]	Loss: 0.001730, KL fake Loss: 0.006446
Classification Train Epoch: 29 [32000/63553 (50%)]	Loss: 0.024041, KL fake Loss: 0.000549
Classification Train Epoch: 29 [38400/63553 (60%)]	Loss: 0.030452, KL fake Loss: 0.005417
Classification Train Epoch: 29 [44800/63553 (70%)]	Loss: 0.010820, KL fake Loss: 0.001461
Classification Train Epoch: 29 [51200/63553 (80%)]	Loss: 0.006802, KL fake Loss: 0.000941
Classification Train Epoch: 29 [57600/63553 (91%)]	Loss: 0.035407, KL fake Loss: 0.007637

Test set: Average loss: 0.4063, Accuracy: 21281/22777 (93%)

Classification Train Epoch: 30 [0/63553 (0%)]	Loss: 0.000300, KL fake Loss: 0.000310
Classification Train Epoch: 30 [6400/63553 (10%)]	Loss: 0.030100, KL fake Loss: 0.001069
Classification Train Epoch: 30 [12800/63553 (20%)]	Loss: 0.000878, KL fake Loss: 0.001580
Classification Train Epoch: 30 [19200/63553 (30%)]	Loss: 0.048596, KL fake Loss: 0.001675
Classification Train Epoch: 30 [25600/63553 (40%)]	Loss: 0.013123, KL fake Loss: 0.003308
Classification Train Epoch: 30 [32000/63553 (50%)]	Loss: 0.017896, KL fake Loss: 0.006261
Classification Train Epoch: 30 [38400/63553 (60%)]	Loss: 0.017386, KL fake Loss: 0.002802
Classification Train Epoch: 30 [44800/63553 (70%)]	Loss: 0.001877, KL fake Loss: 0.010408
Classification Train Epoch: 30 [51200/63553 (80%)]	Loss: 0.002866, KL fake Loss: 0.001911
Classification Train Epoch: 30 [57600/63553 (91%)]	Loss: 0.000676, KL fake Loss: 0.000249

Test set: Average loss: 0.7376, Accuracy: 19568/22777 (86%)

Classification Train Epoch: 31 [0/63553 (0%)]	Loss: 0.002777, KL fake Loss: 0.011388
Classification Train Epoch: 31 [6400/63553 (10%)]	Loss: 0.001805, KL fake Loss: 0.005281
Classification Train Epoch: 31 [12800/63553 (20%)]	Loss: 0.006428, KL fake Loss: 0.001110
Classification Train Epoch: 31 [19200/63553 (30%)]	Loss: 0.003250, KL fake Loss: 0.000633
Classification Train Epoch: 31 [25600/63553 (40%)]	Loss: 0.030212, KL fake Loss: 0.008993
Classification Train Epoch: 31 [32000/63553 (50%)]	Loss: 0.002051, KL fake Loss: 0.001047
Classification Train Epoch: 31 [38400/63553 (60%)]	Loss: 0.117607, KL fake Loss: 0.001255
Classification Train Epoch: 31 [44800/63553 (70%)]	Loss: 0.009320, KL fake Loss: 0.001341
Classification Train Epoch: 31 [51200/63553 (80%)]	Loss: 0.007716, KL fake Loss: 0.002241
Classification Train Epoch: 31 [57600/63553 (91%)]	Loss: 0.002983, KL fake Loss: 0.002373

Test set: Average loss: 0.5397, Accuracy: 20844/22777 (92%)

Classification Train Epoch: 32 [0/63553 (0%)]	Loss: 0.016240, KL fake Loss: 0.026562
Classification Train Epoch: 32 [6400/63553 (10%)]	Loss: 0.058790, KL fake Loss: 0.002549
Classification Train Epoch: 32 [12800/63553 (20%)]	Loss: 0.026816, KL fake Loss: 0.000819
Classification Train Epoch: 32 [19200/63553 (30%)]	Loss: 0.043489, KL fake Loss: 0.010135
Classification Train Epoch: 32 [25600/63553 (40%)]	Loss: 0.009544, KL fake Loss: 0.000697
Classification Train Epoch: 32 [32000/63553 (50%)]	Loss: 0.036965, KL fake Loss: 0.000583
Classification Train Epoch: 32 [38400/63553 (60%)]	Loss: 0.009755, KL fake Loss: 0.003390
Classification Train Epoch: 32 [44800/63553 (70%)]	Loss: 0.002365, KL fake Loss: 0.001273
Classification Train Epoch: 32 [51200/63553 (80%)]	Loss: 0.000676, KL fake Loss: 0.006489
Classification Train Epoch: 32 [57600/63553 (91%)]	Loss: 0.121233, KL fake Loss: 0.000663

Test set: Average loss: 0.4560, Accuracy: 20978/22777 (92%)

Classification Train Epoch: 33 [0/63553 (0%)]	Loss: 0.000348, KL fake Loss: 0.001997
Classification Train Epoch: 33 [6400/63553 (10%)]	Loss: 0.013178, KL fake Loss: 0.002632
Classification Train Epoch: 33 [12800/63553 (20%)]	Loss: 0.014870, KL fake Loss: 0.004940
Classification Train Epoch: 33 [19200/63553 (30%)]	Loss: 0.002211, KL fake Loss: 0.004344
Classification Train Epoch: 33 [25600/63553 (40%)]	Loss: 0.038639, KL fake Loss: 0.000579
Classification Train Epoch: 33 [32000/63553 (50%)]	Loss: 0.011115, KL fake Loss: 0.000850
Classification Train Epoch: 33 [38400/63553 (60%)]	Loss: 0.001272, KL fake Loss: 0.029718
Classification Train Epoch: 33 [44800/63553 (70%)]	Loss: 0.049758, KL fake Loss: 0.012573
Classification Train Epoch: 33 [51200/63553 (80%)]	Loss: 0.015619, KL fake Loss: 0.000418
Classification Train Epoch: 33 [57600/63553 (91%)]	Loss: 0.002135, KL fake Loss: 0.002779

Test set: Average loss: 0.5478, Accuracy: 20410/22777 (90%)

Classification Train Epoch: 34 [0/63553 (0%)]	Loss: 0.055052, KL fake Loss: 0.020335
Classification Train Epoch: 34 [6400/63553 (10%)]	Loss: 0.001743, KL fake Loss: 0.001612
Classification Train Epoch: 34 [12800/63553 (20%)]	Loss: 0.027074, KL fake Loss: 0.015110
Classification Train Epoch: 34 [19200/63553 (30%)]	Loss: 0.030569, KL fake Loss: 0.001311
Classification Train Epoch: 34 [25600/63553 (40%)]	Loss: 0.001187, KL fake Loss: 0.002504
Classification Train Epoch: 34 [32000/63553 (50%)]	Loss: 0.003659, KL fake Loss: 0.000196
Classification Train Epoch: 34 [38400/63553 (60%)]	Loss: 0.004883, KL fake Loss: 0.000249
Classification Train Epoch: 34 [44800/63553 (70%)]	Loss: 0.002204, KL fake Loss: 0.004177
Classification Train Epoch: 34 [51200/63553 (80%)]	Loss: 0.001195, KL fake Loss: 0.002293
Classification Train Epoch: 34 [57600/63553 (91%)]	Loss: 0.017884, KL fake Loss: 0.000602

Test set: Average loss: 0.5334, Accuracy: 20517/22777 (90%)

Classification Train Epoch: 35 [0/63553 (0%)]	Loss: 0.000752, KL fake Loss: 0.111338
Classification Train Epoch: 35 [6400/63553 (10%)]	Loss: 0.067343, KL fake Loss: 0.002341
Classification Train Epoch: 35 [12800/63553 (20%)]	Loss: 0.013768, KL fake Loss: 0.005443
 35%|███▌      | 35/100 [2:34:08<4:46:13, 264.21s/it] 36%|███▌      | 36/100 [2:38:32<4:41:49, 264.21s/it] 37%|███▋      | 37/100 [2:42:56<4:37:25, 264.21s/it] 38%|███▊      | 38/100 [2:47:20<4:33:00, 264.21s/it] 39%|███▉      | 39/100 [2:51:45<4:28:36, 264.21s/it] 40%|████      | 40/100 [2:56:09<4:24:14, 264.24s/it] 41%|████      | 41/100 [3:00:33<4:19:49, 264.23s/it] 42%|████▏     | 42/100 [3:04:57<4:15:24, 264.22s/it]Classification Train Epoch: 35 [19200/63553 (30%)]	Loss: 0.030069, KL fake Loss: 0.000856
Classification Train Epoch: 35 [25600/63553 (40%)]	Loss: 0.024822, KL fake Loss: 0.000791
Classification Train Epoch: 35 [32000/63553 (50%)]	Loss: 0.000399, KL fake Loss: 0.000730
Classification Train Epoch: 35 [38400/63553 (60%)]	Loss: 0.001813, KL fake Loss: 0.000476
Classification Train Epoch: 35 [44800/63553 (70%)]	Loss: 0.012597, KL fake Loss: 0.001361
Classification Train Epoch: 35 [51200/63553 (80%)]	Loss: 0.006270, KL fake Loss: 0.000503
Classification Train Epoch: 35 [57600/63553 (91%)]	Loss: 0.011567, KL fake Loss: 0.005700

Test set: Average loss: 0.5816, Accuracy: 20574/22777 (90%)

Classification Train Epoch: 36 [0/63553 (0%)]	Loss: 0.012953, KL fake Loss: 0.001994
Classification Train Epoch: 36 [6400/63553 (10%)]	Loss: 0.000577, KL fake Loss: 0.001987
Classification Train Epoch: 36 [12800/63553 (20%)]	Loss: 0.013855, KL fake Loss: 0.023193
Classification Train Epoch: 36 [19200/63553 (30%)]	Loss: 0.011520, KL fake Loss: 0.001540
Classification Train Epoch: 36 [25600/63553 (40%)]	Loss: 0.017945, KL fake Loss: 0.017206
Classification Train Epoch: 36 [32000/63553 (50%)]	Loss: 0.000263, KL fake Loss: 0.000494
Classification Train Epoch: 36 [38400/63553 (60%)]	Loss: 0.000577, KL fake Loss: 0.000280
Classification Train Epoch: 36 [44800/63553 (70%)]	Loss: 0.001557, KL fake Loss: 0.000421
Classification Train Epoch: 36 [51200/63553 (80%)]	Loss: 0.000250, KL fake Loss: 0.002800
Classification Train Epoch: 36 [57600/63553 (91%)]	Loss: 0.000872, KL fake Loss: 0.001071

Test set: Average loss: 0.7669, Accuracy: 20175/22777 (89%)

Classification Train Epoch: 37 [0/63553 (0%)]	Loss: 0.029868, KL fake Loss: 0.081973
Classification Train Epoch: 37 [6400/63553 (10%)]	Loss: 0.041612, KL fake Loss: 0.003813
Classification Train Epoch: 37 [12800/63553 (20%)]	Loss: 0.001770, KL fake Loss: 0.003486
Classification Train Epoch: 37 [19200/63553 (30%)]	Loss: 0.010573, KL fake Loss: 0.000484
Classification Train Epoch: 37 [25600/63553 (40%)]	Loss: 0.008051, KL fake Loss: 0.000370
Classification Train Epoch: 37 [32000/63553 (50%)]	Loss: 0.011672, KL fake Loss: 0.000580
Classification Train Epoch: 37 [38400/63553 (60%)]	Loss: 0.024568, KL fake Loss: 0.000170
Classification Train Epoch: 37 [44800/63553 (70%)]	Loss: 0.001277, KL fake Loss: 0.000856
Classification Train Epoch: 37 [51200/63553 (80%)]	Loss: 0.026772, KL fake Loss: 0.000272
Classification Train Epoch: 37 [57600/63553 (91%)]	Loss: 0.000233, KL fake Loss: 0.000675

Test set: Average loss: 0.3819, Accuracy: 21272/22777 (93%)

Classification Train Epoch: 38 [0/63553 (0%)]	Loss: 0.000309, KL fake Loss: 0.037806
Classification Train Epoch: 38 [6400/63553 (10%)]	Loss: 0.065987, KL fake Loss: 0.000855
Classification Train Epoch: 38 [12800/63553 (20%)]	Loss: 0.053755, KL fake Loss: 0.000854
Classification Train Epoch: 38 [19200/63553 (30%)]	Loss: 0.018349, KL fake Loss: 0.000290
Classification Train Epoch: 38 [25600/63553 (40%)]	Loss: 0.002511, KL fake Loss: 0.000144
Classification Train Epoch: 38 [32000/63553 (50%)]	Loss: 0.000537, KL fake Loss: 0.000282
Classification Train Epoch: 38 [38400/63553 (60%)]	Loss: 0.004046, KL fake Loss: 0.000991
Classification Train Epoch: 38 [44800/63553 (70%)]	Loss: 0.043870, KL fake Loss: 0.000625
Classification Train Epoch: 38 [51200/63553 (80%)]	Loss: 0.001800, KL fake Loss: 0.088170
Classification Train Epoch: 38 [57600/63553 (91%)]	Loss: 0.027554, KL fake Loss: 0.000514

Test set: Average loss: 0.5292, Accuracy: 20678/22777 (91%)

Classification Train Epoch: 39 [0/63553 (0%)]	Loss: 0.002638, KL fake Loss: 0.000718
Classification Train Epoch: 39 [6400/63553 (10%)]	Loss: 0.048077, KL fake Loss: 0.000418
Classification Train Epoch: 39 [12800/63553 (20%)]	Loss: 0.003599, KL fake Loss: 0.000292
Classification Train Epoch: 39 [19200/63553 (30%)]	Loss: 0.079779, KL fake Loss: 0.000496
Classification Train Epoch: 39 [25600/63553 (40%)]	Loss: 0.016139, KL fake Loss: 0.000704
Classification Train Epoch: 39 [32000/63553 (50%)]	Loss: 0.014621, KL fake Loss: 0.025034
Classification Train Epoch: 39 [38400/63553 (60%)]	Loss: 0.071781, KL fake Loss: 0.000213
Classification Train Epoch: 39 [44800/63553 (70%)]	Loss: 0.008288, KL fake Loss: 0.035053
Classification Train Epoch: 39 [51200/63553 (80%)]	Loss: 0.009984, KL fake Loss: 0.000524
Classification Train Epoch: 39 [57600/63553 (91%)]	Loss: 0.008306, KL fake Loss: 0.004187

Test set: Average loss: 0.4836, Accuracy: 20884/22777 (92%)

Classification Train Epoch: 40 [0/63553 (0%)]	Loss: 0.075612, KL fake Loss: 0.006827
Classification Train Epoch: 40 [6400/63553 (10%)]	Loss: 0.054235, KL fake Loss: 0.000469
Classification Train Epoch: 40 [12800/63553 (20%)]	Loss: 0.001292, KL fake Loss: 0.001075
Classification Train Epoch: 40 [19200/63553 (30%)]	Loss: 0.005321, KL fake Loss: 0.001367
Classification Train Epoch: 40 [25600/63553 (40%)]	Loss: 0.001702, KL fake Loss: 0.000356
Classification Train Epoch: 40 [32000/63553 (50%)]	Loss: 0.040234, KL fake Loss: 0.001689
Classification Train Epoch: 40 [38400/63553 (60%)]	Loss: 0.026382, KL fake Loss: 0.000160
Classification Train Epoch: 40 [44800/63553 (70%)]	Loss: 0.000741, KL fake Loss: 0.000267
Classification Train Epoch: 40 [51200/63553 (80%)]	Loss: 0.000709, KL fake Loss: 0.000474
Classification Train Epoch: 40 [57600/63553 (91%)]	Loss: 0.001691, KL fake Loss: 0.000470

Test set: Average loss: 0.4643, Accuracy: 21332/22777 (94%)

Classification Train Epoch: 41 [0/63553 (0%)]	Loss: 0.001916, KL fake Loss: 0.000358
Classification Train Epoch: 41 [6400/63553 (10%)]	Loss: 0.015515, KL fake Loss: 0.001172
Classification Train Epoch: 41 [12800/63553 (20%)]	Loss: 0.006263, KL fake Loss: 0.000724
Classification Train Epoch: 41 [19200/63553 (30%)]	Loss: 0.015823, KL fake Loss: 0.000704
Classification Train Epoch: 41 [25600/63553 (40%)]	Loss: 0.001688, KL fake Loss: 0.002359
Classification Train Epoch: 41 [32000/63553 (50%)]	Loss: 0.001095, KL fake Loss: 0.002664
Classification Train Epoch: 41 [38400/63553 (60%)]	Loss: 0.001302, KL fake Loss: 0.003516
Classification Train Epoch: 41 [44800/63553 (70%)]	Loss: 0.006903, KL fake Loss: 0.000138
Classification Train Epoch: 41 [51200/63553 (80%)]	Loss: 0.004186, KL fake Loss: 0.011269
Classification Train Epoch: 41 [57600/63553 (91%)]	Loss: 0.060340, KL fake Loss: 0.000143

Test set: Average loss: 0.5810, Accuracy: 20650/22777 (91%)

Classification Train Epoch: 42 [0/63553 (0%)]	Loss: 0.000563, KL fake Loss: 0.006582
Classification Train Epoch: 42 [6400/63553 (10%)]	Loss: 0.003244, KL fake Loss: 0.000169
Classification Train Epoch: 42 [12800/63553 (20%)]	Loss: 0.002986, KL fake Loss: 0.002101
Classification Train Epoch: 42 [19200/63553 (30%)]	Loss: 0.000586, KL fake Loss: 0.000310
Classification Train Epoch: 42 [25600/63553 (40%)]	Loss: 0.003903, KL fake Loss: 0.002233
Classification Train Epoch: 42 [32000/63553 (50%)]	Loss: 0.000866, KL fake Loss: 0.000843
Classification Train Epoch: 42 [38400/63553 (60%)]	Loss: 0.001389, KL fake Loss: 0.000108
Classification Train Epoch: 42 [44800/63553 (70%)]	Loss: 0.026065, KL fake Loss: 0.000330
Classification Train Epoch: 42 [51200/63553 (80%)]	Loss: 0.022680, KL fake Loss: 0.000389
Classification Train Epoch: 42 [57600/63553 (91%)]	Loss: 0.076228, KL fake Loss: 0.003803

Test set: Average loss: 0.5377, Accuracy: 20810/22777 (91%)

Classification Train Epoch: 43 [0/63553 (0%)]	Loss: 0.000094, KL fake Loss: 0.000286
Classification Train Epoch: 43 [6400/63553 (10%)]	Loss: 0.026137, KL fake Loss: 0.000927
Classification Train Epoch: 43 [12800/63553 (20%)]	Loss: 0.002889, KL fake Loss: 0.000353
Classification Train Epoch: 43 [19200/63553 (30%)]	Loss: 0.000285, KL fake Loss: 0.013806
Classification Train Epoch: 43 [25600/63553 (40%)]	Loss: 0.001725, KL fake Loss: 0.000760
Classification Train Epoch: 43 [32000/63553 (50%)]	Loss: 0.002377, KL fake Loss: 0.000567
Classification Train Epoch: 43 [38400/63553 (60%)]	Loss: 0.015250, KL fake Loss: 0.000883
Classification Train Epoch: 43 [44800/63553 (70%)]	Loss: 0.038345, KL fake Loss: 0.000261
Classification Train Epoch: 43 [51200/63553 (80%)]	Loss: 0.002770, KL fake Loss: 0.000404
 43%|████▎     | 43/100 [3:09:21<4:11:00, 264.21s/it] 44%|████▍     | 44/100 [3:13:46<4:06:35, 264.21s/it] 45%|████▌     | 45/100 [3:18:10<4:02:11, 264.20s/it] 46%|████▌     | 46/100 [3:22:34<3:57:46, 264.20s/it] 47%|████▋     | 47/100 [3:26:58<3:53:22, 264.20s/it] 48%|████▊     | 48/100 [3:31:22<3:48:58, 264.19s/it] 49%|████▉     | 49/100 [3:35:47<3:44:33, 264.19s/it] 50%|█████     | 50/100 [3:40:11<3:40:09, 264.19s/it] 51%|█████     | 51/100 [3:44:35<3:35:45, 264.19s/it]Classification Train Epoch: 43 [57600/63553 (91%)]	Loss: 0.035857, KL fake Loss: 0.001924

Test set: Average loss: 0.4443, Accuracy: 21197/22777 (93%)

Classification Train Epoch: 44 [0/63553 (0%)]	Loss: 0.045458, KL fake Loss: 0.005058
Classification Train Epoch: 44 [6400/63553 (10%)]	Loss: 0.002805, KL fake Loss: 0.000270
Classification Train Epoch: 44 [12800/63553 (20%)]	Loss: 0.007636, KL fake Loss: 0.000297
Classification Train Epoch: 44 [19200/63553 (30%)]	Loss: 0.016566, KL fake Loss: 0.000137
Classification Train Epoch: 44 [25600/63553 (40%)]	Loss: 0.004022, KL fake Loss: 0.000526
Classification Train Epoch: 44 [32000/63553 (50%)]	Loss: 0.004211, KL fake Loss: 0.002244
Classification Train Epoch: 44 [38400/63553 (60%)]	Loss: 0.034372, KL fake Loss: 0.001270
Classification Train Epoch: 44 [44800/63553 (70%)]	Loss: 0.006901, KL fake Loss: 0.000261
Classification Train Epoch: 44 [51200/63553 (80%)]	Loss: 0.048921, KL fake Loss: 0.001530
Classification Train Epoch: 44 [57600/63553 (91%)]	Loss: 0.011858, KL fake Loss: 0.000378

Test set: Average loss: 0.4509, Accuracy: 21156/22777 (93%)

Classification Train Epoch: 45 [0/63553 (0%)]	Loss: 0.037729, KL fake Loss: 0.002719
Classification Train Epoch: 45 [6400/63553 (10%)]	Loss: 0.000079, KL fake Loss: 0.000437
Classification Train Epoch: 45 [12800/63553 (20%)]	Loss: 0.000375, KL fake Loss: 0.004337
Classification Train Epoch: 45 [19200/63553 (30%)]	Loss: 0.014567, KL fake Loss: 0.001197
Classification Train Epoch: 45 [25600/63553 (40%)]	Loss: 0.066660, KL fake Loss: 0.000176
Classification Train Epoch: 45 [32000/63553 (50%)]	Loss: 0.011782, KL fake Loss: 0.000698
Classification Train Epoch: 45 [38400/63553 (60%)]	Loss: 0.002021, KL fake Loss: 0.001903
Classification Train Epoch: 45 [44800/63553 (70%)]	Loss: 0.010270, KL fake Loss: 0.000617
Classification Train Epoch: 45 [51200/63553 (80%)]	Loss: 0.008230, KL fake Loss: 0.035586
Classification Train Epoch: 45 [57600/63553 (91%)]	Loss: 0.021150, KL fake Loss: 0.000369

Test set: Average loss: 0.5878, Accuracy: 20641/22777 (91%)

Classification Train Epoch: 46 [0/63553 (0%)]	Loss: 0.004433, KL fake Loss: 0.002016
Classification Train Epoch: 46 [6400/63553 (10%)]	Loss: 0.048689, KL fake Loss: 0.000459
Classification Train Epoch: 46 [12800/63553 (20%)]	Loss: 0.023081, KL fake Loss: 0.000242
Classification Train Epoch: 46 [19200/63553 (30%)]	Loss: 0.003873, KL fake Loss: 0.000439
Classification Train Epoch: 46 [25600/63553 (40%)]	Loss: 0.000311, KL fake Loss: 0.002997
Classification Train Epoch: 46 [32000/63553 (50%)]	Loss: 0.006779, KL fake Loss: 0.000584
Classification Train Epoch: 46 [38400/63553 (60%)]	Loss: 0.000189, KL fake Loss: 0.001632
Classification Train Epoch: 46 [44800/63553 (70%)]	Loss: 0.000564, KL fake Loss: 0.002071
Classification Train Epoch: 46 [51200/63553 (80%)]	Loss: 0.041974, KL fake Loss: 0.001672
Classification Train Epoch: 46 [57600/63553 (91%)]	Loss: 0.000287, KL fake Loss: 0.000392

Test set: Average loss: 0.5480, Accuracy: 20919/22777 (92%)

Classification Train Epoch: 47 [0/63553 (0%)]	Loss: 0.000881, KL fake Loss: 0.001323
Classification Train Epoch: 47 [6400/63553 (10%)]	Loss: 0.064205, KL fake Loss: 0.000970
Classification Train Epoch: 47 [12800/63553 (20%)]	Loss: 0.018612, KL fake Loss: 0.003944
Classification Train Epoch: 47 [19200/63553 (30%)]	Loss: 0.005041, KL fake Loss: 0.000290
Classification Train Epoch: 47 [25600/63553 (40%)]	Loss: 0.000171, KL fake Loss: 0.000886
Classification Train Epoch: 47 [32000/63553 (50%)]	Loss: 0.005152, KL fake Loss: 0.000253
Classification Train Epoch: 47 [38400/63553 (60%)]	Loss: 0.001335, KL fake Loss: 0.002322
Classification Train Epoch: 47 [44800/63553 (70%)]	Loss: 0.003389, KL fake Loss: 0.000119
Classification Train Epoch: 47 [51200/63553 (80%)]	Loss: 0.000613, KL fake Loss: 0.003265
Classification Train Epoch: 47 [57600/63553 (91%)]	Loss: 0.012075, KL fake Loss: 0.014556

Test set: Average loss: 0.6256, Accuracy: 20340/22777 (89%)

Classification Train Epoch: 48 [0/63553 (0%)]	Loss: 0.008002, KL fake Loss: 0.000212
Classification Train Epoch: 48 [6400/63553 (10%)]	Loss: 0.000110, KL fake Loss: 0.000964
Classification Train Epoch: 48 [12800/63553 (20%)]	Loss: 0.000198, KL fake Loss: 0.002997
Classification Train Epoch: 48 [19200/63553 (30%)]	Loss: 0.000161, KL fake Loss: 0.000447
Classification Train Epoch: 48 [25600/63553 (40%)]	Loss: 0.008966, KL fake Loss: 0.000433
Classification Train Epoch: 48 [32000/63553 (50%)]	Loss: 0.015711, KL fake Loss: 0.000160
Classification Train Epoch: 48 [38400/63553 (60%)]	Loss: 0.003306, KL fake Loss: 0.000155
Classification Train Epoch: 48 [44800/63553 (70%)]	Loss: 0.005350, KL fake Loss: 0.005993
Classification Train Epoch: 48 [51200/63553 (80%)]	Loss: 0.004342, KL fake Loss: 0.004604
Classification Train Epoch: 48 [57600/63553 (91%)]	Loss: 0.001557, KL fake Loss: 0.000518

Test set: Average loss: 0.6537, Accuracy: 20498/22777 (90%)

Classification Train Epoch: 49 [0/63553 (0%)]	Loss: 0.082256, KL fake Loss: 0.015844
Classification Train Epoch: 49 [6400/63553 (10%)]	Loss: 0.066842, KL fake Loss: 0.000812
Classification Train Epoch: 49 [12800/63553 (20%)]	Loss: 0.006932, KL fake Loss: 0.000238
Classification Train Epoch: 49 [19200/63553 (30%)]	Loss: 0.000772, KL fake Loss: 0.000161
Classification Train Epoch: 49 [25600/63553 (40%)]	Loss: 0.000737, KL fake Loss: 0.000237
Classification Train Epoch: 49 [32000/63553 (50%)]	Loss: 0.026278, KL fake Loss: 0.000180
Classification Train Epoch: 49 [38400/63553 (60%)]	Loss: 0.029122, KL fake Loss: 0.000382
Classification Train Epoch: 49 [44800/63553 (70%)]	Loss: 0.011149, KL fake Loss: 0.003052
Classification Train Epoch: 49 [51200/63553 (80%)]	Loss: 0.010098, KL fake Loss: 0.000808
Classification Train Epoch: 49 [57600/63553 (91%)]	Loss: 0.004134, KL fake Loss: 0.000487

Test set: Average loss: 0.6011, Accuracy: 20518/22777 (90%)

Classification Train Epoch: 50 [0/63553 (0%)]	Loss: 0.002174, KL fake Loss: 0.001215
Classification Train Epoch: 50 [6400/63553 (10%)]	Loss: 0.000255, KL fake Loss: 0.000151
Classification Train Epoch: 50 [12800/63553 (20%)]	Loss: 0.000265, KL fake Loss: 0.000184
Classification Train Epoch: 50 [19200/63553 (30%)]	Loss: 0.000826, KL fake Loss: 0.000812
Classification Train Epoch: 50 [25600/63553 (40%)]	Loss: 0.023507, KL fake Loss: 0.000700
Classification Train Epoch: 50 [32000/63553 (50%)]	Loss: 0.000146, KL fake Loss: 0.000535
Classification Train Epoch: 50 [38400/63553 (60%)]	Loss: 0.012095, KL fake Loss: 0.000111
Classification Train Epoch: 50 [44800/63553 (70%)]	Loss: 0.011932, KL fake Loss: 0.000568
Classification Train Epoch: 50 [51200/63553 (80%)]	Loss: 0.010718, KL fake Loss: 0.000325
Classification Train Epoch: 50 [57600/63553 (91%)]	Loss: 0.000546, KL fake Loss: 0.000101

Test set: Average loss: 0.8776, Accuracy: 19705/22777 (87%)

Classification Train Epoch: 51 [0/63553 (0%)]	Loss: 0.000067, KL fake Loss: 0.012940
Classification Train Epoch: 51 [6400/63553 (10%)]	Loss: 0.015836, KL fake Loss: 0.000353
Classification Train Epoch: 51 [12800/63553 (20%)]	Loss: 0.001190, KL fake Loss: 0.000417
Classification Train Epoch: 51 [19200/63553 (30%)]	Loss: 0.000679, KL fake Loss: 0.000267
Classification Train Epoch: 51 [25600/63553 (40%)]	Loss: 0.007650, KL fake Loss: 0.000122
Classification Train Epoch: 51 [32000/63553 (50%)]	Loss: 0.000402, KL fake Loss: 0.000207
Classification Train Epoch: 51 [38400/63553 (60%)]	Loss: 0.063844, KL fake Loss: 0.000112
Classification Train Epoch: 51 [44800/63553 (70%)]	Loss: 0.002649, KL fake Loss: 0.000635
Classification Train Epoch: 51 [51200/63553 (80%)]	Loss: 0.011560, KL fake Loss: 0.000164
Classification Train Epoch: 51 [57600/63553 (91%)]	Loss: 0.014332, KL fake Loss: 0.000487

Test set: Average loss: 0.5018, Accuracy: 21076/22777 (93%)

Classification Train Epoch: 52 [0/63553 (0%)]	Loss: 0.000374, KL fake Loss: 0.000463
Classification Train Epoch: 52 [6400/63553 (10%)]	Loss: 0.006837, KL fake Loss: 0.000080
Classification Train Epoch: 52 [12800/63553 (20%)]	Loss: 0.000166, KL fake Loss: 0.000290
Classification Train Epoch: 52 [19200/63553 (30%)]	Loss: 0.019145, KL fake Loss: 0.000130
 52%|█████▏    | 52/100 [3:48:59<3:31:20, 264.18s/it] 53%|█████▎    | 53/100 [3:53:23<3:26:56, 264.18s/it] 54%|█████▍    | 54/100 [3:57:47<3:22:32, 264.18s/it] 55%|█████▌    | 55/100 [4:02:12<3:18:08, 264.19s/it] 56%|█████▌    | 56/100 [4:06:36<3:13:44, 264.19s/it] 57%|█████▋    | 57/100 [4:11:00<3:09:20, 264.19s/it] 58%|█████▊    | 58/100 [4:15:24<3:04:56, 264.19s/it] 59%|█████▉    | 59/100 [4:19:48<3:00:31, 264.20s/it]Classification Train Epoch: 52 [25600/63553 (40%)]	Loss: 0.001067, KL fake Loss: 0.001049
Classification Train Epoch: 52 [32000/63553 (50%)]	Loss: 0.000365, KL fake Loss: 0.000382
Classification Train Epoch: 52 [38400/63553 (60%)]	Loss: 0.000736, KL fake Loss: 0.000231
Classification Train Epoch: 52 [44800/63553 (70%)]	Loss: 0.017976, KL fake Loss: 0.000352
Classification Train Epoch: 52 [51200/63553 (80%)]	Loss: 0.003276, KL fake Loss: 0.002502
Classification Train Epoch: 52 [57600/63553 (91%)]	Loss: 0.017665, KL fake Loss: 0.000550

Test set: Average loss: 0.4688, Accuracy: 21182/22777 (93%)

Classification Train Epoch: 53 [0/63553 (0%)]	Loss: 0.002115, KL fake Loss: 0.009506
Classification Train Epoch: 53 [6400/63553 (10%)]	Loss: 0.000103, KL fake Loss: 0.003249
Classification Train Epoch: 53 [12800/63553 (20%)]	Loss: 0.001210, KL fake Loss: 0.000363
Classification Train Epoch: 53 [19200/63553 (30%)]	Loss: 0.004480, KL fake Loss: 0.001232
Classification Train Epoch: 53 [25600/63553 (40%)]	Loss: 0.002109, KL fake Loss: 0.000305
Classification Train Epoch: 53 [32000/63553 (50%)]	Loss: 0.000727, KL fake Loss: 0.000198
Classification Train Epoch: 53 [38400/63553 (60%)]	Loss: 0.001434, KL fake Loss: 0.000786
Classification Train Epoch: 53 [44800/63553 (70%)]	Loss: 0.000758, KL fake Loss: 0.000287
Classification Train Epoch: 53 [51200/63553 (80%)]	Loss: 0.029542, KL fake Loss: 0.000822
Classification Train Epoch: 53 [57600/63553 (91%)]	Loss: 0.016033, KL fake Loss: 0.007540

Test set: Average loss: 0.7547, Accuracy: 20135/22777 (88%)

Classification Train Epoch: 54 [0/63553 (0%)]	Loss: 0.002250, KL fake Loss: 0.009390
Classification Train Epoch: 54 [6400/63553 (10%)]	Loss: 0.002732, KL fake Loss: 0.004369
Classification Train Epoch: 54 [12800/63553 (20%)]	Loss: 0.005350, KL fake Loss: 0.000582
Classification Train Epoch: 54 [19200/63553 (30%)]	Loss: 0.000669, KL fake Loss: 0.018138
Classification Train Epoch: 54 [25600/63553 (40%)]	Loss: 0.000111, KL fake Loss: 0.001203
Classification Train Epoch: 54 [32000/63553 (50%)]	Loss: 0.001055, KL fake Loss: 0.000406
Classification Train Epoch: 54 [38400/63553 (60%)]	Loss: 0.000878, KL fake Loss: 0.000153
Classification Train Epoch: 54 [44800/63553 (70%)]	Loss: 0.011408, KL fake Loss: 0.000884
Classification Train Epoch: 54 [51200/63553 (80%)]	Loss: 0.000328, KL fake Loss: 0.000109
Classification Train Epoch: 54 [57600/63553 (91%)]	Loss: 0.076840, KL fake Loss: 0.003021

Test set: Average loss: 0.7515, Accuracy: 19972/22777 (88%)

Classification Train Epoch: 55 [0/63553 (0%)]	Loss: 0.001493, KL fake Loss: 0.000545
Classification Train Epoch: 55 [6400/63553 (10%)]	Loss: 0.000226, KL fake Loss: 0.001688
Classification Train Epoch: 55 [12800/63553 (20%)]	Loss: 0.002613, KL fake Loss: 0.000121
Classification Train Epoch: 55 [19200/63553 (30%)]	Loss: 0.006688, KL fake Loss: 0.000707
Classification Train Epoch: 55 [25600/63553 (40%)]	Loss: 0.000056, KL fake Loss: 0.001096
Classification Train Epoch: 55 [32000/63553 (50%)]	Loss: 0.018861, KL fake Loss: 0.000133
Classification Train Epoch: 55 [38400/63553 (60%)]	Loss: 0.013975, KL fake Loss: 0.000674
Classification Train Epoch: 55 [44800/63553 (70%)]	Loss: 0.000220, KL fake Loss: 0.000182
Classification Train Epoch: 55 [51200/63553 (80%)]	Loss: 0.000204, KL fake Loss: 0.000182
Classification Train Epoch: 55 [57600/63553 (91%)]	Loss: 0.000343, KL fake Loss: 0.000058

Test set: Average loss: 0.4846, Accuracy: 21035/22777 (92%)

Classification Train Epoch: 56 [0/63553 (0%)]	Loss: 0.000153, KL fake Loss: 0.000082
Classification Train Epoch: 56 [6400/63553 (10%)]	Loss: 0.000413, KL fake Loss: 0.000930
Classification Train Epoch: 56 [12800/63553 (20%)]	Loss: 0.009413, KL fake Loss: 0.000064
Classification Train Epoch: 56 [19200/63553 (30%)]	Loss: 0.006054, KL fake Loss: 0.000741
Classification Train Epoch: 56 [25600/63553 (40%)]	Loss: 0.002913, KL fake Loss: 0.012759
Classification Train Epoch: 56 [32000/63553 (50%)]	Loss: 0.054632, KL fake Loss: 0.000816
Classification Train Epoch: 56 [38400/63553 (60%)]	Loss: 0.006129, KL fake Loss: 0.000227
Classification Train Epoch: 56 [44800/63553 (70%)]	Loss: 0.000479, KL fake Loss: 0.000456
Classification Train Epoch: 56 [51200/63553 (80%)]	Loss: 0.029915, KL fake Loss: 0.001608
Classification Train Epoch: 56 [57600/63553 (91%)]	Loss: 0.007686, KL fake Loss: 0.000147

Test set: Average loss: 0.6423, Accuracy: 20573/22777 (90%)

Classification Train Epoch: 57 [0/63553 (0%)]	Loss: 0.000758, KL fake Loss: 0.038226
Classification Train Epoch: 57 [6400/63553 (10%)]	Loss: 0.007438, KL fake Loss: 0.009494
Classification Train Epoch: 57 [12800/63553 (20%)]	Loss: 0.014729, KL fake Loss: 0.000171
Classification Train Epoch: 57 [19200/63553 (30%)]	Loss: 0.009362, KL fake Loss: 0.002503
Classification Train Epoch: 57 [25600/63553 (40%)]	Loss: 0.003645, KL fake Loss: 0.000092
Classification Train Epoch: 57 [32000/63553 (50%)]	Loss: 0.000326, KL fake Loss: 0.003131
Classification Train Epoch: 57 [38400/63553 (60%)]	Loss: 0.005572, KL fake Loss: 0.000089
Classification Train Epoch: 57 [44800/63553 (70%)]	Loss: 0.024782, KL fake Loss: 0.000681
Classification Train Epoch: 57 [51200/63553 (80%)]	Loss: 0.000193, KL fake Loss: 0.006061
Classification Train Epoch: 57 [57600/63553 (91%)]	Loss: 0.012909, KL fake Loss: 0.000545

Test set: Average loss: 0.5489, Accuracy: 20891/22777 (92%)

Classification Train Epoch: 58 [0/63553 (0%)]	Loss: 0.000239, KL fake Loss: 0.000787
Classification Train Epoch: 58 [6400/63553 (10%)]	Loss: 0.002276, KL fake Loss: 0.000069
Classification Train Epoch: 58 [12800/63553 (20%)]	Loss: 0.001253, KL fake Loss: 0.000266
Classification Train Epoch: 58 [19200/63553 (30%)]	Loss: 0.000279, KL fake Loss: 0.002248
Classification Train Epoch: 58 [25600/63553 (40%)]	Loss: 0.000537, KL fake Loss: 0.000207
Classification Train Epoch: 58 [32000/63553 (50%)]	Loss: 0.007907, KL fake Loss: 0.000273
Classification Train Epoch: 58 [38400/63553 (60%)]	Loss: 0.046670, KL fake Loss: 0.003720
Classification Train Epoch: 58 [44800/63553 (70%)]	Loss: 0.018914, KL fake Loss: 0.000752
Classification Train Epoch: 58 [51200/63553 (80%)]	Loss: 0.003797, KL fake Loss: 0.004361
Classification Train Epoch: 58 [57600/63553 (91%)]	Loss: 0.001429, KL fake Loss: 0.000414

Test set: Average loss: 0.6181, Accuracy: 20638/22777 (91%)

Classification Train Epoch: 59 [0/63553 (0%)]	Loss: 0.000482, KL fake Loss: 0.000421
Classification Train Epoch: 59 [6400/63553 (10%)]	Loss: 0.000277, KL fake Loss: 0.000481
Classification Train Epoch: 59 [12800/63553 (20%)]	Loss: 0.004228, KL fake Loss: 0.016154
Classification Train Epoch: 59 [19200/63553 (30%)]	Loss: 0.000586, KL fake Loss: 0.000243
Classification Train Epoch: 59 [25600/63553 (40%)]	Loss: 0.014046, KL fake Loss: 0.000852
Classification Train Epoch: 59 [32000/63553 (50%)]	Loss: 0.002170, KL fake Loss: 0.000205
Classification Train Epoch: 59 [38400/63553 (60%)]	Loss: 0.039370, KL fake Loss: 0.000458
Classification Train Epoch: 59 [44800/63553 (70%)]	Loss: 0.001776, KL fake Loss: 0.000220
Classification Train Epoch: 59 [51200/63553 (80%)]	Loss: 0.000404, KL fake Loss: 0.000313
Classification Train Epoch: 59 [57600/63553 (91%)]	Loss: 0.000061, KL fake Loss: 0.000170

Test set: Average loss: 0.5770, Accuracy: 20961/22777 (92%)

Classification Train Epoch: 60 [0/63553 (0%)]	Loss: 0.003529, KL fake Loss: 0.000151
Classification Train Epoch: 60 [6400/63553 (10%)]	Loss: 0.002332, KL fake Loss: 0.000297
Classification Train Epoch: 60 [12800/63553 (20%)]	Loss: 0.030849, KL fake Loss: 0.003351
Classification Train Epoch: 60 [19200/63553 (30%)]	Loss: 0.002468, KL fake Loss: 0.000585
Classification Train Epoch: 60 [25600/63553 (40%)]	Loss: 0.000491, KL fake Loss: 0.002625
Classification Train Epoch: 60 [32000/63553 (50%)]	Loss: 0.000896, KL fake Loss: 0.000645
Classification Train Epoch: 60 [38400/63553 (60%)]	Loss: 0.000272, KL fake Loss: 0.000696
Classification Train Epoch: 60 [44800/63553 (70%)]	Loss: 0.000423, KL fake Loss: 0.000614
Classification Train Epoch: 60 [51200/63553 (80%)]	Loss: 0.005023, KL fake Loss: 0.000714
Classification Train Epoch: 60 [57600/63553 (91%)]	Loss: 0.000627, KL fake Loss: 0.000110
 60%|██████    | 60/100 [4:24:13<2:56:08, 264.22s/it] 61%|██████    | 61/100 [4:28:37<2:51:44, 264.21s/it] 62%|██████▏   | 62/100 [4:33:01<2:47:19, 264.20s/it] 63%|██████▎   | 63/100 [4:37:25<2:42:55, 264.20s/it] 64%|██████▍   | 64/100 [4:41:50<2:38:31, 264.20s/it] 65%|██████▌   | 65/100 [4:46:14<2:34:06, 264.20s/it] 66%|██████▌   | 66/100 [4:50:38<2:29:42, 264.20s/it] 67%|██████▋   | 67/100 [4:55:02<2:25:18, 264.19s/it] 68%|██████▊   | 68/100 [4:59:26<2:20:54, 264.19s/it]
Test set: Average loss: 0.5384, Accuracy: 20985/22777 (92%)

Classification Train Epoch: 61 [0/63553 (0%)]	Loss: 0.002197, KL fake Loss: 0.002863
Classification Train Epoch: 61 [6400/63553 (10%)]	Loss: 0.000276, KL fake Loss: 0.000286
Classification Train Epoch: 61 [12800/63553 (20%)]	Loss: 0.000508, KL fake Loss: 0.000672
Classification Train Epoch: 61 [19200/63553 (30%)]	Loss: 0.000937, KL fake Loss: 0.000138
Classification Train Epoch: 61 [25600/63553 (40%)]	Loss: 0.000412, KL fake Loss: 0.000076
Classification Train Epoch: 61 [32000/63553 (50%)]	Loss: 0.000557, KL fake Loss: 0.000213
Classification Train Epoch: 61 [38400/63553 (60%)]	Loss: 0.000057, KL fake Loss: 0.000077
Classification Train Epoch: 61 [44800/63553 (70%)]	Loss: 0.000273, KL fake Loss: 0.000044
Classification Train Epoch: 61 [51200/63553 (80%)]	Loss: 0.000149, KL fake Loss: 0.000234
Classification Train Epoch: 61 [57600/63553 (91%)]	Loss: 0.000169, KL fake Loss: 0.000049

Test set: Average loss: 0.4293, Accuracy: 21331/22777 (94%)

Classification Train Epoch: 62 [0/63553 (0%)]	Loss: 0.000553, KL fake Loss: 0.000334
Classification Train Epoch: 62 [6400/63553 (10%)]	Loss: 0.000061, KL fake Loss: 0.000026
Classification Train Epoch: 62 [12800/63553 (20%)]	Loss: 0.000904, KL fake Loss: 0.000035
Classification Train Epoch: 62 [19200/63553 (30%)]	Loss: 0.000225, KL fake Loss: 0.000049
Classification Train Epoch: 62 [25600/63553 (40%)]	Loss: 0.000025, KL fake Loss: 0.000029
Classification Train Epoch: 62 [32000/63553 (50%)]	Loss: 0.014105, KL fake Loss: 0.000032
Classification Train Epoch: 62 [38400/63553 (60%)]	Loss: 0.000048, KL fake Loss: 0.000070
Classification Train Epoch: 62 [44800/63553 (70%)]	Loss: 0.000687, KL fake Loss: 0.000036
Classification Train Epoch: 62 [51200/63553 (80%)]	Loss: 0.000096, KL fake Loss: 0.000035
Classification Train Epoch: 62 [57600/63553 (91%)]	Loss: 0.000030, KL fake Loss: 0.000034

Test set: Average loss: 0.4478, Accuracy: 21313/22777 (94%)

Classification Train Epoch: 63 [0/63553 (0%)]	Loss: 0.000125, KL fake Loss: 0.000029
Classification Train Epoch: 63 [6400/63553 (10%)]	Loss: 0.000013, KL fake Loss: 0.000037
Classification Train Epoch: 63 [12800/63553 (20%)]	Loss: 0.000109, KL fake Loss: 0.000060
Classification Train Epoch: 63 [19200/63553 (30%)]	Loss: 0.000320, KL fake Loss: 0.000022
Classification Train Epoch: 63 [25600/63553 (40%)]	Loss: 0.000524, KL fake Loss: 0.000035
Classification Train Epoch: 63 [32000/63553 (50%)]	Loss: 0.000315, KL fake Loss: 0.000027
Classification Train Epoch: 63 [38400/63553 (60%)]	Loss: 0.000073, KL fake Loss: 0.000117
Classification Train Epoch: 63 [44800/63553 (70%)]	Loss: 0.000060, KL fake Loss: 0.000020
Classification Train Epoch: 63 [51200/63553 (80%)]	Loss: 0.000158, KL fake Loss: 0.000050
Classification Train Epoch: 63 [57600/63553 (91%)]	Loss: 0.000046, KL fake Loss: 0.000035

Test set: Average loss: 0.4368, Accuracy: 21324/22777 (94%)

Classification Train Epoch: 64 [0/63553 (0%)]	Loss: 0.000104, KL fake Loss: 0.000028
Classification Train Epoch: 64 [6400/63553 (10%)]	Loss: 0.000262, KL fake Loss: 0.000023
Classification Train Epoch: 64 [12800/63553 (20%)]	Loss: 0.000397, KL fake Loss: 0.000027
Classification Train Epoch: 64 [19200/63553 (30%)]	Loss: 0.000134, KL fake Loss: 0.000024
Classification Train Epoch: 64 [25600/63553 (40%)]	Loss: 0.000076, KL fake Loss: 0.000056
Classification Train Epoch: 64 [32000/63553 (50%)]	Loss: 0.001152, KL fake Loss: 0.000021
Classification Train Epoch: 64 [38400/63553 (60%)]	Loss: 0.000234, KL fake Loss: 0.000017
Classification Train Epoch: 64 [44800/63553 (70%)]	Loss: 0.000376, KL fake Loss: 0.000023
Classification Train Epoch: 64 [51200/63553 (80%)]	Loss: 0.000397, KL fake Loss: 0.000021
Classification Train Epoch: 64 [57600/63553 (91%)]	Loss: 0.000276, KL fake Loss: 0.000041

Test set: Average loss: 0.4408, Accuracy: 21313/22777 (94%)

Classification Train Epoch: 65 [0/63553 (0%)]	Loss: 0.000040, KL fake Loss: 0.000027
Classification Train Epoch: 65 [6400/63553 (10%)]	Loss: 0.000047, KL fake Loss: 0.000023
Classification Train Epoch: 65 [12800/63553 (20%)]	Loss: 0.000118, KL fake Loss: 0.000035
Classification Train Epoch: 65 [19200/63553 (30%)]	Loss: 0.000291, KL fake Loss: 0.000021
Classification Train Epoch: 65 [25600/63553 (40%)]	Loss: 0.000263, KL fake Loss: 0.000023
Classification Train Epoch: 65 [32000/63553 (50%)]	Loss: 0.000269, KL fake Loss: 0.000039
Classification Train Epoch: 65 [38400/63553 (60%)]	Loss: 0.000146, KL fake Loss: 0.000030
Classification Train Epoch: 65 [44800/63553 (70%)]	Loss: 0.000509, KL fake Loss: 0.000024
Classification Train Epoch: 65 [51200/63553 (80%)]	Loss: 0.000105, KL fake Loss: 0.000023
Classification Train Epoch: 65 [57600/63553 (91%)]	Loss: 0.000676, KL fake Loss: 0.000023

Test set: Average loss: 0.4473, Accuracy: 21290/22777 (93%)

Classification Train Epoch: 66 [0/63553 (0%)]	Loss: 0.000266, KL fake Loss: 0.000084
Classification Train Epoch: 66 [6400/63553 (10%)]	Loss: 0.000075, KL fake Loss: 0.000025
Classification Train Epoch: 66 [12800/63553 (20%)]	Loss: 0.000054, KL fake Loss: 0.000022
Classification Train Epoch: 66 [19200/63553 (30%)]	Loss: 0.000214, KL fake Loss: 0.000023
Classification Train Epoch: 66 [25600/63553 (40%)]	Loss: 0.000175, KL fake Loss: 0.000020
Classification Train Epoch: 66 [32000/63553 (50%)]	Loss: 0.000038, KL fake Loss: 0.000019
Classification Train Epoch: 66 [38400/63553 (60%)]	Loss: 0.000008, KL fake Loss: 0.000025
Classification Train Epoch: 66 [44800/63553 (70%)]	Loss: 0.000071, KL fake Loss: 0.000059
Classification Train Epoch: 66 [51200/63553 (80%)]	Loss: 0.000142, KL fake Loss: 0.000032
Classification Train Epoch: 66 [57600/63553 (91%)]	Loss: 0.000039, KL fake Loss: 0.000026

Test set: Average loss: 0.4736, Accuracy: 21233/22777 (93%)

Classification Train Epoch: 67 [0/63553 (0%)]	Loss: 0.001418, KL fake Loss: 0.000026
Classification Train Epoch: 67 [6400/63553 (10%)]	Loss: 0.000963, KL fake Loss: 0.000034
Classification Train Epoch: 67 [12800/63553 (20%)]	Loss: 0.000047, KL fake Loss: 0.000029
Classification Train Epoch: 67 [19200/63553 (30%)]	Loss: 0.000285, KL fake Loss: 0.000023
Classification Train Epoch: 67 [25600/63553 (40%)]	Loss: 0.000004, KL fake Loss: 0.000017
Classification Train Epoch: 67 [32000/63553 (50%)]	Loss: 0.000006, KL fake Loss: 0.000021
Classification Train Epoch: 67 [38400/63553 (60%)]	Loss: 0.000225, KL fake Loss: 0.000028
Classification Train Epoch: 67 [44800/63553 (70%)]	Loss: 0.000063, KL fake Loss: 0.000021
Classification Train Epoch: 67 [51200/63553 (80%)]	Loss: 0.000006, KL fake Loss: 0.000023
Classification Train Epoch: 67 [57600/63553 (91%)]	Loss: 0.000010, KL fake Loss: 0.000024

Test set: Average loss: 0.4989, Accuracy: 21092/22777 (93%)

Classification Train Epoch: 68 [0/63553 (0%)]	Loss: 0.000075, KL fake Loss: 0.000024
Classification Train Epoch: 68 [6400/63553 (10%)]	Loss: 0.000219, KL fake Loss: 0.000026
Classification Train Epoch: 68 [12800/63553 (20%)]	Loss: 0.000022, KL fake Loss: 0.000019
Classification Train Epoch: 68 [19200/63553 (30%)]	Loss: 0.000002, KL fake Loss: 0.000019
Classification Train Epoch: 68 [25600/63553 (40%)]	Loss: 0.000018, KL fake Loss: 0.000036
Classification Train Epoch: 68 [32000/63553 (50%)]	Loss: 0.000081, KL fake Loss: 0.000021
Classification Train Epoch: 68 [38400/63553 (60%)]	Loss: 0.000064, KL fake Loss: 0.000037
Classification Train Epoch: 68 [44800/63553 (70%)]	Loss: 0.000030, KL fake Loss: 0.000025
Classification Train Epoch: 68 [51200/63553 (80%)]	Loss: 0.000068, KL fake Loss: 0.000024
Classification Train Epoch: 68 [57600/63553 (91%)]	Loss: 0.000344, KL fake Loss: 0.000026

Test set: Average loss: 0.4835, Accuracy: 21172/22777 (93%)

Classification Train Epoch: 69 [0/63553 (0%)]	Loss: 0.000009, KL fake Loss: 0.000309
Classification Train Epoch: 69 [6400/63553 (10%)]	Loss: 0.000065, KL fake Loss: 0.000044
Classification Train Epoch: 69 [12800/63553 (20%)]	Loss: 0.000624, KL fake Loss: 0.000033
Classification Train Epoch: 69 [19200/63553 (30%)]	Loss: 0.000053, KL fake Loss: 0.000027
Classification Train Epoch: 69 [25600/63553 (40%)]	Loss: 0.000374, KL fake Loss: 0.000031
 69%|██████▉   | 69/100 [5:03:50<2:16:29, 264.19s/it] 70%|███████   | 70/100 [5:08:15<2:12:05, 264.19s/it] 71%|███████   | 71/100 [5:12:39<2:07:41, 264.19s/it] 72%|███████▏  | 72/100 [5:17:03<2:03:17, 264.19s/it] 73%|███████▎  | 73/100 [5:21:27<1:58:53, 264.19s/it] 74%|███████▍  | 74/100 [5:25:51<1:54:28, 264.19s/it] 75%|███████▌  | 75/100 [5:30:16<1:50:04, 264.19s/it] 76%|███████▌  | 76/100 [5:34:40<1:45:40, 264.19s/it] 77%|███████▋  | 77/100 [5:39:04<1:41:16, 264.19s/it]Classification Train Epoch: 69 [32000/63553 (50%)]	Loss: 0.000167, KL fake Loss: 0.000026
Classification Train Epoch: 69 [38400/63553 (60%)]	Loss: 0.000014, KL fake Loss: 0.000028
Classification Train Epoch: 69 [44800/63553 (70%)]	Loss: 0.000863, KL fake Loss: 0.000025
Classification Train Epoch: 69 [51200/63553 (80%)]	Loss: 0.000017, KL fake Loss: 0.000028
Classification Train Epoch: 69 [57600/63553 (91%)]	Loss: 0.000009, KL fake Loss: 0.000025

Test set: Average loss: 0.4450, Accuracy: 21265/22777 (93%)

Classification Train Epoch: 70 [0/63553 (0%)]	Loss: 0.000059, KL fake Loss: 0.000028
Classification Train Epoch: 70 [6400/63553 (10%)]	Loss: 0.000005, KL fake Loss: 0.000025
Classification Train Epoch: 70 [12800/63553 (20%)]	Loss: 0.000028, KL fake Loss: 0.000022
Classification Train Epoch: 70 [19200/63553 (30%)]	Loss: 0.000081, KL fake Loss: 0.000023
Classification Train Epoch: 70 [25600/63553 (40%)]	Loss: 0.000006, KL fake Loss: 0.000029
Classification Train Epoch: 70 [32000/63553 (50%)]	Loss: 0.000020, KL fake Loss: 0.000024
Classification Train Epoch: 70 [38400/63553 (60%)]	Loss: 0.000038, KL fake Loss: 0.000027
Classification Train Epoch: 70 [44800/63553 (70%)]	Loss: 0.000008, KL fake Loss: 0.000023
Classification Train Epoch: 70 [51200/63553 (80%)]	Loss: 0.000130, KL fake Loss: 0.000025
Classification Train Epoch: 70 [57600/63553 (91%)]	Loss: 0.000135, KL fake Loss: 0.000021

Test set: Average loss: 0.4801, Accuracy: 21291/22777 (93%)

Classification Train Epoch: 71 [0/63553 (0%)]	Loss: 0.000002, KL fake Loss: 0.000420
Classification Train Epoch: 71 [6400/63553 (10%)]	Loss: 0.000008, KL fake Loss: 0.000050
Classification Train Epoch: 71 [12800/63553 (20%)]	Loss: 0.000181, KL fake Loss: 0.000036
Classification Train Epoch: 71 [19200/63553 (30%)]	Loss: 0.000047, KL fake Loss: 0.000035
Classification Train Epoch: 71 [25600/63553 (40%)]	Loss: 0.000095, KL fake Loss: 0.000030
Classification Train Epoch: 71 [32000/63553 (50%)]	Loss: 0.000011, KL fake Loss: 0.000029
Classification Train Epoch: 71 [38400/63553 (60%)]	Loss: 0.000278, KL fake Loss: 0.000030
Classification Train Epoch: 71 [44800/63553 (70%)]	Loss: 0.000044, KL fake Loss: 0.000026
Classification Train Epoch: 71 [51200/63553 (80%)]	Loss: 0.000015, KL fake Loss: 0.000026
Classification Train Epoch: 71 [57600/63553 (91%)]	Loss: 0.000039, KL fake Loss: 0.000022

Test set: Average loss: 0.5102, Accuracy: 21189/22777 (93%)

Classification Train Epoch: 72 [0/63553 (0%)]	Loss: 0.000087, KL fake Loss: 0.000025
Classification Train Epoch: 72 [6400/63553 (10%)]	Loss: 0.000063, KL fake Loss: 0.000026
Classification Train Epoch: 72 [12800/63553 (20%)]	Loss: 0.000399, KL fake Loss: 0.000025
Classification Train Epoch: 72 [19200/63553 (30%)]	Loss: 0.000013, KL fake Loss: 0.000024
Classification Train Epoch: 72 [25600/63553 (40%)]	Loss: 0.000206, KL fake Loss: 0.000026
Classification Train Epoch: 72 [32000/63553 (50%)]	Loss: 0.000034, KL fake Loss: 0.000025
Classification Train Epoch: 72 [38400/63553 (60%)]	Loss: 0.000056, KL fake Loss: 0.000024
Classification Train Epoch: 72 [44800/63553 (70%)]	Loss: 0.000017, KL fake Loss: 0.000020
Classification Train Epoch: 72 [51200/63553 (80%)]	Loss: 0.000040, KL fake Loss: 0.000025
Classification Train Epoch: 72 [57600/63553 (91%)]	Loss: 0.000018, KL fake Loss: 0.000023

Test set: Average loss: 0.4977, Accuracy: 21241/22777 (93%)

Classification Train Epoch: 73 [0/63553 (0%)]	Loss: 0.000304, KL fake Loss: 0.000027
Classification Train Epoch: 73 [6400/63553 (10%)]	Loss: 0.000075, KL fake Loss: 0.000021
Classification Train Epoch: 73 [12800/63553 (20%)]	Loss: 0.000050, KL fake Loss: 0.000024
Classification Train Epoch: 73 [19200/63553 (30%)]	Loss: 0.000007, KL fake Loss: 0.000028
Classification Train Epoch: 73 [25600/63553 (40%)]	Loss: 0.000008, KL fake Loss: 0.000019
Classification Train Epoch: 73 [32000/63553 (50%)]	Loss: 0.000005, KL fake Loss: 0.000021
Classification Train Epoch: 73 [38400/63553 (60%)]	Loss: 0.000104, KL fake Loss: 0.000021
Classification Train Epoch: 73 [44800/63553 (70%)]	Loss: 0.000057, KL fake Loss: 0.000021
Classification Train Epoch: 73 [51200/63553 (80%)]	Loss: 0.000031, KL fake Loss: 0.000022
Classification Train Epoch: 73 [57600/63553 (91%)]	Loss: 0.000010, KL fake Loss: 0.000023

Test set: Average loss: 0.4942, Accuracy: 21226/22777 (93%)

Classification Train Epoch: 74 [0/63553 (0%)]	Loss: 0.000007, KL fake Loss: 0.000031
Classification Train Epoch: 74 [6400/63553 (10%)]	Loss: 0.000084, KL fake Loss: 0.000022
Classification Train Epoch: 74 [12800/63553 (20%)]	Loss: 0.000028, KL fake Loss: 0.000024
Classification Train Epoch: 74 [19200/63553 (30%)]	Loss: 0.000009, KL fake Loss: 0.000021
Classification Train Epoch: 74 [25600/63553 (40%)]	Loss: 0.000003, KL fake Loss: 0.000021
Classification Train Epoch: 74 [32000/63553 (50%)]	Loss: 0.000037, KL fake Loss: 0.000020
Classification Train Epoch: 74 [38400/63553 (60%)]	Loss: 0.000019, KL fake Loss: 0.000017
Classification Train Epoch: 74 [44800/63553 (70%)]	Loss: 0.000019, KL fake Loss: 0.000019
Classification Train Epoch: 74 [51200/63553 (80%)]	Loss: 0.000177, KL fake Loss: 0.000021
Classification Train Epoch: 74 [57600/63553 (91%)]	Loss: 0.000078, KL fake Loss: 0.000018

Test set: Average loss: 0.5637, Accuracy: 21073/22777 (93%)

Classification Train Epoch: 75 [0/63553 (0%)]	Loss: 0.000007, KL fake Loss: 0.000023
Classification Train Epoch: 75 [6400/63553 (10%)]	Loss: 0.000027, KL fake Loss: 0.000020
Classification Train Epoch: 75 [12800/63553 (20%)]	Loss: 0.000006, KL fake Loss: 0.000019
Classification Train Epoch: 75 [19200/63553 (30%)]	Loss: 0.000015, KL fake Loss: 0.000019
Classification Train Epoch: 75 [25600/63553 (40%)]	Loss: 0.000152, KL fake Loss: 0.000016
Classification Train Epoch: 75 [32000/63553 (50%)]	Loss: 0.000024, KL fake Loss: 0.000015
Classification Train Epoch: 75 [38400/63553 (60%)]	Loss: 0.000008, KL fake Loss: 0.000015
Classification Train Epoch: 75 [44800/63553 (70%)]	Loss: 0.000005, KL fake Loss: 0.000017
Classification Train Epoch: 75 [51200/63553 (80%)]	Loss: 0.000031, KL fake Loss: 0.000018
Classification Train Epoch: 75 [57600/63553 (91%)]	Loss: 0.000026, KL fake Loss: 0.000018

Test set: Average loss: 0.4714, Accuracy: 21372/22777 (94%)

Classification Train Epoch: 76 [0/63553 (0%)]	Loss: 0.000001, KL fake Loss: 0.000034
Classification Train Epoch: 76 [6400/63553 (10%)]	Loss: 0.000035, KL fake Loss: 0.000020
Classification Train Epoch: 76 [12800/63553 (20%)]	Loss: 0.000094, KL fake Loss: 0.000022
Classification Train Epoch: 76 [19200/63553 (30%)]	Loss: 0.000028, KL fake Loss: 0.000018
Classification Train Epoch: 76 [25600/63553 (40%)]	Loss: 0.000011, KL fake Loss: 0.000018
Classification Train Epoch: 76 [32000/63553 (50%)]	Loss: 0.000060, KL fake Loss: 0.000016
Classification Train Epoch: 76 [38400/63553 (60%)]	Loss: 0.000009, KL fake Loss: 0.000019
Classification Train Epoch: 76 [44800/63553 (70%)]	Loss: 0.000015, KL fake Loss: 0.000017
Classification Train Epoch: 76 [51200/63553 (80%)]	Loss: 0.000026, KL fake Loss: 0.000016
Classification Train Epoch: 76 [57600/63553 (91%)]	Loss: 0.000013, KL fake Loss: 0.000055

Test set: Average loss: 0.5210, Accuracy: 21199/22777 (93%)

Classification Train Epoch: 77 [0/63553 (0%)]	Loss: 0.000003, KL fake Loss: 0.000030
Classification Train Epoch: 77 [6400/63553 (10%)]	Loss: 0.000001, KL fake Loss: 0.000021
Classification Train Epoch: 77 [12800/63553 (20%)]	Loss: 0.000048, KL fake Loss: 0.000019
Classification Train Epoch: 77 [19200/63553 (30%)]	Loss: 0.000006, KL fake Loss: 0.000018
Classification Train Epoch: 77 [25600/63553 (40%)]	Loss: 0.000024, KL fake Loss: 0.000015
Classification Train Epoch: 77 [32000/63553 (50%)]	Loss: 0.000045, KL fake Loss: 0.000014
Classification Train Epoch: 77 [38400/63553 (60%)]	Loss: 0.000034, KL fake Loss: 0.000016
Classification Train Epoch: 77 [44800/63553 (70%)]	Loss: 0.000034, KL fake Loss: 0.000016
Classification Train Epoch: 77 [51200/63553 (80%)]	Loss: 0.000046, KL fake Loss: 0.000016
Classification Train Epoch: 77 [57600/63553 (91%)]	Loss: 0.000018, KL fake Loss: 0.000017

Test set: Average loss: 0.5063, Accuracy: 21252/22777 (93%)

 78%|███████▊  | 78/100 [5:43:28<1:36:52, 264.19s/it] 79%|███████▉  | 79/100 [5:47:52<1:32:28, 264.19s/it] 80%|████████  | 80/100 [5:52:17<1:28:04, 264.23s/it] 81%|████████  | 81/100 [5:56:41<1:23:40, 264.22s/it] 82%|████████▏ | 82/100 [6:01:05<1:19:15, 264.21s/it] 83%|████████▎ | 83/100 [6:05:29<1:14:51, 264.20s/it] 84%|████████▍ | 84/100 [6:09:53<1:10:27, 264.19s/it] 85%|████████▌ | 85/100 [6:14:18<1:06:02, 264.19s/it]Classification Train Epoch: 78 [0/63553 (0%)]	Loss: 0.000002, KL fake Loss: 0.000074
Classification Train Epoch: 78 [6400/63553 (10%)]	Loss: 0.000014, KL fake Loss: 0.000029
Classification Train Epoch: 78 [12800/63553 (20%)]	Loss: 0.001339, KL fake Loss: 0.000028
Classification Train Epoch: 78 [19200/63553 (30%)]	Loss: 0.000034, KL fake Loss: 0.000028
Classification Train Epoch: 78 [25600/63553 (40%)]	Loss: 0.000011, KL fake Loss: 0.000016
Classification Train Epoch: 78 [32000/63553 (50%)]	Loss: 0.000034, KL fake Loss: 0.000018
Classification Train Epoch: 78 [38400/63553 (60%)]	Loss: 0.000005, KL fake Loss: 0.000026
Classification Train Epoch: 78 [44800/63553 (70%)]	Loss: 0.000189, KL fake Loss: 0.000016
Classification Train Epoch: 78 [51200/63553 (80%)]	Loss: 0.000009, KL fake Loss: 0.000015
Classification Train Epoch: 78 [57600/63553 (91%)]	Loss: 0.000009, KL fake Loss: 0.000015

Test set: Average loss: 0.5153, Accuracy: 21136/22777 (93%)

Classification Train Epoch: 79 [0/63553 (0%)]	Loss: 0.000015, KL fake Loss: 0.000035
Classification Train Epoch: 79 [6400/63553 (10%)]	Loss: 0.000002, KL fake Loss: 0.000014
Classification Train Epoch: 79 [12800/63553 (20%)]	Loss: 0.000002, KL fake Loss: 0.000016
Classification Train Epoch: 79 [19200/63553 (30%)]	Loss: 0.000029, KL fake Loss: 0.000014
Classification Train Epoch: 79 [25600/63553 (40%)]	Loss: 0.000049, KL fake Loss: 0.000015
Classification Train Epoch: 79 [32000/63553 (50%)]	Loss: 0.000008, KL fake Loss: 0.000022
Classification Train Epoch: 79 [38400/63553 (60%)]	Loss: 0.000020, KL fake Loss: 0.000012
Classification Train Epoch: 79 [44800/63553 (70%)]	Loss: 0.000011, KL fake Loss: 0.000015
Classification Train Epoch: 79 [51200/63553 (80%)]	Loss: 0.000031, KL fake Loss: 0.000013
Classification Train Epoch: 79 [57600/63553 (91%)]	Loss: 0.000083, KL fake Loss: 0.000016

Test set: Average loss: 0.5202, Accuracy: 21150/22777 (93%)

Classification Train Epoch: 80 [0/63553 (0%)]	Loss: 0.000017, KL fake Loss: 0.000016
Classification Train Epoch: 80 [6400/63553 (10%)]	Loss: 0.000563, KL fake Loss: 0.000015
Classification Train Epoch: 80 [12800/63553 (20%)]	Loss: 0.000020, KL fake Loss: 0.000018
Classification Train Epoch: 80 [19200/63553 (30%)]	Loss: 0.000007, KL fake Loss: 0.000013
Classification Train Epoch: 80 [25600/63553 (40%)]	Loss: 0.000005, KL fake Loss: 0.000019
Classification Train Epoch: 80 [32000/63553 (50%)]	Loss: 0.000008, KL fake Loss: 0.000016
Classification Train Epoch: 80 [38400/63553 (60%)]	Loss: 0.000005, KL fake Loss: 0.000021
Classification Train Epoch: 80 [44800/63553 (70%)]	Loss: 0.000004, KL fake Loss: 0.000018
Classification Train Epoch: 80 [51200/63553 (80%)]	Loss: 0.000015, KL fake Loss: 0.000017
Classification Train Epoch: 80 [57600/63553 (91%)]	Loss: 0.000045, KL fake Loss: 0.000017

Test set: Average loss: 0.4961, Accuracy: 21282/22777 (93%)

Classification Train Epoch: 81 [0/63553 (0%)]	Loss: 0.000013, KL fake Loss: 0.000036
Classification Train Epoch: 81 [6400/63553 (10%)]	Loss: 0.000000, KL fake Loss: 0.000014
Classification Train Epoch: 81 [12800/63553 (20%)]	Loss: 0.000052, KL fake Loss: 0.000012
Classification Train Epoch: 81 [19200/63553 (30%)]	Loss: 0.000037, KL fake Loss: 0.000012
Classification Train Epoch: 81 [25600/63553 (40%)]	Loss: 0.000047, KL fake Loss: 0.000013
Classification Train Epoch: 81 [32000/63553 (50%)]	Loss: 0.000028, KL fake Loss: 0.000013
Classification Train Epoch: 81 [38400/63553 (60%)]	Loss: 0.000028, KL fake Loss: 0.000013
Classification Train Epoch: 81 [44800/63553 (70%)]	Loss: 0.000036, KL fake Loss: 0.000014
Classification Train Epoch: 81 [51200/63553 (80%)]	Loss: 0.000010, KL fake Loss: 0.000012
Classification Train Epoch: 81 [57600/63553 (91%)]	Loss: 0.000005, KL fake Loss: 0.000011

Test set: Average loss: 0.4820, Accuracy: 21298/22777 (94%)

Classification Train Epoch: 82 [0/63553 (0%)]	Loss: 0.000004, KL fake Loss: 0.000040
Classification Train Epoch: 82 [6400/63553 (10%)]	Loss: 0.000032, KL fake Loss: 0.000012
Classification Train Epoch: 82 [12800/63553 (20%)]	Loss: 0.000039, KL fake Loss: 0.000013
Classification Train Epoch: 82 [19200/63553 (30%)]	Loss: 0.000005, KL fake Loss: 0.000010
Classification Train Epoch: 82 [25600/63553 (40%)]	Loss: 0.000004, KL fake Loss: 0.000012
Classification Train Epoch: 82 [32000/63553 (50%)]	Loss: 0.000351, KL fake Loss: 0.000013
Classification Train Epoch: 82 [38400/63553 (60%)]	Loss: 0.000052, KL fake Loss: 0.000011
Classification Train Epoch: 82 [44800/63553 (70%)]	Loss: 0.000002, KL fake Loss: 0.000013
Classification Train Epoch: 82 [51200/63553 (80%)]	Loss: 0.000002, KL fake Loss: 0.000014
Classification Train Epoch: 82 [57600/63553 (91%)]	Loss: 0.000010, KL fake Loss: 0.000017

Test set: Average loss: 0.4995, Accuracy: 21283/22777 (93%)

Classification Train Epoch: 83 [0/63553 (0%)]	Loss: 0.000027, KL fake Loss: 0.000089
Classification Train Epoch: 83 [6400/63553 (10%)]	Loss: 0.000098, KL fake Loss: 0.000022
Classification Train Epoch: 83 [12800/63553 (20%)]	Loss: 0.000009, KL fake Loss: 0.000018
Classification Train Epoch: 83 [19200/63553 (30%)]	Loss: 0.000003, KL fake Loss: 0.000026
Classification Train Epoch: 83 [25600/63553 (40%)]	Loss: 0.000027, KL fake Loss: 0.000017
Classification Train Epoch: 83 [32000/63553 (50%)]	Loss: 0.000014, KL fake Loss: 0.000013
Classification Train Epoch: 83 [38400/63553 (60%)]	Loss: 0.000009, KL fake Loss: 0.000021
Classification Train Epoch: 83 [44800/63553 (70%)]	Loss: 0.000003, KL fake Loss: 0.000021
Classification Train Epoch: 83 [51200/63553 (80%)]	Loss: 0.000011, KL fake Loss: 0.000015
Classification Train Epoch: 83 [57600/63553 (91%)]	Loss: 0.000208, KL fake Loss: 0.000012

Test set: Average loss: 0.5016, Accuracy: 21249/22777 (93%)

Classification Train Epoch: 84 [0/63553 (0%)]	Loss: 0.000068, KL fake Loss: 0.000055
Classification Train Epoch: 84 [6400/63553 (10%)]	Loss: 0.000003, KL fake Loss: 0.000019
Classification Train Epoch: 84 [12800/63553 (20%)]	Loss: 0.000115, KL fake Loss: 0.000016
Classification Train Epoch: 84 [19200/63553 (30%)]	Loss: 0.000035, KL fake Loss: 0.000019
Classification Train Epoch: 84 [25600/63553 (40%)]	Loss: 0.000007, KL fake Loss: 0.000018
Classification Train Epoch: 84 [32000/63553 (50%)]	Loss: 0.000007, KL fake Loss: 0.000015
Classification Train Epoch: 84 [38400/63553 (60%)]	Loss: 0.000017, KL fake Loss: 0.000021
Classification Train Epoch: 84 [44800/63553 (70%)]	Loss: 0.000006, KL fake Loss: 0.000017
Classification Train Epoch: 84 [51200/63553 (80%)]	Loss: 0.000047, KL fake Loss: 0.000017
Classification Train Epoch: 84 [57600/63553 (91%)]	Loss: 0.000012, KL fake Loss: 0.000019

Test set: Average loss: 0.4576, Accuracy: 21442/22777 (94%)

Classification Train Epoch: 85 [0/63553 (0%)]	Loss: 0.000007, KL fake Loss: 0.000028
Classification Train Epoch: 85 [6400/63553 (10%)]	Loss: 0.000007, KL fake Loss: 0.000022
Classification Train Epoch: 85 [12800/63553 (20%)]	Loss: 0.000017, KL fake Loss: 0.000019
Classification Train Epoch: 85 [19200/63553 (30%)]	Loss: 0.000023, KL fake Loss: 0.000020
Classification Train Epoch: 85 [25600/63553 (40%)]	Loss: 0.000017, KL fake Loss: 0.000015
Classification Train Epoch: 85 [32000/63553 (50%)]	Loss: 0.000003, KL fake Loss: 0.000018
Classification Train Epoch: 85 [38400/63553 (60%)]	Loss: 0.000004, KL fake Loss: 0.000020
Classification Train Epoch: 85 [44800/63553 (70%)]	Loss: 0.000002, KL fake Loss: 0.000017
Classification Train Epoch: 85 [51200/63553 (80%)]	Loss: 0.000057, KL fake Loss: 0.000015
Classification Train Epoch: 85 [57600/63553 (91%)]	Loss: 0.000005, KL fake Loss: 0.000015

Test set: Average loss: 0.5571, Accuracy: 21131/22777 (93%)

Classification Train Epoch: 86 [0/63553 (0%)]	Loss: 0.000007, KL fake Loss: 0.000054
Classification Train Epoch: 86 [6400/63553 (10%)]	Loss: 0.000020, KL fake Loss: 0.000021
Classification Train Epoch: 86 [12800/63553 (20%)]	Loss: 0.000008, KL fake Loss: 0.000023
Classification Train Epoch: 86 [19200/63553 (30%)]	Loss: 0.000009, KL fake Loss: 0.000018
Classification Train Epoch: 86 [25600/63553 (40%)]	Loss: 0.000007, KL fake Loss: 0.000019
Classification Train Epoch: 86 [32000/63553 (50%)]	Loss: 0.000001, KL fake Loss: 0.000016
 86%|████████▌ | 86/100 [6:18:42<1:01:38, 264.19s/it] 87%|████████▋ | 87/100 [6:23:06<57:14, 264.19s/it]   88%|████████▊ | 88/100 [6:27:30<52:50, 264.19s/it] 89%|████████▉ | 89/100 [6:31:54<48:26, 264.19s/it] 90%|█████████ | 90/100 [6:36:19<44:01, 264.19s/it] 91%|█████████ | 91/100 [6:40:43<39:37, 264.19s/it] 92%|█████████▏| 92/100 [6:45:07<35:13, 264.18s/it] 93%|█████████▎| 93/100 [6:49:31<30:49, 264.18s/it] 94%|█████████▍| 94/100 [6:53:55<26:25, 264.18s/it]Classification Train Epoch: 86 [38400/63553 (60%)]	Loss: 0.000004, KL fake Loss: 0.000020
Classification Train Epoch: 86 [44800/63553 (70%)]	Loss: 0.000004, KL fake Loss: 0.000017
Classification Train Epoch: 86 [51200/63553 (80%)]	Loss: 0.000007, KL fake Loss: 0.000016
Classification Train Epoch: 86 [57600/63553 (91%)]	Loss: 0.000021, KL fake Loss: 0.000015

Test set: Average loss: 0.5384, Accuracy: 21232/22777 (93%)

Classification Train Epoch: 87 [0/63553 (0%)]	Loss: 0.000026, KL fake Loss: 0.000077
Classification Train Epoch: 87 [6400/63553 (10%)]	Loss: 0.000052, KL fake Loss: 0.000026
Classification Train Epoch: 87 [12800/63553 (20%)]	Loss: 0.000001, KL fake Loss: 0.000021
Classification Train Epoch: 87 [19200/63553 (30%)]	Loss: 0.000004, KL fake Loss: 0.000020
Classification Train Epoch: 87 [25600/63553 (40%)]	Loss: 0.000006, KL fake Loss: 0.000024
Classification Train Epoch: 87 [32000/63553 (50%)]	Loss: 0.000008, KL fake Loss: 0.000017
Classification Train Epoch: 87 [38400/63553 (60%)]	Loss: 0.000006, KL fake Loss: 0.000149
Classification Train Epoch: 87 [44800/63553 (70%)]	Loss: 0.000013, KL fake Loss: 0.000025
Classification Train Epoch: 87 [51200/63553 (80%)]	Loss: 0.000011, KL fake Loss: 0.000032
Classification Train Epoch: 87 [57600/63553 (91%)]	Loss: 0.000008, KL fake Loss: 0.000021

Test set: Average loss: 0.5017, Accuracy: 21325/22777 (94%)

Classification Train Epoch: 88 [0/63553 (0%)]	Loss: 0.000002, KL fake Loss: 0.000638
Classification Train Epoch: 88 [6400/63553 (10%)]	Loss: 0.000004, KL fake Loss: 0.000045
Classification Train Epoch: 88 [12800/63553 (20%)]	Loss: 0.000009, KL fake Loss: 0.000024
Classification Train Epoch: 88 [19200/63553 (30%)]	Loss: 0.000009, KL fake Loss: 0.000019
Classification Train Epoch: 88 [25600/63553 (40%)]	Loss: 0.000014, KL fake Loss: 0.000022
Classification Train Epoch: 88 [32000/63553 (50%)]	Loss: 0.000002, KL fake Loss: 0.000019
Classification Train Epoch: 88 [38400/63553 (60%)]	Loss: 0.000012, KL fake Loss: 0.000017
Classification Train Epoch: 88 [44800/63553 (70%)]	Loss: 0.000013, KL fake Loss: 0.000016
Classification Train Epoch: 88 [51200/63553 (80%)]	Loss: 0.000009, KL fake Loss: 0.000014
Classification Train Epoch: 88 [57600/63553 (91%)]	Loss: 0.000003, KL fake Loss: 0.000016

Test set: Average loss: 0.5616, Accuracy: 21056/22777 (92%)

Classification Train Epoch: 89 [0/63553 (0%)]	Loss: 0.000104, KL fake Loss: 0.000206
Classification Train Epoch: 89 [6400/63553 (10%)]	Loss: 0.000001, KL fake Loss: 0.000019
Classification Train Epoch: 89 [12800/63553 (20%)]	Loss: 0.000000, KL fake Loss: 0.000015
Classification Train Epoch: 89 [19200/63553 (30%)]	Loss: 0.000000, KL fake Loss: 0.000017
Classification Train Epoch: 89 [25600/63553 (40%)]	Loss: 0.000002, KL fake Loss: 0.000027
Classification Train Epoch: 89 [32000/63553 (50%)]	Loss: 0.000007, KL fake Loss: 0.000021
Classification Train Epoch: 89 [38400/63553 (60%)]	Loss: 0.000012, KL fake Loss: 0.000021
Classification Train Epoch: 89 [44800/63553 (70%)]	Loss: 0.000006, KL fake Loss: 0.000015
Classification Train Epoch: 89 [51200/63553 (80%)]	Loss: 0.000003, KL fake Loss: 0.000014
Classification Train Epoch: 89 [57600/63553 (91%)]	Loss: 0.000051, KL fake Loss: 0.000016

Test set: Average loss: 0.5874, Accuracy: 21021/22777 (92%)

Classification Train Epoch: 90 [0/63553 (0%)]	Loss: 0.000080, KL fake Loss: 0.000312
Classification Train Epoch: 90 [6400/63553 (10%)]	Loss: 0.000002, KL fake Loss: 0.000019
Classification Train Epoch: 90 [12800/63553 (20%)]	Loss: 0.000013, KL fake Loss: 0.000020
Classification Train Epoch: 90 [19200/63553 (30%)]	Loss: 0.000004, KL fake Loss: 0.000018
Classification Train Epoch: 90 [25600/63553 (40%)]	Loss: 0.000009, KL fake Loss: 0.000019
Classification Train Epoch: 90 [32000/63553 (50%)]	Loss: 0.000004, KL fake Loss: 0.000014
Classification Train Epoch: 90 [38400/63553 (60%)]	Loss: 0.000020, KL fake Loss: 0.000015
Classification Train Epoch: 90 [44800/63553 (70%)]	Loss: 0.000003, KL fake Loss: 0.000017
Classification Train Epoch: 90 [51200/63553 (80%)]	Loss: 0.000022, KL fake Loss: 0.000024
Classification Train Epoch: 90 [57600/63553 (91%)]	Loss: 0.000000, KL fake Loss: 0.000023

Test set: Average loss: 0.5675, Accuracy: 21125/22777 (93%)

Classification Train Epoch: 91 [0/63553 (0%)]	Loss: 0.000016, KL fake Loss: 0.000059
Classification Train Epoch: 91 [6400/63553 (10%)]	Loss: 0.000034, KL fake Loss: 0.000017
Classification Train Epoch: 91 [12800/63553 (20%)]	Loss: 0.000000, KL fake Loss: 0.000017
Classification Train Epoch: 91 [19200/63553 (30%)]	Loss: 0.000036, KL fake Loss: 0.000014
Classification Train Epoch: 91 [25600/63553 (40%)]	Loss: 0.000003, KL fake Loss: 0.000016
Classification Train Epoch: 91 [32000/63553 (50%)]	Loss: 0.000001, KL fake Loss: 0.000020
Classification Train Epoch: 91 [38400/63553 (60%)]	Loss: 0.000006, KL fake Loss: 0.000017
Classification Train Epoch: 91 [44800/63553 (70%)]	Loss: 0.000111, KL fake Loss: 0.000018
Classification Train Epoch: 91 [51200/63553 (80%)]	Loss: 0.000036, KL fake Loss: 0.000017
Classification Train Epoch: 91 [57600/63553 (91%)]	Loss: 0.000008, KL fake Loss: 0.000017

Test set: Average loss: 0.5205, Accuracy: 21243/22777 (93%)

Classification Train Epoch: 92 [0/63553 (0%)]	Loss: 0.000006, KL fake Loss: 0.000168
Classification Train Epoch: 92 [6400/63553 (10%)]	Loss: 0.000008, KL fake Loss: 0.000019
Classification Train Epoch: 92 [12800/63553 (20%)]	Loss: 0.000060, KL fake Loss: 0.000018
Classification Train Epoch: 92 [19200/63553 (30%)]	Loss: 0.000021, KL fake Loss: 0.000030
Classification Train Epoch: 92 [25600/63553 (40%)]	Loss: 0.000000, KL fake Loss: 0.000017
Classification Train Epoch: 92 [32000/63553 (50%)]	Loss: 0.000004, KL fake Loss: 0.000016
Classification Train Epoch: 92 [38400/63553 (60%)]	Loss: 0.000027, KL fake Loss: 0.000017
Classification Train Epoch: 92 [44800/63553 (70%)]	Loss: 0.000007, KL fake Loss: 0.000016
Classification Train Epoch: 92 [51200/63553 (80%)]	Loss: 0.000002, KL fake Loss: 0.000017
Classification Train Epoch: 92 [57600/63553 (91%)]	Loss: 0.000004, KL fake Loss: 0.000014

Test set: Average loss: 0.5833, Accuracy: 21030/22777 (92%)

Classification Train Epoch: 93 [0/63553 (0%)]	Loss: 0.000006, KL fake Loss: 0.000039
Classification Train Epoch: 93 [6400/63553 (10%)]	Loss: 0.000000, KL fake Loss: 0.000017
Classification Train Epoch: 93 [12800/63553 (20%)]	Loss: 0.000002, KL fake Loss: 0.000019
Classification Train Epoch: 93 [19200/63553 (30%)]	Loss: 0.000004, KL fake Loss: 0.000014
Classification Train Epoch: 93 [25600/63553 (40%)]	Loss: 0.000018, KL fake Loss: 0.000019
Classification Train Epoch: 93 [32000/63553 (50%)]	Loss: 0.000002, KL fake Loss: 0.000015
Classification Train Epoch: 93 [38400/63553 (60%)]	Loss: 0.000007, KL fake Loss: 0.000015
Classification Train Epoch: 93 [44800/63553 (70%)]	Loss: 0.000001, KL fake Loss: 0.000014
Classification Train Epoch: 93 [51200/63553 (80%)]	Loss: 0.000017, KL fake Loss: 0.000015
Classification Train Epoch: 93 [57600/63553 (91%)]	Loss: 0.000004, KL fake Loss: 0.000013

Test set: Average loss: 0.5709, Accuracy: 21065/22777 (92%)

Classification Train Epoch: 94 [0/63553 (0%)]	Loss: 0.000006, KL fake Loss: 0.000033
Classification Train Epoch: 94 [6400/63553 (10%)]	Loss: 0.000005, KL fake Loss: 0.000017
Classification Train Epoch: 94 [12800/63553 (20%)]	Loss: 0.000003, KL fake Loss: 0.000015
Classification Train Epoch: 94 [19200/63553 (30%)]	Loss: 0.000000, KL fake Loss: 0.000017
Classification Train Epoch: 94 [25600/63553 (40%)]	Loss: 0.000004, KL fake Loss: 0.000017
Classification Train Epoch: 94 [32000/63553 (50%)]	Loss: 0.000001, KL fake Loss: 0.000014
Classification Train Epoch: 94 [38400/63553 (60%)]	Loss: 0.000009, KL fake Loss: 0.000013
Classification Train Epoch: 94 [44800/63553 (70%)]	Loss: 0.000006, KL fake Loss: 0.000012
Classification Train Epoch: 94 [51200/63553 (80%)]	Loss: 0.000001, KL fake Loss: 0.000012
Classification Train Epoch: 94 [57600/63553 (91%)]	Loss: 0.000025, KL fake Loss: 0.000014

Test set: Average loss: 0.4865, Accuracy: 21434/22777 (94%)

Classification Train Epoch: 95 [0/63553 (0%)]	Loss: 0.000001, KL fake Loss: 0.000071
 95%|█████████▌| 95/100 [6:58:19<22:00, 264.18s/it] 96%|█████████▌| 96/100 [7:02:44<17:36, 264.18s/it] 97%|█████████▋| 97/100 [7:07:08<13:12, 264.19s/it] 98%|█████████▊| 98/100 [7:11:32<08:48, 264.18s/it] 99%|█████████▉| 99/100 [7:15:56<04:24, 264.18s/it]100%|██████████| 100/100 [7:20:20<00:00, 264.22s/it]100%|██████████| 100/100 [7:20:20<00:00, 264.21s/it]
Classification Train Epoch: 95 [6400/63553 (10%)]	Loss: 0.000001, KL fake Loss: 0.000012
Classification Train Epoch: 95 [12800/63553 (20%)]	Loss: 0.000034, KL fake Loss: 0.000012
Classification Train Epoch: 95 [19200/63553 (30%)]	Loss: 0.000002, KL fake Loss: 0.000011
Classification Train Epoch: 95 [25600/63553 (40%)]	Loss: 0.000001, KL fake Loss: 0.000012
Classification Train Epoch: 95 [32000/63553 (50%)]	Loss: 0.000002, KL fake Loss: 0.000010
Classification Train Epoch: 95 [38400/63553 (60%)]	Loss: 0.000015, KL fake Loss: 0.000020
Classification Train Epoch: 95 [44800/63553 (70%)]	Loss: 0.000003, KL fake Loss: 0.000013
Classification Train Epoch: 95 [51200/63553 (80%)]	Loss: 0.000003, KL fake Loss: 0.000015
Classification Train Epoch: 95 [57600/63553 (91%)]	Loss: 0.000003, KL fake Loss: 0.000011

Test set: Average loss: 0.5306, Accuracy: 21318/22777 (94%)

Classification Train Epoch: 96 [0/63553 (0%)]	Loss: 0.000003, KL fake Loss: 0.000034
Classification Train Epoch: 96 [6400/63553 (10%)]	Loss: 0.000005, KL fake Loss: 0.000012
Classification Train Epoch: 96 [12800/63553 (20%)]	Loss: 0.000003, KL fake Loss: 0.000013
Classification Train Epoch: 96 [19200/63553 (30%)]	Loss: 0.000003, KL fake Loss: 0.000012
Classification Train Epoch: 96 [25600/63553 (40%)]	Loss: 0.000004, KL fake Loss: 0.000014
Classification Train Epoch: 96 [32000/63553 (50%)]	Loss: 0.000015, KL fake Loss: 0.000013
Classification Train Epoch: 96 [38400/63553 (60%)]	Loss: 0.000007, KL fake Loss: 0.000010
Classification Train Epoch: 96 [44800/63553 (70%)]	Loss: 0.000001, KL fake Loss: 0.000014
Classification Train Epoch: 96 [51200/63553 (80%)]	Loss: 0.000001, KL fake Loss: 0.000012
Classification Train Epoch: 96 [57600/63553 (91%)]	Loss: 0.000002, KL fake Loss: 0.000010

Test set: Average loss: 0.5128, Accuracy: 21304/22777 (94%)

Classification Train Epoch: 97 [0/63553 (0%)]	Loss: 0.000011, KL fake Loss: 0.001177
Classification Train Epoch: 97 [6400/63553 (10%)]	Loss: 0.000003, KL fake Loss: 0.000043
Classification Train Epoch: 97 [12800/63553 (20%)]	Loss: 0.000001, KL fake Loss: 0.000033
Classification Train Epoch: 97 [19200/63553 (30%)]	Loss: 0.000004, KL fake Loss: 0.000027
Classification Train Epoch: 97 [25600/63553 (40%)]	Loss: 0.000007, KL fake Loss: 0.000024
Classification Train Epoch: 97 [32000/63553 (50%)]	Loss: 0.000001, KL fake Loss: 0.000033
Classification Train Epoch: 97 [38400/63553 (60%)]	Loss: 0.000005, KL fake Loss: 0.000025
Classification Train Epoch: 97 [44800/63553 (70%)]	Loss: 0.000005, KL fake Loss: 0.000028
Classification Train Epoch: 97 [51200/63553 (80%)]	Loss: 0.000050, KL fake Loss: 0.000022
Classification Train Epoch: 97 [57600/63553 (91%)]	Loss: 0.000001, KL fake Loss: 0.000023

Test set: Average loss: 0.6753, Accuracy: 20740/22777 (91%)

Classification Train Epoch: 98 [0/63553 (0%)]	Loss: 0.000001, KL fake Loss: 0.000241
Classification Train Epoch: 98 [6400/63553 (10%)]	Loss: 0.000012, KL fake Loss: 0.000025
Classification Train Epoch: 98 [12800/63553 (20%)]	Loss: 0.000031, KL fake Loss: 0.000025
Classification Train Epoch: 98 [19200/63553 (30%)]	Loss: 0.000002, KL fake Loss: 0.000040
Classification Train Epoch: 98 [25600/63553 (40%)]	Loss: 0.000001, KL fake Loss: 0.000022
Classification Train Epoch: 98 [32000/63553 (50%)]	Loss: 0.000071, KL fake Loss: 0.000018
Classification Train Epoch: 98 [38400/63553 (60%)]	Loss: 0.000000, KL fake Loss: 0.000020
Classification Train Epoch: 98 [44800/63553 (70%)]	Loss: 0.000002, KL fake Loss: 0.000019
Classification Train Epoch: 98 [51200/63553 (80%)]	Loss: 0.000001, KL fake Loss: 0.000018
Classification Train Epoch: 98 [57600/63553 (91%)]	Loss: 0.000003, KL fake Loss: 0.000019

Test set: Average loss: 0.7441, Accuracy: 20582/22777 (90%)

Classification Train Epoch: 99 [0/63553 (0%)]	Loss: 0.000005, KL fake Loss: 0.000030
Classification Train Epoch: 99 [6400/63553 (10%)]	Loss: 0.000012, KL fake Loss: 0.000016
Classification Train Epoch: 99 [12800/63553 (20%)]	Loss: 0.000014, KL fake Loss: 0.000016
Classification Train Epoch: 99 [19200/63553 (30%)]	Loss: 0.000003, KL fake Loss: 0.000016
Classification Train Epoch: 99 [25600/63553 (40%)]	Loss: 0.000006, KL fake Loss: 0.000019
Classification Train Epoch: 99 [32000/63553 (50%)]	Loss: 0.000002, KL fake Loss: 0.000016
Classification Train Epoch: 99 [38400/63553 (60%)]	Loss: 0.000001, KL fake Loss: 0.000018
Classification Train Epoch: 99 [44800/63553 (70%)]	Loss: 0.000002, KL fake Loss: 0.000015
Classification Train Epoch: 99 [51200/63553 (80%)]	Loss: 0.000000, KL fake Loss: 0.000015
Classification Train Epoch: 99 [57600/63553 (91%)]	Loss: 0.000002, KL fake Loss: 0.000015

Test set: Average loss: 0.6777, Accuracy: 20692/22777 (91%)

Classification Train Epoch: 100 [0/63553 (0%)]	Loss: 0.000005, KL fake Loss: 0.000033
Classification Train Epoch: 100 [6400/63553 (10%)]	Loss: 0.000063, KL fake Loss: 0.000028
Classification Train Epoch: 100 [12800/63553 (20%)]	Loss: 0.000030, KL fake Loss: 0.000016
Classification Train Epoch: 100 [19200/63553 (30%)]	Loss: 0.000001, KL fake Loss: 0.000017
Classification Train Epoch: 100 [25600/63553 (40%)]	Loss: 0.000016, KL fake Loss: 0.000013
Classification Train Epoch: 100 [32000/63553 (50%)]	Loss: 0.000039, KL fake Loss: 0.000015
Classification Train Epoch: 100 [38400/63553 (60%)]	Loss: 0.000011, KL fake Loss: 0.000028
Classification Train Epoch: 100 [44800/63553 (70%)]	Loss: 0.000010, KL fake Loss: 0.000015
Classification Train Epoch: 100 [51200/63553 (80%)]	Loss: 0.000003, KL fake Loss: 0.000015
Classification Train Epoch: 100 [57600/63553 (91%)]	Loss: 0.000024, KL fake Loss: 0.000025

Test set: Average loss: 0.6956, Accuracy: 20618/22777 (91%)

Namespace(batch_size=128, no_cuda=False, seed=1, dataset='SVHN', dataroot='./data', imageSize=32, outf='./results/joint_confidence_loss/SV-0.0001/', out_dataset='SVHN', num_classes=8, num_channels=3, pre_trained_net='results/joint_confidence_loss/SV-0.0001/model_epoch_100.pth')
Random Seed:  1
Load model
DenseNet3(
  (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock(
    (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock(
    (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv1): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock(
    (layer): Sequential(
      (0): BottleneckBlock(
        (bn1): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock(
        (bn1): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock(
        (bn1): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock(
        (bn1): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock(
        (bn1): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock(
        (bn1): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (6): BottleneckBlock(
        (bn1): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (7): BottleneckBlock(
        (bn1): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (8): BottleneckBlock(
        (bn1): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (9): BottleneckBlock(
        (bn1): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (10): BottleneckBlock(
        (bn1): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (11): BottleneckBlock(
        (bn1): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (12): BottleneckBlock(
        (bn1): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (13): BottleneckBlock(
        (bn1): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (14): BottleneckBlock(
        (bn1): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (15): BottleneckBlock(
        (bn1): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv1): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(342, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (fc): Linear(in_features=342, out_features=8, bias=True)
  (softmax): Softmax(dim=1)
)ic| len(dset): 73257
ic| len(dset): 26032
ic| len(dset): 73257
ic| len(dset): 26032

load target data:  SVHN
Using downloaded and verified file: ./Datasets/SVHN/train_32x32.mat
Using downloaded and verified file: ./Datasets/SVHN/test_32x32.mat
load non target data:  SVHN
Using downloaded and verified file: ./Datasets/SVHN/train_32x32.mat
Using downloaded and verified file: ./Datasets/SVHN/test_32x32.mat
generate log from in-distribution data

 Final Accuracy: 20618/22777 (90.52%)

generate log  from out-of-distribution data
calculate metrics
  Performance of Baseline detector
TNR at TPR 95%:            16.445%
TNR at TPR 99%:             3.936%
AUROC:                     65.107%
Detection acc:             68.144%
AUPR In:                   67.838%
AUPR Out:                  71.503%
